@inproceedings{abadi2006integrating,
  title = {Integrating Compression and Execution in Column-Oriented Database Systems},
  booktitle = {Proceedings of the 2006 {{ACM SIGMOD}} International Conference on {{Management}} of Data},
  author = {Abadi, Daniel and Madden, Samuel and Ferreira, Miguel},
  year = {2006},
  pages = {671--682},
  abstract = {Column-oriented database system architectures invite a re-evaluation of how and when data in databases is compressed. Storing data in a column-oriented fashion greatly increases the similarity of adjacent records on disk and thus opportunities for compression. The ability to compress many adjacent tuples at once lowers the per-tuple cost of compression, both in terms of CPU and space overheads.In this paper, we discuss how we extended C-Store (a column-oriented DBMS) with a compression sub-system. We show how compression schemes not traditionally used in row-oriented DBMSs can be applied to column-oriented systems. We then evaluate a set of compression schemes and show that the best scheme depends not only on the properties of the data but also on the nature of the query workload.}
}

@inproceedings{abadi2008column,
  title = {Column-Stores vs. Row-Stores: How Different Are They Really?},
  booktitle = {Proceedings of the 2008 {{ACM SIGMOD}} International Conference on {{Management}} of Data},
  author = {Abadi, Daniel J and Madden, Samuel R and Hachem, Nabil},
  year = {2008},
  pages = {967--980},
  abstract = {There has been a significant amount of excitement and recent work on column-oriented database systems ("column-stores"). These database systems have been shown to perform more than an order of magnitude better than traditional row-oriented database systems ("row-stores") on analytical workloads such as those found in data warehouses, decision support, and business intelligence applications. The elevator pitch behind this performance difference is straightforward: column-stores are more I/O efficient for read-only queries since they only have to read from disk (or from memory) those attributes accessed by a query. This simplistic view leads to the assumption that one can obtain the performance benefits of a column-store using a row-store: either by vertically partitioning the schema, or by indexing every column so that columns can be accessed independently. In this paper, we demonstrate that this assumption is false. We compare the performance of a commercial row-store under a variety of different configurations with a column-store and show that the row-store performance is significantly slower on a recently proposed data warehouse benchmark. We then analyze the performance difference and show that there are some important differences between the two systems at the query executor level (in addition to the obvious differences at the storage layer level). Using the column-store, we then tease apart these differences, demonstrating the impact on performance of a variety of column-oriented query execution techniques, including vectorized query processing, compression, and a new join algorithm we introduce in this paper. We conclude that while it is not impossible for a row-store to achieve some of the performance advantages of a column-store, changes must be made to both the storage layer and the query executor to fully obtain the benefits of a column-oriented approach.}
}

@article{abadi2013design,
  title = {The Design and Implementation of Modern Column-Oriented Database Systems},
  author = {Abadi, Daniel and Boncz, Peter and Harizopoulos, Stavros and Idreos, Stratos and Madden, Samuel and others},
  year = {2013},
  journal = {Foundations and Trends{\textregistered} in Databases},
  volume = {5},
  number = {3},
  pages = {197--280},
  publisher = {{Now Publishers, Inc.}},
  abstract = {In this article, we survey recent research on column-oriented database systems, or column-stores, where each attribute of a table is stored in a separate file or region on storage. Such databases have seen a resurgence in recent years with a rise in interest in analytic queries that perform scans and aggregates over large portions of a few columns of a table. The main advantage of a column-store is that it can access just the columns needed to answer such queries. We specifically focus on three influential research prototypes, MonetDB [46], MonetDB/X100 [18], and C-Store [86]. These systems have formed the basis for several well-known commercial column-store implementations. We describe their similarities and differences and discuss their specific architectural features for compression, late materialization, join processing, vectorization and adaptive indexing (database cracking).}
}

@article{abe2016urgent,
  title = {Urgent Virtual Machine Eviction with Enlightened Post-Copy},
  author = {Abe, Yoshihisa and Geambasu, Roxana and Joshi, Kaustubh and Satyanarayanan, Mahadev},
  year = {2016},
  journal = {ACM SIGPLAN Notices},
  volume = {51},
  number = {7},
  pages = {51--64},
  publisher = {{ACM New York, NY, USA}},
  abstract = {Virtual machine (VM) migration demands distinct properties under resource oversubscription and workload surges. We present enlightened post-copy, a new mechanism for VMs under contention that evicts the target VM with fast execution transfer and short total duration. This design contrasts with common live migration, which uses the down time of the migrated VM as its primary metric; it instead focuses on recovering the aggregate performance of the VMs being affected. In enlightened post-copy, the guest OS identifies memory state that is expected to encompass the VM's working set. The hypervisor accordingly transfers its state, mitigating the performance impact on the migrated VM resulting from post-copy transfer. We show that our implementation, with modest instrumentation in guest Linux, resolves VM contention up to several times faster than live migration.}
}

@inproceedings{abeni1998integrating,
  title = {Integrating Multimedia Applications in Hard Real-Time Systems},
  booktitle = {Proceedings 19th {{IEEE}} Real-Time Systems Symposium (Cat. {{No}}. {{98CB36279}})},
  author = {Abeni, Luca and Buttazzo, Giorgio},
  year = {1998},
  pages = {4--13},
  publisher = {{IEEE}},
  abstract = {This paper focuses on the problem of providing efficient run-time support to multimedia applications in a real-time system, where two types of tasks can coexist simultaneously: multimedia soft real-time tasks and hard real-time tasks. Hard tasks are guaranteed based on worst case execution times and minimum interarrival times, whereas multimedia and soft tasks are served based on mean parameters. The paper describes a server-based mechanism for scheduling soft and multimedia tasks without jeopardizing the a priori guarantee of hard real-time activities. The performance of the proposed method is compared with that of similar service mechanisms through extensive simulation experiments and several multimedia applications have been implemented on the HARTIK kerne}
}

@article{abouzeid2009hadoopdb,
  title = {{{HadoopDB}}: An Architectural Hybrid of {{MapReduce}} and {{DBMS}} Technologies for Analytical Workloads},
  author = {Abouzeid, Azza and {Bajda-Pawlikowski}, Kamil and Abadi, Daniel and Silberschatz, Avi and Rasin, Alexander},
  year = {2009},
  journal = {Proceedings of the VLDB Endowment},
  volume = {2},
  number = {1},
  pages = {922--933},
  publisher = {{VLDB Endowment}},
  abstract = {The production environment for analytical data management applications is rapidly changing. Many enterprises are shifting away from deploying their analytical databases on high-end proprietary machines, and moving towards cheaper, lower-end, commodity hardware, typically arranged in a shared-nothing MPP architecture, often in a virtualized environment inside public or private "clouds". At the same time, the amount of data that needs to be analyzed is exploding, requiring hundreds to thousands of machines to work in parallel to perform the analysis. There tend to be two schools of thought regarding what technology to use for data analysis in such an environment. Proponents of parallel databases argue that the strong emphasis on performance and efficiency of parallel databases makes them well-suited to perform such analysis. On the other hand, others argue that MapReduce-based systems are better suited due to their superior scalability, fault tolerance, and flexibility to handle unstructured data. In this paper, we explore the feasibility of building a hybrid system that takes the best features from both technologies; the prototype we built approaches parallel databases in performance and efficiency, yet still yields the scalability, fault tolerance, and flexibility of MapReduce-based systems.}
}

@article{adya1995efficient,
  title = {Efficient Optimistic Concurrency Control Using Loosely Synchronized Clocks},
  author = {Adya, Atul and Gruber, Robert and Liskov, Barbara and Maheshwari, Umesh},
  year = {1995},
  journal = {ACM SIGMOD Record},
  volume = {24},
  number = {2},
  pages = {23--34},
  publisher = {{ACM New York, NY, USA}},
  abstract = {This paper describes an efficient optimistic concurrency control scheme for use in distributed database systems in which objects are cached and manipulated at client machines while persistent storage and transactional support are provided by servers. The scheme provides both serializability and external consistency for committed transactions; it uses loosely synchronized clocks to achieve global serialization. It stores only a single version of each object, and avoids maintaining any concurrency control information on a per-object basis; instead, it tracks recent invalidations on a per-client basis, an approach that has low in-memory space overhead and no per-object disk overhead. In addition to its low space overheads, the scheme also performs well. The paper presents a simulation study that compares the scheme to adaptive callback locking, the best concurrency control scheme for client-server object-oriented database systems studied to date. The study shows that our scheme outperforms adaptive callback locking for low to moderate contention workloads, and scales better with the number of clients. For high contention workloads, optimism can result in a high abort rate; the scheme presented here is a first step toward a hybrid scheme that we expect to perform well across the full range of workloads.}
}

@inproceedings{adya2000generalized,
  title = {Generalized Isolation Level Definitions},
  booktitle = {Proceedings of 16th International Conference on Data Engineering (Cat. {{No}}. {{00CB37073}})},
  author = {Adya, Atul and Liskov, Barbara and O'Neil, Patrick},
  year = {2000},
  pages = {67--78},
  publisher = {{IEEE}},
  abstract = {Commercial databases support different isolation levels to allow programmers to trade off consistency for a potential gain in performance. The isolation levels are defined in the current ANSI standard, but the definitions are ambiguous and revised definitions proposed to correct the problem are too constrained since they allow only pessimistic (locking) implementations. This paper presents new specifications for the ANSI levels. Our specifications are portable: they apply not only to locking implementations, but also to optimistic and multi-version concurrency control schemes. Furthermore, unlike earlier definitions, our new specifications handle predicates in a correct and flexible manner at all levels.}
}

@article{aggarwal2018neural,
  title = {Neural Networks and Deep Learning},
  author = {Aggarwal, Charu C and others},
  year = {2018},
  journal = {Springer},
  volume = {10},
  number = {978},
  pages = {3},
  publisher = {{Springer}}
}

@article{agrawal1987concurrency,
  title = {Concurrency Control Performance Modeling: {{Alternatives}} and Implications},
  author = {Agrawal, Rakesh and Carey, Michael J and Livny, Miron},
  year = {1987},
  journal = {ACM Transactions on Database Systems (TODS)},
  volume = {12},
  number = {4},
  pages = {609--654},
  publisher = {{ACM New York, NY, USA}},
  abstract = {A number of recent studies have examined the performance of concurrency control algorithms for database management systems. The results reported to date, rather than being definitive, have tended to be contradictory. In this paper, rather than presenting ``yet another algorithm performance study,'' we critically investigate the assumptions made in the models used in past studies and their implications. We employ a fairly complete model of a database environment for studying the relative performance of three different approaches to the concurrency control problem under a variety of modeling assumptions. The three approaches studied represent different extremes in how transaction conflicts are dealt with, and the assumptions addressed pertain to the nature of the database system's resources, how transaction restarts are modeled, and the amount of information available to the concurrency control algorithm about transactions' reference strings. We show that differences in the underlying assumptions explain the seemingly contradictory performance results. We also address the question of how realistic the various assumptions are for actual database systems.}
}

@article{albutiu2012massively,
  title = {Massively Parallel Sort-Merge Joins in Main Memory Multi-Core Database Systems},
  author = {Albutiu, Martina-Cezara and Kemper, Alfons and Neumann, Thomas},
  year = {2012},
  journal = {arXiv preprint arXiv:1207.0145},
  eprint = {1207.0145},
  abstract = {Two emerging hardware trends will dominate the database system technology in the near future: increasing main memory capacities of several TB per server and massively parallel multi-core processing. Many algorithmic and control techniques in current database technology were devised for disk-based systems where I/O dominated the performance. In this work we take a new look at the well-known sort-merge join which, so far, has not been in the focus of research in scalable massively parallel multi-core data processing as it was deemed inferior to hash joins. We devise a suite of new massively parallel sort-merge (MPSM) join algorithms that are based on partial partition-based sorting. Contrary to classical sort-merge joins, our MPSM algorithms do not rely on a hard to parallelize final merge step to create one complete sort order. Rather they work on the independently created runs in parallel. This way our MPSM algorithms are NUMA-affine as all the sorting is carried out on local memory partitions. An extensive experimental evaluation on a modern 32-core machine with one TB of main memory proves the competitive performance of MPSM on large main memory databases with billions of objects. It scales (almost) linearly in the number of employed cores and clearly outperforms competing hash join proposals - in particular it outperforms the "cutting-edge" Vectorwise parallel query engine by a factor of four.},
  archiveprefix = {arxiv},
  keywords = {to-read}
}

@inproceedings{alvaro2011consistency,
  title = {Consistency Analysis in Bloom: A {{CALM}} and Collected Approach.},
  booktitle = {{{CIDR}}},
  author = {Alvaro, Peter and Conway, Neil and Hellerstein, Joseph M and Marczak, William R},
  year = {2011},
  pages = {249--260},
  publisher = {{Citeseer}}
}

@book{arenas2022database,
  title = {Database Theory},
  author = {Arenas, Marcelo and Barcel{\'o}, Pablo and Libkin, Leonid and Martens, Wim and Pieris, Andreas},
  year = {2022},
  publisher = {{Open source at {$<$}a href="https://github.com/pdm-book/community"{$>$}https://github.com/pdm-book/community{$<$}/a{$>$}}},
  file = {/Users/jayithac/Library/CloudStorage/GoogleDrive-jayithareddyp@gmail.com/My Drive/Papers/arenas2022database.pdf}
}

@book{arpacidusseau2018operating,
  title = {Operating Systems: {{Three}} Easy Pieces},
  author = {{Arpaci-Dusseau}, Remzi H. and {Arpaci-Dusseau}, Andrea C.},
  year = {2018},
  month = aug,
  edition = {1.00},
  publisher = {{Arpaci-Dusseau Books}}
}

@article{astrahan1976system,
  title = {System {{R}}: {{Relational}} Approach to Database Management},
  author = {Astrahan, Morton M. and Blasgen, Mike W. and Chamberlin, Donald D. and Eswaran, Kapali P. and Gray, Jim N and Griffiths, Patricia P. and King, W Frank and Lorie, Raymond A. and McJones, Paul R. and Mehl, James W. and others},
  year = {1976},
  journal = {ACM Transactions on Database Systems (TODS)},
  volume = {1},
  number = {2},
  pages = {97--137},
  publisher = {{ACM New York, NY, USA}},
  abstract = {System R is a database management system which provides a high level relational data interface. The systems provides a high level of data independence by isolating the end user as much as possible from underlying storage structures. The system permits definition of a variety of relational views on common underlying data. Data control features are provided, including authorization, integrity assertions, triggered transactions, a logging and recovery subsystem, and facilities for maintaining data consistency in a shared-update environment. This paper contains a description of the overall architecture and design of the system. At the present time the system is being implemented and the design evaluated. We emphasize that System R is a vehicle for research in database architecture, and is not planned as a product.}
}

@inproceedings{avnur2000eddies,
  title = {Eddies: {{Continuously}} Adaptive Query Processing},
  booktitle = {Proceedings of the 2000 {{ACM SIGMOD}} International Conference on {{Management}} of Data},
  author = {Avnur, Ron and Hellerstein, Joseph M},
  year = {2000},
  pages = {261--272},
  abstract = {In large federated and shared-nothing databases, resources can exhibit widely fluctuating characteristics. Assumptions made at the time a query is submitted will rarely hold throughout the duration of query processing. As a result, traditional static query optimization and execution techniques are ineffective in these environments. In this paper we introduce a query processing mechanism called an eddy, which continuously reorders operators in a query plan as it runs. We characterize the moments of symmetry during which pipelined joins can be easily reordered, and the synchronization barriers that require inputs from different sources to be coordinated. By combining eddies with appropriate join algorithms, we merge the optimization and execution phases of query processing, allowing each tuple to have a flexible ordering of the query operators. This flexibility is controlled by a combination of fluid dynamics and a simple learning algorithm. Our initial implementation demonstrates promising results, with eddies performing nearly as well as a static optimizer/executor in static scenarios, and providing dramatic improvements in dynamic execution environments.}
}

@article{bailis2013highly,
  title = {Highly Available Transactions: Virtues and Limitations (Extended Version)},
  author = {Bailis, Peter and Davidson, Aaron and Fekete, Alan and Ghodsi, Ali and Hellerstein, Joseph M and Stoica, Ion},
  year = {2013},
  journal = {arXiv preprint arXiv:1302.0309},
  eprint = {1302.0309},
  abstract = {To minimize network latency and remain online during server failures and network partitions, many modern distributed data storage systems eschew transactional functionality, which provides strong semantic guarantees for groups of multiple operations over multiple data items. In this work, we consider the problem of providing Highly Available Transactions (HATs): transactional guarantees that do not suffer unavailability during system partitions or incur high network latency. We introduce a taxonomy of highly available systems and analyze existing ACID isolation and distributed data consistency guarantees to identify which can and cannot be achieved in HAT systems. This unifies the literature on weak transactional isolation, replica consistency, and highly available systems. We analytically and experimentally quantify the availability and performance benefits of HATs--often two to three orders of magnitude over wide-area networks--and discuss their necessary semantic compromises.},
  archiveprefix = {arxiv}
}

@article{bailis2014coordination,
  title = {Coordination Avoidance in Database Systems ({{Extended}} Version)},
  author = {Bailis, Peter and Fekete, Alan and Franklin, Michael J and Ghodsi, Ali and Hellerstein, Joseph M and Stoica, Ion},
  year = {2014},
  journal = {arXiv preprint arXiv:1402.2237},
  eprint = {1402.2237},
  abstract = {Minimizing coordination, or blocking communication between concurrently executing operations, is key to maximizing scalability, availability, and high performance in database systems. However, uninhibited coordination-free execution can compromise application correctness, or consistency. When is coordination necessary for correctness? The classic use of serializable transactions is sufficient to maintain correctness but is not necessary for all applications, sacrificing potential scalability. In this paper, we develop a formal framework, invariant confluence, that determines whether an application requires coordination for correct execution. By operating on application-level invariants over database states (e.g., integrity constraints), invariant confluence analysis provides a necessary and sufficient condition for safe, coordination-free execution. When programmers specify their application invariants, this analysis allows databases to coordinate only when anomalies that might violate invariants are possible. We analyze the invariant confluence of common invariants and operations from real-world database systems (i.e., integrity constraints) and applications and show that many are invariant confluent and therefore achievable without coordination. We apply these results to a proof-of-concept coordination-avoiding database prototype and demonstrate sizable performance gains compared to serializable execution, notably a 25-fold improvement over prior TPC-C New-Order performance on a 200 server cluster.},
  archiveprefix = {arxiv}
}

@inproceedings{bailis2015feral,
  title = {Feral Concurrency Control: {{An}} Empirical Investigation of Modern Application Integrity},
  booktitle = {Proceedings of the 2015 {{ACM SIGMOD}} International Conference on Management of Data},
  author = {Bailis, Peter and Fekete, Alan and Franklin, Michael J and Ghodsi, Ali and Hellerstein, Joseph M and Stoica, Ion},
  year = {2015},
  pages = {1327--1342},
  abstract = {The rise of data-intensive "Web 2.0" Internet services has led to a range of popular new programming frameworks that collectively embody the latest incarnation of the vision of Object-Relational Mapping (ORM) systems, albeit at unprecedented scale. In this work, we empirically investigate modern ORM-backed applications' use and disuse of database concurrency control mechanisms. Specifically, we focus our study on the common use of feral, or application-level, mechanisms for maintaining database integrity, which, across a range of ORM systems, often take the form of declarative correctness criteria, or invariants. We quantitatively analyze the use of these mechanisms in a range of open source applications written using the Ruby on Rails ORM and find that feral invariants are the most popular means of ensuring integrity (and, by usage, are over 37 times more popular than transactions). We evaluate which of these feral invariants actually ensure integrity (by usage, up to 86.9\%) and which---due to concurrency errors and lack of database support---may lead to data corruption (the remainder), which we experimentally quantify. In light of these findings, we present recommendations for database system designers for better supporting these modern ORM programming patterns, thus eliminating their adverse effects on application integrity.}
}

@article{bailis2015readings,
  title = {Readings in Database Systems},
  author = {Bailis, Peter and Hellerstein, Joseph M and Stonebraker, Michael},
  year = {2015},
  journal = {URL: http://www. redbook. io/all-chapters. html (26.09. 2017)}
}

@article{bailis2016scalable,
  title = {Scalable Atomic Visibility with {{RAMP}} Transactions},
  author = {Bailis, Peter and Fekete, Alan and Ghodsi, Ali and Hellerstein, Joseph M and Stoica, Ion},
  year = {2016},
  journal = {ACM Transactions on Database Systems (TODS)},
  volume = {41},
  number = {3},
  pages = {1--45},
  publisher = {{ACM New York, NY, USA}},
  abstract = {Databases can provide scalability by partitioning data across several servers. However, multipartition, multioperation transactional access is often expensive, employing coordination-intensive locking, validation, or scheduling mechanisms. Accordingly, many real-world systems avoid mechanisms that provide useful semantics for multipartition operations. This leads to incorrect behavior for a large class of applications including secondary indexing, foreign key enforcement, and materialized view maintenance. In this work, we identify a new isolation model{\textemdash}Read Atomic (RA) isolation{\textemdash}that matches the requirements of these use cases by ensuring atomic visibility: either all or none of each transaction's updates are observed by other transactions. We present algorithms for Read Atomic Multipartition (RAMP) transactions that enforce atomic visibility while offering excellent scalability, guaranteed commit despite partial failures (via coordination-free execution), and minimized communication between servers (via partition independence). These RAMP transactions correctly mediate atomic visibility of updates and provide readers with snapshot access to database state by using limited multiversioning and by allowing clients to independently resolve nonatomic reads. We demonstrate that, in contrast with existing algorithms, RAMP transactions incur limited overhead{\textemdash}even under high contention{\textemdash}and scale linearly to 100 servers.}
}

@inproceedings{bandle2021partition,
  title = {To Partition, or Not to Partition, That Is the Join Question in a Real System},
  booktitle = {Proceedings of the 2021 International Conference on Management of Data},
  author = {Bandle, Maximilian and Giceva, Jana and Neumann, Thomas},
  year = {2021},
  pages = {168--180},
  abstract = {An efficient implementation of a hash join has been a highly researched problem for decades. Recently, the radix join has been shown to have superior performance over the alternatives (e.g., the non-partitioned hash join), albeit on synthetic microbenchmarks. Therefore, it is unclear whether one can simply replace the hash join in an RDBMS or use the radix join as a performance booster for selected queries. If the latter, it is still unknown when one should rely on the radix join to improve performance. In this paper, we address these questions, show how to integrate the radix join in Umbra, a code-generating DBMS, and make it competitive for selective queries by introducing a Bloom-filter based semi-join reducer. We have evaluated how well it runs when used in queries from more representative workloads like TPC-H. Surprisingly, the radix join brings a noticeable improvement in only one out of all 59 joins in TPC-H. Thus, with an extensive range of microbenchmarks, we have isolated the effects of the most important workload factors and synthesized the range of values where partitioning the data for the radix join pays off. Our analysis shows that the benefit of data partitioning quickly diminishes as soon as we deviate from the optimal parameters, and even late materialization rarely helps in real workloads. We thus, conclude that integrating the radix join within a code-generating database rarely justifies the increase in code and optimizer complexity and advise against it for processing real-world workloads.}
}

@article{barham2003xen,
  title = {Xen and the Art of Virtualization},
  author = {Barham, Paul and Dragovic, Boris and Fraser, Keir and Hand, Steven and Harris, Tim and Ho, Alex and Neugebauer, Rolf and Pratt, Ian and Warfield, Andrew},
  year = {2003},
  journal = {ACM SIGOPS operating systems review},
  volume = {37},
  number = {5},
  pages = {164--177},
  publisher = {{ACM New York, NY, USA}},
  abstract = {Numerous systems have been designed which use virtualization to subdivide the ample resources of a modern computer. Some require specialized hardware, or cannot support commodity operating systems. Some target 100\% binary compatibility at the expense of performance. Others sacrifice security or functionality for speed. Few offer resource isolation or performance guarantees; most provide only best-effort provisioning, risking denial of service.This paper presents Xen, an x86 virtual machine monitor which allows multiple commodity operating systems to share conventional hardware in a safe and resource managed fashion, but without sacrificing either performance or functionality. This is achieved by providing an idealized virtual machine abstraction to which operating systems such as Linux, BSD and Windows XP, can be ported with minimal effort.Our design is targeted at hosting up to 100 virtual machine instances simultaneously on a modern server. The virtualization approach taken by Xen is extremely efficient: we allow operating systems such as Linux and Windows XP to be hosted simultaneously for a negligible performance overhead --- at most a few percent compared with the unvirtualized case. We considerably outperform competing commercial and freely available solutions in a range of microbenchmarks and system-wide tests.}
}

@inproceedings{baruah2002implementing,
  title = {Implementing Constant-Bandwidth Servers upon Multiprocessor Platforms},
  booktitle = {Proceedings. {{Eighth IEEE}} Real-Time and Embedded Technology and Applications Symposium},
  author = {Baruah, Sanjoy and Goossens, Jo{\"e}l and Lipari, Giuseppe},
  year = {2002},
  pages = {154--163},
  publisher = {{IEEE}},
  abstract = {In constant-bandwidth server (CBS) systems, several different applications are executed upon a shared computing platform in such a manner that each application seems to be executing on a slower dedicated processor. CBS systems have thus far only been implemented upon uniprocessors; here, a multiprocessor extension, which can be implemented upon computing platforms comprised of several identical preemptable processors, is proposed and proven correct.}
}

@inproceedings{beaver2010finding,
  title = {Finding a Needle in Haystack: {{Facebook}}'s Photo Storage},
  booktitle = {9th {{USENIX}} Symposium on Operating Systems Design and Implementation ({{OSDI}} 10)},
  author = {Beaver, Doug and Kumar, Sanjeev and Li, Harry C and Sobel, Jason and Vajgel, Peter},
  year = {2010}
}

@book{bengio2017deep,
  title = {Deep Learning},
  author = {Bengio, Yoshua and Goodfellow, Ian and Courville, Aaron},
  year = {2017},
  volume = {1},
  publisher = {{MIT press Cambridge, MA, USA}},
  file = {/Users/jayithac/Library/CloudStorage/GoogleDrive-jayithareddyp@gmail.com/My Drive/Books/Machine Learning/Deep Learning/Deep Learning.pdf}
}

@inproceedings{berchtold1998pyramid,
  title = {The Pyramid-Technique: {{Towards}} Breaking the Curse of Dimensionality},
  booktitle = {Proceedings of the 1998 {{ACM SIGMOD}} International Conference on {{Management}} of Data},
  author = {Berchtold, Stefan and B{\"o}hm, Christian and Kriegal, Hans-Peter},
  year = {1998},
  pages = {142--153},
  abstract = {In this paper, we propose the Pyramid-Technique, a new indexing method for high-dimensional data spaces. The Pyramid-Technique is highly adapted to range query processing using the maximum metric Lmax. In contrast to all other index structures, the performance of the Pyramid-Technique does not deteriorate when processing range queries on data of higher dimensionality. The Pyramid-Technique is based on a special partitioning strategy which is optimized for high-dimensional data. The basic idea is to divide the data space first into 2d pyramids sharing the center point of the space as a top. In a second step, the single pyramids are cut into slices parallel to the basis of the pyramid. These slices from the data pages. Furthermore, we show that this partition provides a mapping from the given d-dimensional space to a 1-dimensional space. Therefore, we are able to use a B+-tree to manage the transformed data. As an analytical evaluation of our technique for hypercube range queries and uniform data distribution shows, the Pyramid-Technique clearly outperforms index structures using other partitioning strategies. To demonstrate the practical relevance of our technique, we experimentally compared the Pyramid-Technique with the X-tree, the Hilbert R-tree, and the Linear Scan. The results of our experiments using both, synthetic and real data, demonstrate that the Pyramid-Technique outperforms the X-tree and the Hilbert R-tree by a factor of up to 14 (number of page accesses) and up to 2500 (total elapsed time) for range queries.},
  file = {/Users/jayithac/Library/CloudStorage/GoogleDrive-jayithareddyp@gmail.com/My Drive/Papers/berchtold1998pyramid.pdf}
}

@inproceedings{berchtold2000independent,
  title = {Independent Quantization: {{An}} Index Compression Technique for High-Dimensional Data Spaces},
  booktitle = {Proceedings of 16th International Conference on Data Engineering (Cat. {{No}}. {{00CB37073}})},
  author = {Berchtold, Stefan and Bohm, Christian and Jagadish, Hosagrahar V and Kriegel, H-P and Sander, J{\"o}rg},
  year = {2000},
  pages = {577--588},
  publisher = {{IEEE}},
  abstract = {Two major approaches have been proposed to efficiently process queries in databases: speeding up the search by using index structures, and speeding up the search by operating on a compressed database, such as a signature file. Both approaches have their limitations: indexing techniques are inefficient in extreme configurations, such as high-dimensional spaces, where even a simple scan may be cheaper than an index-based search. Compression techniques are not very efficient in all other situations. We propose to combine both techniques to search for nearest neighbors in a high-dimensional space. For this purpose, we develop a compressed index, called the IQ-tree, with a three-level structure: the first level is a regular (flat) directory consisting of minimum bounding boxes, the second level contains data points in a compressed representation, and the third level contains the actual data. We overcome several engineering challenges in constructing an effective index structure of this type. The most significant of these is to decide how much to compress at the second level. Too much compression will lead to many needless expensive accesses to the third level. Too little compression will increase both the storage and the access cost for the first two levels. We develop a cost model and an optimization algorithm based on this cost model that permits an independent determination of the degree of compression for each second level page to minimize expected query cost. In an experimental evaluation, we demonstrate that the IQ-tree shows a performance that is the "best of both worlds" for a wide range of data distributions and dimensionalities.}
}

@article{berenson1995critique,
  title = {A Critique of {{ANSI SQL}} Isolation Levels},
  author = {Berenson, Hal and Bernstein, Phil and Gray, Jim and Melton, Jim and O'Neil, Elizabeth and O'Neil, Patrick},
  year = {1995},
  journal = {ACM SIGMOD Record},
  volume = {24},
  number = {2},
  pages = {1--10},
  publisher = {{ACM New York, NY, USA}},
  abstract = {ANSI SQL-92 [MS, ANSI] defines Isolation Levels in terms of phenomena: Dirty Reads, Non-Repeatable Reads, and Phantoms. This paper shows that these phenomena and the ANSI SQL definitions fail to properly characterize several popular isolation levels, including the standard locking implementations of the levels covered. Ambiguity in the statement of the phenomena is investigated and a more formal statement is arrived at; in addition new phenomena that better characterize isolation types are introduced. Finally, an important multiversion isolation type, called Snapshot Isolation, is defined.}
}

@article{bernstein1981concurrency,
  title = {Concurrency Control in Distributed Database Systems},
  author = {Bernstein, Philip A and Goodman, Nathan},
  year = {1981},
  journal = {ACM Computing Surveys (CSUR)},
  volume = {13},
  number = {2},
  pages = {185--221},
  publisher = {{ACM New York, NY, USA}}
}

@inproceedings{bernstein2014orleans,
  title = {Orleans: {{Distributed}} Virtual Actors for Programmability and Scalability},
  booktitle = {{{MSR-TR-2014}}{\textendash}41},
  author = {Bernstein, Phil and Bykov, Sergey and Geller, Alan and Kliot, Gabriel and Thelin, Jorgen},
  year = {2014}
}

@inproceedings{bershad1994spin,
  title = {{{SPIN}}: {{An}} Extensible Microkernel for Application-Specific Operating System Services},
  booktitle = {Proceedings of the 6th Workshop on {{ACM SIGOPS European}} Workshop: {{Matching}} Operating Systems to Application Needs},
  author = {Bershad, Brian N and Chambers, Craig and Eggers, Susan and Maeda, Chris and McNamee, Dylan and Pardyak, Przemyslaw and Savage, Stefan and Sirer, Emin G{\"u}n},
  year = {1994},
  pages = {68--71},
  abstract = {Application domains such as multimedia, databases, and parallel computing, require operating system services with high performance and high functionality. Existing operating systems provide fixed interfaces and implementations to system services and resources. This makes them inappropriate for applications whose resource demands and usage patterns are poorly matched by the services provided. The SPIN operating system enables system services to be defined in an application-specific fashion through an extensible microkernel. It offers applications fine-grained control over a machine's logical and physical resources through run-time adaptation of the system to application requirements.}
}

@inproceedings{beyer1999nearest,
  title = {When Is ``Nearest Neighbor'' Meaningful?},
  booktitle = {Database {{Theory}}{\textemdash}{{ICDT}}'99: 7th International Conference Jerusalem, Israel, January 10{\textendash}12, 1999 Proceedings 7},
  author = {Beyer, Kevin and Goldstein, Jonathan and Ramakrishnan, Raghu and Shaft, Uri},
  year = {1999},
  pages = {217--235},
  publisher = {{Springer}},
  abstract = {We explore the effect of dimensionality on the ``nearest neighbor'' problem. We show that under a broad set of conditions (much broader than independent and identically distributed dimensions), as dimensionality increases, the distance to the nearest data point approaches the distance to the farthest data point. To provide a practical perspective, we present empirical results on both real and synthetic data sets that demonstrate that this effect can occur for as few as 10{\textendash}15 dimensions. These results should not be interpreted to mean that high-dimensional indexing is never meaningful; we illustrate this point by identifying some high-dimensional workloads for which this effect does not occur. However, our results do emphasize that the methodology used almost universally in the database literature to evaluate high-dimensional indexing techniques is flawed, and should be modified. In particular, most such techniques proposed in the literature are not evaluated versus simple linear scan, and are evaluated over workloads for which nearest neighbor is not meaningful. Often, even the reported experiments, when analyzed carefully, show that linear scan would outperform the techniques being proposed on the workloads studied in high (10{\textendash}15) dimensionality!}
}

@article{blaum1994evenodd,
  title = {{{EVENODD}}: {{An}} Optimal Scheme for Tolerating Double Disk Failures in {{RAID}} Architectures},
  author = {Blaum, Mario and Brady, Jim and Bruck, Jehoshua and Menon, Jai},
  year = {1994},
  journal = {ACM SIGARCH Computer Architecture News},
  volume = {22},
  number = {2},
  pages = {245--254},
  publisher = {{ACM New York, NY, USA}},
  abstract = {We present a novel method, that we call EVENODD, for tolerating up to two disk failures in RAID architectures. EVENODD is the first known scheme for tolerating double disk failures that is optimal with regard to both storage and performance. EVENODD employs the addition of only two redundant disks and consists of simple exclusive-OR computations. A major advantage of EVENODD is that it only requires parity hardware, which is typically present in standard RAID-5 controllers. Hence, EVENODD can be implemented on standard RAID-5 controllers without any hardware changes. The only previously known scheme that employes optimal redundant storage (i.e. two extra disks) is based on Reed-Solomon (RS) error-correcting codes, requires computation over finite fields and results in a more complex implementation. For example, we show that the number of exclusive-OR operations involved in implementing EVENODD in a disk array with 15 disks is about 50\% of the number required when using the RS scheme.}
}

@article{blumofe1995cilk,
  title = {Cilk: {{An}} Efficient Multithreaded Runtime System},
  author = {Blumofe, Robert D and Joerg, Christopher F and Kuszmaul, Bradley C and Leiserson, Charles E and Randall, Keith H and Zhou, Yuli},
  year = {1995},
  journal = {ACM SigPlan Notices},
  volume = {30},
  number = {8},
  pages = {207--216},
  publisher = {{ACM New York, NY, USA}},
  abstract = {Cilk (pronounced ``silk'') is a C-based runtime system for multi-threaded parallel programming. In this paper, we document the efficiency of the Cilk work-stealing scheduler, both empirically and analytically. We show that on real and synthetic applications, the ``work'' and ``critical path'' of a Cilk computation can be used to accurately model performance. Consequently, a Cilk programmer can focus on reducing the work and critical path of his computation, insulated from load balancing and other runtime scheduling issues. We also prove that for the class of ``fully strict'' (well-structured) programs, the Cilk scheduler achieves space, time and communication bounds all within a constant factor of optimal. The Cilk runtime system currently runs on the Connection Machine CM5 MPP, the Intel Paragon MPP, the Silicon Graphics Power Challenge SMP, and the MIT Phish network of workstations. Applications written in Cilk include protein folding, graphic rendering, backtrack search, and the *Socrates chess program, which won third prize in the 1994 ACM International Computer Chess Championship.}
}

@article{b√∂hm2009high,
  title = {High Dimensional Indexing},
  author = {B{\"o}hm, Christian and Plant, Claudia},
  year = {2009},
  month = jan,
  doi = {10.1007/978-0-387-39940-9_804}
}

@misc{bos2023rust,
  title = {Rust Temporary Lifetimes and "Super Let"},
  author = {Bos, Mara},
  year = {2023},
  month = nov,
  urldate = {2023-12-01},
  abstract = {The lifetime of temporaries in Rust is a complicated but often ignored topic. In simple cases, Rust keeps temporaries around for exactly long enough, such that we don't have to think about them. However, there are plenty of cases were we might not get exactly what we want, right away. In this post, we (re)discover the rules for the lifetime of temporaries, go over a few use cases for temporary lifetime extension, and explore a new language idea, super let, to give us more control.},
  howpublished = {https://blog.m-ou.se/super-let/},
  langid = {english},
  file = {/Users/jayithac/Zotero/storage/5Q8F2G3B/super-let.html}
}

@article{brewer2012cap,
  title = {{{CAP}} Twelve Years Later: {{How}} the" Rules" Have Changed},
  author = {Brewer, Eric},
  year = {2012},
  journal = {Computer},
  volume = {45},
  number = {2},
  pages = {23--29},
  publisher = {{IEEE}},
  abstract = {The CAP theorem asserts that any networked shared-data system can have only two of three desirable properties. However, by explicitly handling partitions, designers can optimize consistency and availability, thereby achieving some trade-off of all three. The featured Web extra is a podcast from Software Engineering Radio, in which the host interviews Dwight Merriman about the emerging NoSQL movement, the three types of nonrelational data stores, Brewer's CAP theorem, and much more.}
}

@book{brodie2018making,
  title = {Making Databases Work: {{The}} Pragmatic Wisdom of {{Michael Stonebraker}}},
  author = {Brodie, Michael L},
  year = {2018},
  publisher = {{Association for Computing Machinery and Morgan \& Claypool}},
  abstract = {At the ACM Awards banquet in June 2017, during the 50th anniversary celebration of the A.M. Turing Award, ACM announced the launch of the ACM A.M. Turing Book Series, a sub-series of ACM Books, to celebrate the winners of the A.M. Turing Award, computing's highest honor, the "Nobel Prize" for computing. This series aims to highlight the accomplishments of awardees, explaining their major contributions of lasting importance in computing. "Making Databases Work: The Pragmatic Wisdom of Michael Stonebraker," the first book in the series, celebrates Mike's contributions and impact. What accomplishments warranted computing's highest honor? How did Stonebraker do it? Who is Mike Stonebraker---researcher, professor, CTO, lecturer, innovative product developer, serial entrepreneur, and decades-long leader, and research evangelist for the database community. This book describes Mike's many contributions and evaluates them in light of the Turing Award. The book describes, in 36 chapters, the unique nature, significance, and impact of Mike's achievements in advancing modern database systems over more than 40 years. The stories involve technical concepts, projects, people, prototype systems, failures, lucky accidents, crazy risks, startups, products, venture capital, and lots of applications that drove Mike Stonebraker's achievements and career. Even if you have no interest in databases at all, you'll gain insights into the birth and evolution of Turing Award-worthy achievements from the perspectives of 39 remarkable computer scientists and professionals. Today, data is considered the world's most valuable resource ("The Economist," May 6, 2017), whether it is in the tens of millions of databases used to manage the world's businesses and governments, in the billions of databases in our smartphones and watches, or residing elsewhere, as yet unmanaged, awaiting the elusive next generation of database systems. Every one of the millions or billions of databases includes features that are celebrated by the 2014 A.M. Turing Award and are described in this book.}
}

@misc{brucedawsonComparingFloatingPoint2012,
  title = {Comparing {{Floating Point Numbers}}, 2012 {{Edition}}},
  author = {{brucedawson}},
  year = {2012},
  month = feb,
  journal = {Random ASCII - tech blog of Bruce Dawson},
  urldate = {2023-11-08},
  abstract = {This post is a more carefully thought out and peer reviewed version of a floating-point comparison article I wrote many years ago. This one gives solid advice and some surprising observations about{\ldots}},
  langid = {english}
}

@article{bruno2009teaching,
  title = {Teaching an Old Elephant New Tricks},
  author = {Bruno, Nicolas},
  year = {2009},
  journal = {arXiv preprint arXiv:0909.1758},
  eprint = {0909.1758},
  abstract = {In recent years, column stores (or C-stores for short) have emerged as a novel approach to deal with read-mostly data warehousing applications. Experimental evidence suggests that, for certain types of queries, the new features of C-stores result in orders of magnitude improvement over traditional relational engines. At the same time, some C-store proponents argue that C-stores are fundamentally different from traditional engines, and therefore their benefits cannot be incorporated into a relational engine short of a complete rewrite. In this paper we challenge this claim and show that many of the benefits of C-stores can indeed be simulated in traditional engines with no changes whatsoever. We then identify some limitations of our ?pure-simulation? approach for the case of more complex queries. Finally, we predict that traditional relational engines will eventually leverage most of the benefits of C-stores natively, as is currently happening in other domains such as XML data.},
  archiveprefix = {arxiv}
}

@article{bugnion1997disco,
  title = {Disco: {{Running}} Commodity Operating Systems on Scalable Multiprocessors},
  author = {Bugnion, Edouard and Devine, Scott and Govil, Kinshuk and Rosenblum, Mendel},
  year = {1997},
  journal = {ACM Transactions on Computer Systems (TOCS)},
  volume = {15},
  number = {4},
  pages = {412--447},
  publisher = {{ACM New York, NY, USA}},
  abstract = {In this article we examine the problem of extending modern operating systems to run efficiently on large-scale shared-memory multiprocessors without a large implementation effort. Our approach brings back an idea popular in the 1970s: virtual machine monitors. We use virtual machines to run multiple commodity operating systems on a scalable multiprocessor. This solution addresses many of the challenges facing the system software for these machines. We demonstrate our approach with a prototype called Disco that runs multiple copies of Silicon Graphics' IRIX operating system on a multiprocessor. Our experience shows that the overheads of the monitor are small and that the approach provides scalability as well as the ability to deal with the nonuniform memory access time of these systems. To reduce the memory overheads associated with running multiple operating systems, virtual machines transparently share major data structures such as the program code and the file system buffer cache. We use the distributed-system support of modern operating systems to export a partial single system image to the users. The overall solution achieves most of the benefits of operating systems customized for scalable multiprocessors, yet it can be achieved with a significantly smaller implementation effort.}
}

@misc{BuildingLargescaleDistributed,
  title = {Building a {{Large-scale Distributed Storage System Based}} on {{Raft}}},
  urldate = {2023-11-27},
  abstract = {In recent years, building a large-scale distributed storage system has become a hot topic. Distributed consensus algorithms like Paxos and Raft are the focus of many technical articles. But those articles tend to be introductory, describing the basics of the algorithm and log replication. They seldom cover how to build a large-scale distributed storage system based on the distributed consensus algorithm. Since April 2015, we PingCAP have been building TiKV, a large-scale open-source distributed database based on Raft.},
  howpublished = {https://tikv.org/blog/building-distributed-storage-system-on-raft/},
  langid = {american},
  file = {/Users/jayithac/Zotero/storage/RZNQVAE4/building-distributed-storage-system-on-raft.html}
}

@inproceedings{burrows2006chubby,
  title = {The {{Chubby}} Lock Service for Loosely-Coupled Distributed Systems},
  booktitle = {Proceedings of the 7th Symposium on {{Operating}} Systems Design and Implementation},
  author = {Burrows, Mike},
  year = {2006},
  pages = {335--350},
  abstract = {We describe our experiences with the Chubby lock service, which is intended to provide coarse-grained locking as well as reliable (though low-volume) storage for a loosely-coupled distributed system. Chubby provides an interface much like a distributed file system with advisory locks, but the design emphasis is on availability and reliability, as opposed to high performance. Many instances of the service have been used for over a year, with several of them each handling a few tens of thousands of clients concurrently. The paper describes the initial design and expected use, compares it with actual use, and explains how the design had to be modified to accommodate the differences.}
}

@incollection{carey1991architecture,
  title = {The Architecture of the {{EXODUS}} Extensible {{DBMS}}},
  booktitle = {On Object-Oriented Database Systems},
  author = {Carey, Michael J and DeWitt, David J and Frank, Daniel and Graefe, Goetz and Richardson, Joel E and Shekita, Eugene J and Muralikrlshna, M},
  year = {1991},
  pages = {231--256},
  publisher = {{Springer}},
  abstract = {With non-traditional application areas such as engineering design, image/voice data management, scientific/statistical applications, and artificial intelligence systems all clamoring for ways to store and efficiently process larger and larger volumes of data, it is clear that traditional database technology has been pushed to its limits. It also seems clear that no single database system will be capable of simultaneously meeting the functionality and performance requirements of such a diverse set of applications. In this paper we describe the initial design of EXODUS, an extensible database system that will facilitate the fast development of high-performance, application-specific database systems. EXODUS provides certain kernel facilities, including a versatile storage manager and a type manager. In addition, it provides an architectural framework for building application-specific database systems, tools to partially automate the generation of such systems, and libraries of software components (e.g., access methods) that are likely to be useful for many application domains.}
}

@inproceedings{castro1999practical,
  title = {Practical Byzantine Fault Tolerance},
  booktitle = {{{OsDI}}},
  author = {Castro, Miguel and Liskov, Barbara and others},
  year = {1999},
  volume = {99},
  pages = {173--186}
}

@inproceedings{castro2000proactive,
  title = {Proactive Recovery in a \{byzantine-Fault-Tolerant\} System},
  booktitle = {Fourth Symposium on Operating Systems Design and Implementation ({{OSDI}} 2000)},
  author = {Castro, Miguel and Liskov, Barbara},
  year = {2000},
  abstract = {This paper describes an asynchronous state-machine replication system that tolerates Byzantine faults, which can be caused by malicious attacks or software errors. Our system is the first to recover Byzantine-faulty replicas proactively and it performs well because it uses symmetric rather than public-key cryptography for authentication. The recovery mechanism allows us to tolerate any number of faults over the lifetime of the system provided fewer than 1/3 of the replicas become faulty within a window of vulnerability that is small under normal conditions. The window may increase under a denial-of-service attack but we can detect and respond to such attacks. The paper presents results of experiments showing that overall performance is good and that even a small window of vulnerability has little impact on service latency.}
}

@inproceedings{chamberlin1974sequel,
  title = {{{SEQUEL}}: {{A}} Structured English Query Language},
  booktitle = {Proceedings of the 1974 {{ACM SIGFIDET}} (Now {{SIGMOD}}) Workshop on {{Data}} Description, Access and Control},
  author = {Chamberlin, Donald D and Boyce, Raymond F},
  year = {1974},
  pages = {249--264},
  abstract = {In this paper we present the data manipulation facility for a structured English query language (SEQUEL) which can be used for accessing data in an integrated relational data base. Without resorting to the concepts of bound variables and quantifiers SEQUEL identifies a set of simple operations on tabular structures, which can be shown to be of equivalent power to the first order predicate calculus. A SEQUEL user is presented with a consistent set of keyword English templates which reflect how people use tables to obtain information. Moreover, the SEQUEL user is able to compose these basic templates in a structured manner in order to form more complex queries. SEQUEL is intended as a data base sublanguage for both the professional programmer and the more infrequent data base user.}
}

@inproceedings{chamberlin1975views,
  title = {Views, Authorization, and Locking in a Relational Data Base System},
  booktitle = {Proceedings of the {{May}} 19-22, 1975, National Computer Conference and Exposition},
  author = {Chamberlin, Donald D and Gray, Jim N and Traiger, Irving L},
  year = {1975},
  pages = {425--430},
  abstract = {In the interest of brevity we assume that the reader is familiar with the notion of a relational data base. In particular, we assume a familiarity with the work of Codd or Boyce and Chamberlin. The examples in this paper will be drawn from a data base which describes a department store and consists of three relations: EMP(NAME, SAL, MGR, DEPT) SALES(DEPT, ITEM, VOL) LOC(DEPT, FLOOR)}
}

@article{chamberlin1981history,
  title = {A History and Evaluation of {{System R}}},
  author = {Chamberlin, Donald D and Astrahan, Morton M and Blasgen, Michael W and Gray, James N and King, W Frank and Lindsay, Bruce G and Lorie, Raymond and Mehl, James W and Price, Thomas G and Putzolu, Franco and others},
  year = {1981},
  journal = {Communications of the ACM},
  volume = {24},
  number = {10},
  pages = {632--646},
  publisher = {{ACM New York, NY, USA}},
  abstract = {System R, an experimental database system, was constructed to demonstrate that the usability advantages of the relational data model can be realized in a system with the complete function and high performance required for everyday production use. This paper describes the three principal phases of the System R project and discusses some of the lessons learned from System R about the design of relational systems and database systems in general.}
}

@inproceedings{chandra2007paxos,
  title = {Paxos Made Live: An Engineering Perspective},
  booktitle = {Proceedings of the Twenty-Sixth Annual {{ACM}} Symposium on {{Principles}} of Distributed Computing},
  author = {Chandra, Tushar D and Griesemer, Robert and Redstone, Joshua},
  year = {2007},
  pages = {398--407},
  abstract = {We describe our experience in building a fault-tolerant data-base using the Paxos consensus algorithm. Despite the existing literature in the field, building such a database proved to be non-trivial. We describe selected algorithmic and engineering problems encountered, and the solutions we found for them. Our measurements indicate that we have built a competitive system.}
}

@inproceedings{chandramouli2018faster,
  title = {Faster: {{A}} Concurrent Key-Value Store with in-Place Updates},
  booktitle = {Proceedings of the 2018 International Conference on Management of Data},
  author = {Chandramouli, Badrish and Prasaad, Guna and Kossmann, Donald and Levandoski, Justin and Hunter, James and Barnett, Mike},
  year = {2018},
  pages = {275--290},
  abstract = {Over the last decade, there has been a tremendous growth in data-intensive applications and services in the cloud. Data is created on a variety of edge sources, e.g., devices, browsers, and servers, and processed by cloud applications to gain insights or take decisions. Applications and services either work on collected data, or monitor and process data in real time. These applications are typically update intensive and involve a large amount of state beyond what can fit in main memory. However, they display significant temporal locality in their access pattern. This paper presents FASTER, a new key-value store for point read, blind update, and read-modify-write operations. FASTER combines a highly cache-optimized concurrent hash index with a hybrid log: a concurrent log-structured record store that spans main memory and storage, while supporting fast in-place updates of the hot set in memory. Experiments show that FASTER achieves orders-of-magnitude better throughput - up to 160M operations per second on a single machine - than alternative systems deployed widely today, and exceeds the performance of pure in-memory data structures when the workload fits in memory.}
}

@article{chang2008bigtable,
  title = {Bigtable: {{A}} Distributed Storage System for Structured Data},
  author = {Chang, Fay and Dean, Jeffrey and Ghemawat, Sanjay and Hsieh, Wilson C and Wallach, Deborah A and Burrows, Mike and Chandra, Tushar and Fikes, Andrew and Gruber, Robert E},
  year = {2008},
  journal = {ACM Transactions on Computer Systems (TOCS)},
  volume = {26},
  number = {2},
  pages = {1--26},
  publisher = {{ACM New York, NY, USA}},
  abstract = {Bigtable is a distributed storage system for managing structured data that is designed to scale to a very large size: petabytes of data across thousands of commodity servers. Many projects at Google store data in Bigtable, including web indexing, Google Earth, and Google Finance. These applications place very different demands on Bigtable, both in terms of data size (from URLs to web pages to satellite imagery) and latency requirements (from backend bulk processing to real-time data serving). Despite these varied demands, Bigtable has successfully provided a flexible, high-performance solution for all of these Google products. In this article, we describe the simple data model provided by Bigtable, which gives clients dynamic control over data layout and format, and we describe the design and implementation of Bigtable.}
}

@inproceedings{chen2015using,
  title = {Using {{Crash Hoare}} Logic for Certifying the {{FSCQ}} File System},
  booktitle = {Proceedings of the 25th Symposium on Operating Systems Principles},
  author = {Chen, Haogang and Ziegler, Daniel and Chajed, Tej and Chlipala, Adam and Kaashoek, M Frans and Zeldovich, Nickolai},
  year = {2015},
  pages = {18--37},
  abstract = {FSCQ is the first file system with a machine-checkable proof (using the Coq proof assistant) that its implementation meets its specification and whose specification includes crashes. FSCQ provably avoids bugs that have plagued previous file systems, such as performing disk writes without sufficient barriers or forgetting to zero out directory blocks. If a crash happens at an inopportune time, these bugs can lead to data loss. FSCQ's theorems prove that, under any sequence of crashes followed by reboots, FSCQ will recover the file system correctly without losing data. To state FSCQ's theorems, this paper introduces the Crash Hoare logic (CHL), which extends traditional Hoare logic with a crash condition, a recovery procedure, and logical address spaces for specifying disk states at different abstraction levels. CHL also reduces the proof effort for developers through proof automation. Using CHL, we developed, specified, and proved the correctness of the FSCQ file system. Although FSCQ's design is relatively simple, experiments with FSCQ running as a user-level file system show that it is sufficient to run Unix applications with usable performance. FSCQ's specifications and proofs required significantly more work than the implementation, but the work was manageable even for a small team of a few researchers.}
}

@article{cheung2013optimizing,
  title = {Optimizing Database-Backed Applications with Query Synthesis},
  author = {Cheung, Alvin and {Solar-Lezama}, Armando and Madden, Samuel},
  year = {2013},
  journal = {ACM SIGPLAN Notices},
  volume = {48},
  number = {6},
  pages = {3--14},
  publisher = {{ACM New York, NY, USA}},
  abstract = {Object-relational mapping libraries are a popular way for applications to interact with databases because they provide transparent access to the database using the same language as the application. Unfortunately, using such frameworks often leads to poor performance, as modularity concerns encourage developers to implement relational operations in application code. Such application code does not take advantage of the optimized relational implementations that database systems provide, such as efficient implementations of joins or push down of selection predicates. In this paper we present QBS, a system that automatically transforms fragments of application logic into SQL queries. QBS differs from traditional compiler optimizations as it relies on synthesis technology to generate invariants and postconditions for a code fragment. The postconditions and invariants are expressed using a new theory of ordered relations that allows us to reason precisely about both the contents and order of the records produced complex code fragments that compute joins and aggregates. The theory is close in expressiveness to SQL, so the synthesized postconditions can be readily translated to SQL queries. Using 75 code fragments automatically extracted from over 120k lines of open-source code written using the Java Hibernate ORM, we demonstrate that our approach can convert a variety of imperative constructs into relational specifications and significantly improve application performance asymptotically by orders of magnitude.}
}

@article{cheung2021new,
  title = {New Directions in Cloud Programming},
  author = {Cheung, Alvin and Crooks, Natacha and Hellerstein, Joseph M and Milano, Matthew},
  year = {2021},
  journal = {arXiv preprint arXiv:2101.01159},
  eprint = {2101.01159},
  abstract = {Nearly twenty years after the launch of AWS, it remains difficult for most developers to harness the enormous potential of the cloud. In this paper we lay out an agenda for a new generation of cloud programming research aimed at bringing research ideas to programmers in an evolutionary fashion. Key to our approach is a separation of distributed programs into a PACT of four facets: Program semantics, Availablity, Consistency and Targets of optimization. We propose to migrate developers gradually to PACT programming by lifting familiar code into our more declarative level of abstraction. We then propose a multi-stage compiler that emits human-readable code at each stage that can be hand-tuned by developers seeking more control. Our agenda raises numerous research challenges across multiple areas including language design, query optimization, transactions, distributed consistency, compilers and program synthesis.},
  archiveprefix = {arxiv}
}

@techreport{childs1968feasibility,
  title = {Feasibility of a Set-Theoretic Data Structure: A General Structure Based on a Reconstituted Definition of Relation},
  author = {Childs, David L},
  year = {1968}
}

@article{chun2007attested,
  title = {Attested Append-Only Memory: {{Making}} Adversaries Stick to Their Word},
  author = {Chun, Byung-Gon and Maniatis, Petros and Shenker, Scott and Kubiatowicz, John},
  year = {2007},
  journal = {ACM SIGOPS Operating Systems Review},
  volume = {41},
  number = {6},
  pages = {189--204},
  publisher = {{ACM New York, NY, USA}},
  abstract = {Researchers have made great strides in improving the fault tolerance of both centralized and replicated systems against arbitrary (Byzantine) faults. However, there are hard limits to how much can be done with entirely untrusted components; for example, replicated state machines cannot tolerate more than a third of their replica population being Byzantine. In this paper, we investigate how minimal trusted abstractions can push through these hard limits in practical ways. We propose Attested Append-Only Memory (A2M), a trusted system facility that is small, easy to implement and easy to verify formally. A2M provides the programming abstraction of a trusted log, which leads to protocol designs immune to equivocation -- the ability of a faulty host to lie in different ways to different clients or servers -- which is a common source of Byzantine headaches. Using A2M, we improve upon the state of the art in Byzantine-fault tolerant replicated state machines, producing A2M-enabled protocols (variants of Castro and Liskov's PBFT) that remain correct (linearizable) and keep making progress (live) even when half the replicas are faulty, in contrast to the previous upper bound. We also present an A2M-enabled single-server shared storage protocol that guarantees linearizability despite server faults. We implement A2M and our protocols, evaluate them experimentally through micro- and macro-benchmarks, and argue that the improved fault tolerance is cost-effective for a broad range of uses, opening up new avenues for practical, more reliable services.}
}

@inproceedings{clark2005live,
  title = {Live Migration of Virtual Machines},
  booktitle = {Proceedings of the 2nd Conference on Symposium on Networked Systems Design \& Implementation-Volume 2},
  author = {Clark, Christopher and Fraser, Keir and Hand, Steven and Hansen, Jacob Gorm and Jul, Eric and Limpach, Christian and Pratt, Ian and Warfield, Andrew},
  year = {2005},
  pages = {273--286}
}

@article{codd1970relational,
  title = {A Relational Model of Data for Large Shared Data Banks},
  author = {Codd, Edgar F},
  year = {1970},
  journal = {Communications of the ACM},
  volume = {13},
  number = {6},
  pages = {377--387},
  publisher = {{ACM New York, NY, USA}},
  abstract = {Future users of large data banks must be protected from having to know how the data is organized in the machine (the internal representation). A prompting service which supplies such information is not a satisfactory solution. Activities of users at terminals and most application programs should remain unaffected when the internal representation of data is changed and even when some aspects of the external representation are changed. Changes in data representation will often be needed as a result of changes in query, update, and report traffic and natural growth in the types of stored information. Existing noninferential, formatted data systems provide users with tree-structured files or slightly more general network models of the data. In Section 1, inadequacies of these models are discussed. A model based on n-ary relations, a normal form for data base relations, and the concept of a universal data sublanguage are introduced. In Section 2, certain operations on relations (other than logical inference) are discussed and applied to the problems of redundancy and consistency in the user's model.},
  file = {/Users/jayithac/Library/CloudStorage/GoogleDrive-jayithareddyp@gmail.com/My Drive/Papers/codd1970relational.pdf}
}

@inproceedings{codd1971data,
  title = {A Data Base Sublanguage Founded on the Relational Calculus},
  booktitle = {Proceedings of the 1971 {{ACM SIGFIDET}} (Now {{SIGMOD}}) Workshop on Data Description, Access and Control},
  author = {Codd, Edgar F},
  year = {1971},
  pages = {35--68},
  abstract = {Three principal types of language for data base manipulation are identified: the low-level, procedure-oriented (typified by the CODASYL-proposed DML), the intermediate level, algebraic (typified by the Project MAC MacAIMS language), and the high level, relational calculus-based data sublanguage, an example of which is described in this paper. The language description is informal and stresses concepts and principles. Following this, arguments are presented for the superiority of the calculus-based type of data base sub-language over the algebraic, and for the algebraic over the low-level procedural. These arguments are particularly relevant to the questions of inter-system compatibility and standardization.}
}

@inproceedings{conway2012logic,
  title = {Logic and Lattices for Distributed Programming},
  booktitle = {Proceedings of the Third {{ACM}} Symposium on Cloud Computing},
  author = {Conway, Neil and Marczak, William R and Alvaro, Peter and Hellerstein, Joseph M and Maier, David},
  year = {2012},
  pages = {1--14},
  abstract = {In recent years there has been interest in achieving application-level consistency criteria without the latency and availability costs of strongly consistent storage infrastructure. A standard technique is to adopt a vocabulary of commutative operations; this avoids the risk of inconsistency due to message reordering. Another approach was recently captured by the CALM theorem, which proves that logically monotonic programs are guaranteed to be eventually consistent. In logic languages such as Bloom, CALM analysis can automatically verify that programs achieve consistency without coordination. In this paper we present BloomL, an extension to Bloom that takes inspiration from both of these traditions. BloomL generalizes Bloom to support lattices and extends the power of CALM analysis to whole programs containing arbitrary lattices. We show how the Bloom interpreter can be generalized to support efficient evaluation of lattice-based code using well-known strategies from logic programming. Finally, we use BloomL to develop several practical distributed programs, including a key-value store similar to Amazon Dynamo, and show how BloomL encourages the safe composition of small, easy-to-analyze lattices into larger programs.}
}

@article{corbett2013spanner,
  title = {Spanner: {{Google}}'s Globally Distributed Database},
  author = {Corbett, James C and Dean, Jeffrey and Epstein, Michael and Fikes, Andrew and Frost, Christopher and Furman, Jeffrey John and Ghemawat, Sanjay and Gubarev, Andrey and Heiser, Christopher and Hochschild, Peter and others},
  year = {2013},
  journal = {ACM Transactions on Computer Systems (TOCS)},
  volume = {31},
  number = {3},
  pages = {1--22},
  publisher = {{ACM New York, NY, USA}},
  abstract = {Spanner is Google's scalable, multiversion, globally distributed, and synchronously replicated database. It is the first system to distribute data at global scale and support externally-consistent distributed transactions. This article describes how Spanner is structured, its feature set, the rationale underlying various design decisions, and a novel time API that exposes clock uncertainty. This API and its implementation are critical to supporting external consistency and a variety of powerful features: nonblocking reads in the past, lock-free snapshot transactions, and atomic schema changes, across all of Spanner.}
}

@article{cormode2017data,
  title = {Data Sketching},
  author = {Cormode, Graham},
  year = {2017},
  journal = {Communications of the ACM},
  volume = {60},
  number = {9},
  pages = {48--55},
  publisher = {{ACM New York, NY, USA}}
}

@inproceedings{crooks2017seeing,
  title = {Seeing Is Believing: {{A}} Client-Centric Specification of Database Isolation},
  booktitle = {Proceedings of the {{ACM}} Symposium on Principles of Distributed Computing},
  author = {Crooks, Natacha and Pu, Youer and Alvisi, Lorenzo and Clement, Allen},
  year = {2017},
  pages = {73--82},
  abstract = {This paper introduces the first state-based formalization of isolation guarantees. Our approach is premised on a simple observation: applications view storage systems as black-boxes that transition through a series of states, a subset of which are observed by applications. Defining isolation guarantees in terms of these states frees definitions from implementation-specific assumptions. It makes immediately clear what anomalies, if any, applications can expect to observe, thus bridging the gap that exists today between how isolation guarantees are defined and how they are perceived. The clarity that results from definitions based on client-observable states brings forth several benefits. First, it allows us to easily compare the guarantees of distinct, but semantically close, isolation guarantees. We find that several well-known guarantees, previously thought to be distinct, are in fact equivalent, and that many previously incomparable flavors of snapshot isolation can be organized in a clean hierarchy. Second, freeing definitions from implementation-specific artefacts can suggest more efficient implementations of the same isolation guarantee. We show how a client-centric implementation of parallel snapshot isolation can be more resilient to slowdown cascades, a common phenomenon in large-scale datacenters.}
}

@article{deanMapReduceSimplifiedData2008,
  title = {{{MapReduce}}: Simplified Data Processing on Large Clusters},
  author = {Dean, Jeffrey and Ghemawat, Sanjay},
  year = {2008},
  journal = {Communications of the ACM},
  volume = {51},
  number = {1},
  pages = {107--113},
  publisher = {{ACM New York, NY, USA}},
  abstract = {MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.},
  keywords = {read,to-note},
  file = {/Users/jayithac/Zotero/storage/9RJU5P3W/Dean and Ghemawat - 2008 - MapReduce simplified data processing on large clu.pdf}
}

@article{decandia2007dynamo,
  title = {Dynamo: {{Amazon}}'s Highly Available Key-Value Store},
  author = {DeCandia, Giuseppe and Hastorun, Deniz and Jampani, Madan and Kakulapati, Gunavardhan and Lakshman, Avinash and Pilchin, Alex and Sivasubramanian, Swaminathan and Vosshall, Peter and Vogels, Werner},
  year = {2007},
  journal = {ACM SIGOPS operating systems review},
  volume = {41},
  number = {6},
  pages = {205--220},
  publisher = {{ACM New York, NY, USA}},
  abstract = {Reliability at massive scale is one of the biggest challenges we face at Amazon.com, one of the largest e-commerce operations in the world; even the slightest outage has significant financial consequences and impacts customer trust. The Amazon.com platform, which provides services for many web sites worldwide, is implemented on top of an infrastructure of tens of thousands of servers and network components located in many datacenters around the world. At this scale, small and large components fail continuously and the way persistent state is managed in the face of these failures drives the reliability and scalability of the software systems. This paper presents the design and implementation of Dynamo, a highly available key-value storage system that some of Amazon's core services use to provide an "always-on" experience. To achieve this level of availability, Dynamo sacrifices consistency under certain failure scenarios. It makes extensive use of object versioning and application-assisted conflict resolution in a manner that provides a novel interface for developers to use.}
}

@article{demers1989analysis,
  title = {Analysis and Simulation of a Fair Queueing Algorithm},
  author = {Demers, Alan and Keshav, Srinivasan and Shenker, Scott},
  year = {1989},
  journal = {ACM SIGCOMM Computer Communication Review},
  volume = {19},
  number = {4},
  pages = {1--12},
  publisher = {{ACM New York, NY, USA}},
  abstract = {We discuss gateway queueing algorithms and their role in controlling congestion in datagram networks. A fair queueing algorithm, based on an earlier suggestion by Nagle, is proposed. Analysis and simulations are used to compare this algorithm to other congestion control schemes. We find that fair queueing provides several important advantages over the usual first-come-first-serve queueing algorithm: fair allocation of bandwidth, lower delay for sources using less than their full share of bandwidth, and protection from ill-behaved sources.}
}

@inproceedings{deshpande2004lifting,
  title = {Lifting the Burden of History from Adaptive Query Processing},
  booktitle = {{{VLDB}}},
  author = {Deshpande, Amol and Hellerstein, Joseph M and others},
  year = {2004},
  pages = {948--959},
  publisher = {{Citeseer}}
}

@article{dewitt1990gamma,
  title = {The {{Gamma}} Database Machine Project},
  author = {DeWitt, D.J. and Ghandeharizadeh, S. and Schneider, D.A. and Bricker, A. and Hsiao, H.-I. and Rasmussen, R.},
  year = {1990},
  month = mar,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {2},
  number = {1},
  pages = {44--62},
  issn = {1558-2191},
  doi = {10.1109/69.50905},
  abstract = {The design of the Gamma database machine and the techniques employed in its implementation are described. Gamma is a relational database machine currently operating on an Intel iPSC/2 hypercube with 32 processors and 32 disk drives. Gamma employs three key technical ideas which enable the architecture to be scaled to hundreds of processors. First, all relations are horizontally partitioned across multiple disk drives, enabling relations to be scanned in parallel. Second, parallel algorithms based on hashing are used to implement the complex relational operators, such as join and aggregate functions. Third, dataflow scheduling techniques are used to coordinate multioperator queries. By using these techniques, it is possible to control the execution of very complex queries with minimal coordination. The design of the Gamma software is described and a thorough performance evaluation of the iPSC/s hypercube version of Gamma is presented.{\textexclamdown}{\textquestiondown}}
}

@misc{DixinHomepage,
  title = {Dixin's {{Homepage}}},
  urldate = {2023-11-15},
  howpublished = {https://people.eecs.berkeley.edu/{\textasciitilde}totemtang/hiring.html}
}

@article{doshi2023kepler,
  title = {Kepler: {{Robust}} Learning for Parametric Query Optimization},
  author = {Doshi, Lyric and Zhuang, Vincent and Jain, Gaurav and Marcus, Ryan and Huang, Haoyu and Altinb{\"u}ken, Deniz and Brevdo, Eugene and Fraser, Campbell},
  year = {2023},
  journal = {Proceedings of the ACM on Management of Data},
  volume = {1},
  number = {1},
  pages = {1--25},
  publisher = {{ACM New York, NY, USA}},
  abstract = {Most existing parametric query optimization (PQO) techniques rely on traditional query optimizer cost models, which are often inaccurate and result in suboptimal query performance. We propose Kepler, an end-to-end learning-based approach to PQO that demonstrates significant speedups in query latency over a traditional query optimizer. Central to our method is Row Count Evolution (RCE), a novel plan generation algorithm based on perturbations in the sub-plan cardinality space. While previous approaches require accurate cost models, we bypass this requirement by evaluating candidate plans via actual execution data and training anML model to predict the fastest plan given parameter binding values. Our models leverage recent advances in neural network uncertainty in order to robustly predict faster plans while avoiding regressions in query performance. Experimentally, we show that Kepler achieves significant improvements in query runtime on multiple datasets on PostgreSQL.}
}

@book{doxsey2016introducing,
  title = {Introducing Go: {{Build}} Reliable, Scalable Programs},
  author = {Doxsey, Caleb},
  year = {2016},
  publisher = {{" O'Reilly Media, Inc."}},
  abstract = {Perfect for beginners familiar with programming basics, this hands-on guide provides an easy introduction to Go, the general-purpose programming language from Google. Author Caleb Doxsey covers the language's core features with step-by-step instructions and exercises in each chapter to help you practice what you learn. Go is a general-purpose programming language with a clean syntax and advanced features, including concurrency. This book provides the one-on-one support you need to get started with the language, with short, easily digestible chapters that build on one another. By the time you finish this book, not only will you be able to write real Go programs, you'll be ready to tackle advanced techniques. Jump into Go basics, including data types, variables, and control structures Learn complex types, such as slices, functions, structs, and interfaces Explore Go's core library and learn how to create your own package Write tests for your code by using the language's go test program Learn how to run programs concurrently with goroutines and channels Get suggestions to help you master the craft of programming}
}

@article{dunlap2002revirt,
  title = {{{ReVirt}}: {{Enabling}} Intrusion Analysis through Virtual-Machine Logging and Replay},
  author = {Dunlap, George W and King, Samuel T and Cinar, Sukru and Basrai, Murtaza A and Chen, Peter M},
  year = {2002},
  journal = {ACM SIGOPS Operating Systems Review},
  volume = {36},
  number = {SI},
  pages = {211--224},
  publisher = {{ACM New York, NY, USA}},
  abstract = {Current system loggers have two problems: they depend on the integrity of the operating system being logged, and they do not save sufficient information to replay and analyze attacks that include any non-deterministic events. ReVirt removes the dependency on the target operating system by moving it into a virtual machine and logging below the virtual machine. This allows ReVirt to replay the system's execution before, during, and after an intruder compromises the system, even if the intruder replaces the target operating system. ReVirt logs enough information to replay a long-term execution of the virtual machine instruction-by-instruction. This enables it to provide arbitrarily detailed observations about what transpired on the system, even in the presence of non-deterministic attacks and executions. ReVirt adds reasonable time and space overhead. Overheads due to virtualization are imperceptible for interactive use and CPU-bound workloads, and 13--58\% for kernel-intensive workloads. Logging adds 0--8\% overhead, and logging traffic for our workloads can be stored on a single disk for several months.}
}

@article{efstathopoulos2005labels,
  title = {Labels and Event Processes in the {{Asbestos}} Operating System},
  author = {Efstathopoulos, Petros and Krohn, Maxwell and VanDeBogart, Steve and Frey, Cliff and Ziegler, David and Kohler, Eddie and Mazieres, David and Kaashoek, Frans and Morris, Robert},
  year = {2005},
  journal = {ACM SIGOPS Operating Systems Review},
  volume = {39},
  number = {5},
  pages = {17--30},
  publisher = {{ACM New York, NY, USA}},
  abstract = {Asbestos, a new prototype operating system, provides novel labeling and isolation mechanisms that help contain the effects of exploitable software flaws. Applications can express a wide range of policies with Asbestos's kernel-enforced label mechanism, including controls on inter-process communication and system-wide information flow. A new event process abstraction provides lightweight, isolated contexts within a single process, allowing the same process to act on behalf of multiple users while preventing it from leaking any single user's data to any other user. A Web server that uses Asbestos labels to isolate user data requires about 1.5 memory pages per user, demonstrating that additional security can come at an acceptable cost.}
}

@article{engler1995exokernel,
  title = {Exokernel: {{An}} Operating System Architecture for Application-Level Resource Management},
  author = {Engler, Dawson R and Kaashoek, M Frans and O'Toole Jr, James},
  year = {1995},
  journal = {ACM SIGOPS Operating Systems Review},
  volume = {29},
  number = {5},
  pages = {251--266},
  publisher = {{ACM New York, NY, USA}}
}

@inproceedings{faleiro2017latch,
  title = {Latch-Free Synchronization in Database Systems: {{Silver}} Bullet or Fool's Gold?},
  booktitle = {{{CIDR}} (Conference on Innovative Data Systems Research)},
  author = {Faleiro, Jose M and Abadi, Daniel J},
  year = {2017},
  abstract = {Recent research on multi-core database architectures has made the argument that, when possible, database systems should abandon the use of latches in favor of latch-free algorithms. Latch-based algorithms are thought to scale poorly due to their use of synchronization based on mutual exclusion. In contrast, latch-free algorithms make strong theoretical guarantees which ensure that the progress of a thread is never impeded due to the delay or failure of other threads. In this paper, we analyze the various factors that influence the performance and scalability of latch-free and latch-based algorithms, and perform a microbenchmark evaluation of latch-free and latch-based synchronization algorithms. Our findings indicate that the argument for latch-free algorithms' superior scalability is far more nuanced than the current state-of-the-art in multi-core database architectures suggests.}
}

@misc{FloatingPointGuideWhat,
  title = {The {{Floating-Point Guide}} - {{What Every Programmer Should Know About Floating-Point Arithmetic}}},
  urldate = {2023-11-08},
  howpublished = {https://floating-point-gui.de/},
  file = {/Users/jayithac/Zotero/storage/V8MNAZ4R/floating-point-gui.de.html}
}

@article{freitag2020adopting,
  title = {Adopting Worst-Case Optimal Joins in Relational Database Systems},
  author = {Freitag, Michael and Bandle, Maximilian and Schmidt, Tobias and Kemper, Alfons and Neumann, Thomas},
  year = {2020},
  journal = {Proceedings of the VLDB Endowment},
  volume = {13},
  number = {12},
  pages = {1891--1904},
  publisher = {{VLDB Endowment}},
  abstract = {Worst-case optimal join algorithms are attractive from a theoretical point of view, as they offer asymptotically better runtime than binary joins on certain types of queries. In particular, they avoid enumerating large intermediate results by processing multiple input relations in a single multi-way join. However, existing implementations incur a sizable overhead in practice, primarily since they rely on suitable ordered index structures on their input. Systems that support worst-case optimal joins often focus on a specific problem domain, such as read-only graph analytic queries, where extensive precomputation allows them to mask these costs. In this paper, we present a comprehensive implementation approach for worst-case optimal joins that is practical within general-purpose relational database management systems supporting both hybrid transactional and analytical workloads. The key component of our approach is a novel hash-based worst-case optimal join algorithm that relies only on data structures that can be built efficiently during query execution. Furthermore, we implement a hybrid query optimizer that intelligently and transparently combines both binary and multi-way joins within the same query plan. We demonstrate that our approach far outperforms existing systems when worst-case optimal joins are beneficial while sacrificing no performance when they are not.}
}

@article{gervasi2019will,
  title = {Will Carbon Nanotube Memory Replace {{DRAM}}?},
  author = {Gervasi, Bill},
  year = {2019},
  journal = {IEEE Micro},
  volume = {39},
  number = {2},
  pages = {45--51},
  publisher = {{IEEE}},
  abstract = {In this paper, we discuss an exciting memory technology made from carbon nanotubes. Carbon nanotubes provide a predictable resistive element that can be used to fabricate very dense and very fast-switching memory cells. Nantero NRAM employs electrostatic forces to connect and disconnect these nanotubes in a memory design notably impervious to external effects including heat, shock and vibration, magnetism, and radiation. NRAM maintains its state permanently and may be rewritten arbitrarily many times without degrading. Not only NRAM is well positioned to replace DRAM in existing applications, but also its combination of high speed, persistence, density, and low power enables a slew of exciting new applications. Production of NRAM devices is on track for near-term commercialization through Nantero licensees.}
}

@inproceedings{ghemawat2003google,
  title = {The Google File System},
  booktitle = {Proceedings of the Nineteenth {{ACM}} Symposium on {{Operating}} Systems Principles},
  author = {Ghemawat, Sanjay and Gobioff, Howard and Leung, Shun-Tak},
  year = {2003},
  pages = {29--43}
}

@inproceedings{ghodsi2011dominant,
  title = {Dominant Resource Fairness: {{Fair}} Allocation of Multiple Resource Types},
  booktitle = {8th {{USENIX}} Symposium on Networked Systems Design and Implementation ({{NSDI}} 11)},
  author = {Ghodsi, Ali and Zaharia, Matei and Hindman, Benjamin and Konwinski, Andy and Shenker, Scott and Stoica, Ion},
  year = {2011}
}

@article{goossens2003priority,
  title = {Priority-Driven Scheduling of Periodic Task Systems on Multiprocessors},
  author = {Goossens, Jo{\"e}l and Funk, Shelby and Baruah, Sanjoy},
  year = {2003},
  journal = {Real-time systems},
  volume = {25},
  pages = {187--205},
  publisher = {{Springer}},
  abstract = {The scheduling of systems of periodic tasks upon multiprocessor platforms is considered. Utilization-based conditions are derived for determining whether a periodic task system meets all deadlines when scheduled using the earliest deadline first scheduling algorithm (EDF) upon a given multiprocessor platform. A new priority-driven algorithm is proposed for scheduling periodic task systems upon multiprocessor platforms: this algorithm is shown to successfully schedule some task systems for which EDF may fail to meet all deadlines.}
}

@article{graefe2012survey,
  title = {A Survey of {{B-tree}} Logging and Recovery Techniques},
  author = {Graefe, Goetz},
  year = {2012},
  journal = {ACM Transactions on Database Systems (TODS)},
  volume = {37},
  number = {1},
  pages = {1--35},
  publisher = {{ACM New York, NY, USA}},
  abstract = {B-trees have been ubiquitous in database management systems for several decades, and they serve in many other storage systems as well. Their basic structure and their basic operations are well understood including search, insertion, and deletion. However, implementation of transactional guarantees such as all-or-nothing failure atomicity and durability in spite of media and system failures seems to be difficult. High-performance techniques such as pseudo-deleted records, allocation-only logging, and transaction processing during crash recovery are widely used in commercial B-tree implementations but not widely understood. This survey collects many of these techniques as a reference for students, researchers, system architects, and software developers. Central in this discussion are physical data independence, separation of logical database contents and physical representation, and the concepts of user transactions and system transactions. Many of the techniques discussed are applicable beyond B-trees.}
}

@article{gray1976granularity,
  title = {Granularity of Locks and Degrees of Consistency},
  author = {Gray, R Lorie J and Putzolu, {\relax GF} and Traiger, {\relax IL}},
  year = {1976},
  journal = {Modeling in Data Base Management Systems, GM Nijssen ed., North Holland Pub},
  file = {/Users/jayithac/Library/CloudStorage/GoogleDrive-jayithareddyp@gmail.com/My Drive/Papers/gray1976granularity.pdf}
}

@inproceedings{gray1981transaction,
  title = {The Transaction Concept: {{Virtues}} and Limitations},
  booktitle = {{{VLDB}}},
  author = {Gray, Jim and others},
  year = {1981},
  volume = {81},
  pages = {144--154}
}

@book{gray1984logic,
  title = {Logic, Algebra and Databases},
  author = {Gray, Peter},
  year = {1984},
  publisher = {{John Wiley \& Sons, Inc.}}
}

@inproceedings{gueta2019sbft,
  title = {{{SBFT}}: {{A}} Scalable and Decentralized Trust Infrastructure},
  booktitle = {2019 49th {{Annual IEEE}}/{{IFIP}} International Conference on Dependable Systems and Networks ({{DSN}})},
  author = {Gueta, Guy Golan and Abraham, Ittai and Grossman, Shelly and Malkhi, Dahlia and Pinkas, Benny and Reiter, Michael and Seredinschi, Dragos-Adrian and Tamir, Orr and Tomescu, Alin},
  year = {2019},
  pages = {568--580},
  publisher = {{IEEE}},
  abstract = {SBFT is a state of the art Byzantine fault tolerant state machine replication system that addresses the challenges of scalability, decentralization and global geo-replication. SBFT is optimized for decentralization and is experimentally evaluated on a deployment of more than 200 active replicas withstanding a malicious adversary controlling f=64 replicas. Our experiments show how the different algorithmic ingredients of SBFT contribute to its performance and scalability. The results show that SBFT simultaneously provides almost 2x better throughput and about 1.5x better latency relative to a highly optimized system that implements the PBFT protocol. To achieve this performance improvement, SBFT uses a combination of four ingredients: using collectors and threshold signatures to reduce communication to linear, using an optimistic fast path, reducing client communication and utilizing redundant servers for the fast path. SBFT is the first system to implement a correct dual-mode view change protocol that allows to efficiently run either an optimistic fast path or a fallback slow path without incurring a view change to switch between modes.}
}

@inproceedings{ha2017you,
  title = {You Can Teach Elephants to Dance: {{Agile VM}} Handoff for Edge Computing},
  booktitle = {Proceedings of the Second {{ACM}}/{{IEEE}} Symposium on Edge Computing},
  author = {Ha, Kiryong and Abe, Yoshihisa and Eiszler, Thomas and Chen, Zhuo and Hu, Wenlu and Amos, Brandon and Upadhyaya, Rohit and Pillai, Padmanabhan and Satyanarayanan, Mahadev},
  year = {2017},
  pages = {1--14},
  abstract = {M handoff enables rapid and transparent placement changes to executing code in edge computing use cases where the safety and management attributes of VM encapsulation are important. This versatile primitive offers the functionality of classic live migration but is highly optimized for the edge. Over WAN bandwidths ranging from 5 to 25 Mbps, VM handoff migrates a running 8 GB VM in about a minute, with a downtime of a few tens of seconds. By dynamically adapting to varying network bandwidth and processing load, VM handoff is more than an order of magnitude faster than live migration at those bandwidths.}
}

@article{haerder1983principles,
  title = {Principles of Transaction-Oriented Database Recovery},
  author = {Haerder, Theo and Reuter, Andreas},
  year = {1983},
  journal = {ACM computing surveys (CSUR)},
  volume = {15},
  number = {4},
  pages = {287--317},
  publisher = {{ACM New York, NY, USA}}
}

@inproceedings{hand2005virtual,
  title = {Are Virtual Machine Monitors Microkernels Done Right?},
  booktitle = {{{HotOS}}},
  author = {Hand, Steven and Warfield, Andrew and Fraser, Keir and Kotsovinos, Evangelos and Magenheimer, Daniel J},
  year = {2005}
}

@article{helland2009building,
  title = {Building on Quicksand},
  author = {Helland, Pat and Campbell, David},
  year = {2009},
  journal = {arXiv preprint arXiv:0909.1788},
  eprint = {0909.1788},
  abstract = {Reliable systems have always been built out of unreliable components. Early on, the reliable components were small such as mirrored disks or ECC (Error Correcting Codes) in core memory. These systems were designed such that failures of these small components were transparent to the application. Later, the size of the unreliable components grew larger and semantic challenges crept into the application when failures occurred. As the granularity of the unreliable component grows, the latency to communicate with a backup becomes unpalatable. This leads to a more relaxed model for fault tolerance. The primary system will acknowledge the work request and its actions without waiting to ensure that the backup is notified of the work. This improves the responsiveness of the system. There are two implications of asynchronous state capture: 1) Everything promised by the primary is probabilistic. There is always a chance that an untimely failure shortly after the promise results in a backup proceeding without knowledge of the commitment. Hence, nothing is guaranteed! 2) Applications must ensure eventual consistency. Since work may be stuck in the primary after a failure and reappear later, the processing order for work cannot be guaranteed. Platform designers are struggling to make this easier for their applications. Emerging patterns of eventual consistency and probabilistic execution may soon yield a way for applications to express requirements for a "looser" form of consistency while providing availability in the face of ever larger failures. This paper recounts portions of the evolution of these trends, attempts to show the patterns that span these changes, and talks about future directions as we continue to "build on quicksand".},
  archiveprefix = {arxiv}
}

@inproceedings{hellerstein1997online,
  title = {Online Aggregation},
  booktitle = {Proceedings of the 1997 {{ACM SIGMOD}} International Conference on {{Management}} of Data},
  author = {Hellerstein, Joseph M and Haas, Peter J and Wang, Helen J},
  year = {1997},
  pages = {171--182},
  abstract = {Aggregation in traditional database systems is performed in batch mode: a query is submitted, the system processes a large volume of data over a long period of time, and, eventually, the final answer is returned. This archaic approach is frustrating to users and has been abandoned in most other areas of computing. In this paper we propose a new online aggregation interface that permits users to both observe the progress of their aggregation queries and control execution on the fly. After outlining usability and performance requirements for a system supporting online aggregation, we present a suite of techniques that extend a database system to meet these requirements. These include methods for returning the output in random order, for providing control over the relative rate at which different aggregates are computed, and for computing running confidence intervals. Finally, we report on an initial implementation of online aggregation in POSTGRES.}
}

@article{hellerstein1999interactive,
  title = {Interactive Data Analysis: The {{Control}} Project},
  author = {Hellerstein, J.M. and Avnur, R. and Chou, A. and Hidber, C. and Olston, C. and Raman, V. and Roth, T. and Haas, P.J.},
  year = {1999},
  journal = {Computer},
  volume = {32},
  number = {8},
  pages = {51--59},
  doi = {10.1109/2.781635},
  abstract = {Data analysis is fundamentally an iterative process in which you issue a query, receive a response, formulate the next query based on the response, and repeat. You usually don't issue a single, perfectly chosen query and get the information you want from a database; indeed, the purpose of data analysis is to extract unknown information, and in most situations, there is no one perfect query. People naturally start by asking broad, big-picture questions and then continually refine their questions based on feedback and domain knowledge. In the Control (Continuous Output and Navigation Technology with Refinement Online) project at the University of California, Berkeley, the authors are working with collaborators at IBM, Informix, and elsewhere to explore ways to improve human-computer interaction during data analysis. The Control project's goal is to develop interactive, intuitive techniques for analyzing massive data sets.}
}

@incollection{hellerstein2018looking,
  title = {Looking Back at Postgres},
  booktitle = {Making Databases Work: The Pragmatic Wisdom of Michael Stonebraker},
  author = {Hellerstein, Joseph M},
  year = {2018},
  pages = {205--224},
  abstract = {This is a recollection of the UC Berkeley Postgres project, which was led by Mike Stonebraker from the mid-1980's to the mid-1990's. The article was solicited for Stonebraker's Turing Award book, as one of many personal/historical recollections. As a result it focuses on Stonebraker's design ideas and leadership. But Stonebraker was never a coder, and he stayed out of the way of his development team. The Postgres codebase was the work of a team of brilliant students and the occasional university "staff programmers" who had little more experience (and only slightly more compensation) than the students. I was lucky to join that team as a student during the latter years of the project. I got helpful input on this writeup from some of the more senior students on the project, but any errors or omissions are mine. If you spot any such, please contact me and I will try to fix them.}
}

@article{hellerstein2020keeping,
  title = {Keeping {{CALM}}: When Distributed Consistency Is Easy},
  author = {Hellerstein, Joseph M and Alvaro, Peter},
  year = {2020},
  journal = {Communications of the ACM},
  volume = {63},
  number = {9},
  pages = {72--81},
  publisher = {{ACM New York, NY, USA}}
}

@article{hellersteinArchitectureDatabaseSystem2007,
  title = {Architecture of a {{Database System}}},
  author = {Hellerstein, Joseph M. and Stonebraker, Michael and Hamilton, James},
  year = {2007},
  journal = {Foundations and Trends{\textregistered} in Databases},
  volume = {1},
  number = {2},
  pages = {141--259},
  issn = {1931-7883, 1931-7891},
  doi = {10.1561/1900000002},
  urldate = {2023-07-04},
  abstract = {Database Management Systems (DBMSs) are a ubiquitous and critical component of modern computing, and the result of decades of research and development in both academia and industry. Historically, DBMSs were among the earliest multi-user server systems to be developed, and thus pioneered many systems design techniques for scalability and reliability now in use in many other contexts. While many of the algorithms and abstractions used by a DBMS are textbook material, there has been relatively sparse coverage in the literature of the systems design issues that make a DBMS work. This paper presents an architectural discussion of DBMS design principles, including process models, parallel architecture, storage system design, transaction system implementation, query processor and optimizer architectures, and typical shared components and utilities. Successful commercial and open-source systems are used as points of reference, particularly when multiple alternative designs have been adopted by different groups.},
  langid = {english},
  file = {/Users/jayithac/Zotero/storage/EF4ANJER/Hellerstein et al. - 2007 - Architecture of a Database System.pdf}
}

@article{hellwig2009xfs,
  title = {{{XFS}}: The Big Storage File System for {{Linux}}},
  author = {Hellwig, Christoph},
  year = {2009},
  journal = {; login:: the magazine of USENIX \& SAGE},
  volume = {34},
  number = {5},
  pages = {10--18},
  publisher = {{USENIX Association}}
}

@inproceedings{hentschel2018column,
  title = {Column Sketches: {{A}} Scan Accelerator for Rapid and Robust Predicate Evaluation},
  booktitle = {Proceedings of the 2018 International Conference on Management of Data},
  author = {Hentschel, Brian and Kester, Michael S and Idreos, Stratos},
  year = {2018},
  pages = {857--872},
  abstract = {While numerous indexing and storage schemes have been developed to address the core functionality of predicate evaluation in data systems, they all require specific workload properties (query selectivity, data distribution, data clustering) to provide good performance and fail in other cases. We present a new class of indexing scheme, termed a Column Sketch, which improves the performance of predicate evaluation independently of workload properties. Column Sketches work primarily through the use of lossy compression schemes which are designed so that the index ingests data quickly, evaluates any query performantly, and has small memory footprint. A Column Sketch works by applying this lossy compression on a value-by-value basis, mapping base data to a representation of smaller fixed width codes. Queries are evaluated affirmatively or negatively for the vast majority of values using the compressed data, and only if needed check the base data for the remaining values. Column Sketches work over column, row, and hybrid storage layouts. We demonstrate that by using a Column Sketch, the select operator in modern analytic systems attains better CPU efficiency and less data movement than state-of-the-art storage and indexing schemes. Compared to standard scans, Column Sketches provide an improvement of 3x-6x for numerical attributes and 2.7x for categorical attributes. Compared to state-of-the-art scan accelerators such as Column Imprints and BitWeaving, Column Sketches perform 1.4 - 4.8{\texttimes} better.}
}

@article{huang2020opportunities,
  title = {Opportunities for Optimism in Contended Main-Memory Multicore Transactions},
  author = {Huang, Yihe and Qian, William and Kohler, Eddie and Liskov, Barbara and Shrira, Liuba},
  year = {2020},
  publisher = {{VLDB Endowment}},
  abstract = {Optimistic concurrency control, or OCC, can achieve excellent performance on uncontended workloads for main-memory transactional databases. Contention causes OCC's performance to degrade, however, and recent concurrency control designs, such as hybrid OCC/locking systems and variations on multiversion concurrency control (MVCC), have claimed to outperform the best OCC systems. We evaluate several concurrency control designs under varying contention and varying workloads, including TPCC, and find that implementation choices unrelated to concurrency control may explain much of OCC's previously-reported degradation. When these implementation choices are made sensibly, OCC performance does not collapse on high-contention TPC-C. We also present two optimization techniques, commit-time updates and timestamp splitting, that can dramatically improve the highcontention performance of both OCC and MVCC. Though these techniques are known, we apply them in a new context and highlight their potency: when combined, they lead to performance gains of 3:4 for MVCC and 3:6 for OCC in a TPC-C workload.}
}

@misc{IdiomaticRustDevs,
  title = {Idiomatic {{Rust}} (for {{C}}++ {{Devs}}): {{Constructors}} \& {{Conversions}} {\textendash} {{Geo}}'s {{Notepad}} {\textendash} {{Mostly Programming}} and {{Math}}},
  urldate = {2023-11-28},
  howpublished = {https://geo-ant.github.io/blog/2023/rust-for-cpp-developers-constructors/}
}

@inproceedings{idreos2007database,
  title = {Database Cracking.},
  booktitle = {{{CIDR}}},
  author = {Idreos, Stratos and Kersten, Martin L and Manegold, Stefan and others},
  year = {2007},
  volume = {7},
  pages = {68--78}
}

@book{ierusalimschy2006programming,
  title = {Programming in Lua},
  author = {Ierusalimschy, Roberto},
  year = {2006},
  publisher = {{Roberto Ierusalimschy}}
}

@inproceedings{kandel2011wrangler,
  title = {Wrangler: {{Interactive}} Visual Specification of Data Transformation Scripts},
  booktitle = {Proceedings of the Sigchi Conference on Human Factors in Computing Systems},
  author = {Kandel, Sean and Paepcke, Andreas and Hellerstein, Joseph and Heer, Jeffrey},
  year = {2011},
  pages = {3363--3372},
  abstract = {Though data analysis tools continue to improve, analysts still expend an inordinate amount of time and effort manipulating data and assessing data quality issues. Such "data wrangling" regularly involves reformatting data values or layout, correcting erroneous or missing values, and integrating multiple data sources. These transforms are often difficult to specify and difficult to reuse across analysis tasks, teams, and tools. In response, we introduce Wrangler, an interactive system for creating data transformations. Wrangler combines direct manipulation of visualized data with automatic inference of relevant transforms, enabling analysts to iteratively explore the space of applicable operations and preview their effects. Wrangler leverages semantic data types (e.g., geographic locations, dates, classification codes) to aid validation and type conversion. Interactive histories support review, refinement, and annotation of transformation scripts. User study results show that Wrangler significantly reduces specification time and promotes the use of robust, auditable transforms instead of manual editing.}
}

@article{kerstenEverythingYouAlways2018,
  title = {Everything You Always Wanted to Know about Compiled and Vectorized Queries but Were Afraid to Ask},
  author = {Kersten, Timo and Leis, Viktor and Kemper, Alfons and Neumann, Thomas and Pavlo, Andrew and Boncz, Peter},
  year = {2018},
  journal = {Proceedings of the VLDB Endowment},
  volume = {11},
  number = {13},
  pages = {2209--2222},
  publisher = {{VLDB Endowment}},
  abstract = {The query engines of most modern database systems are either based on vectorization or data-centric code generation. These two state-of-the-art query processing paradigms are fundamentally different in terms of system structure and query execution code. Both paradigms were used to build fast systems. However, until today it is not clear which paradigm yields faster query execution, as many implementation-specific choices obstruct a direct comparison of architectures. In this paper, we experimentally compare the two models by implementing both within the same test system. This allows us to use for both models the same query processing algorithms, the same data structures, and the same parallelization framework to ultimately create an apples-to-apples comparison. We find that both are efficient, but have different strengths and weaknesses. Vectorization is better at hiding cache miss latency, whereas data-centric compilation requires fewer CPU instructions, which benefits cache-resident workloads. Besides raw, single-threaded performance, we also investigate SIMD as well as multi-core parallelization and different hardware architectures. Finally, we analyze qualitative differences as a guide for system architects.}
}

@article{keshav2007read,
  title = {How to Read a Paper},
  author = {Keshav, Srinivasan},
  year = {2007},
  journal = {ACM SIGCOMM Computer Communication Review},
  volume = {37},
  number = {3},
  pages = {83--84},
  publisher = {{ACM New York, NY, USA}},
  abstract = {Researchers spend a great deal of time reading research papers. However, this skill is rarely taught, leading to much wasted effort. This article outlines a practical and efficient three-pass method for reading research papers. I also describe how to use this method to do a literature survey.},
  file = {/Users/jayithac/Library/CloudStorage/GoogleDrive-jayithareddyp@gmail.com/My Drive/Papers/keshav2007read.pdf}
}

@article{kipf2018learned,
  title = {Learned Cardinalities: {{Estimating}} Correlated Joins with Deep Learning},
  author = {Kipf, Andreas and Kipf, Thomas and Radke, Bernhard and Leis, Viktor and Boncz, Peter and Kemper, Alfons},
  year = {2018},
  journal = {arXiv preprint arXiv:1809.00677},
  eprint = {1809.00677},
  abstract = {We describe a new deep learning approach to cardinality estimation. MSCN is a multi-set convolutional network, tailored to representing relational query plans, that employs set semantics to capture query features and true cardinalities. MSCN builds on sampling-based estimation, addressing its weaknesses when no sampled tuples qualify a predicate, and in capturing join-crossing correlations. Our evaluation of MSCN using a real-world dataset shows that deep learning significantly enhances the quality of cardinality estimation, which is the core problem in query optimization.},
  archiveprefix = {arxiv}
}

@inproceedings{klein2009sel4,
  title = {{{seL4}}: {{Formal}} Verification of an {{OS}} Kernel},
  booktitle = {Proceedings of the {{ACM SIGOPS}} 22nd Symposium on {{Operating}} Systems Principles},
  author = {Klein, Gerwin and Elphinstone, Kevin and Heiser, Gernot and Andronick, June and Cock, David and Derrin, Philip and Elkaduwe, Dhammika and Engelhardt, Kai and Kolanski, Rafal and Norrish, Michael and others},
  year = {2009},
  pages = {207--220},
  abstract = {Complete formal verification is the only known way to guarantee that a system is free of programming errors. We present our experience in performing the formal, machine-checked verification of the seL4 microkernel from an abstract specification down to its C implementation. We assume correctness of compiler, assembly code, and hardware, and we used a unique design approach that fuses formal and operating systems techniques. To our knowledge, this is the first formal proof of functional correctness of a complete, general-purpose operating-system kernel. Functional correctness means here that the implementation always strictly follows our high-level abstract specification of kernel behaviour. This encompasses traditional design and implementation safety properties such as the kernel will never crash, and it will never perform an unsafe operation. It also proves much more: we can predict precisely how the kernel will behave in every possible situation. seL4, a third-generation microkernel of L4 provenance, comprises 8,700 lines of C code and 600 lines of assembler. Its performance is comparable to other high-performance L4 kernels.}
}

@article{klein2014comprehensive,
  title = {Comprehensive Formal Verification of an {{OS}} Microkernel},
  author = {Klein, Gerwin and Andronick, June and Elphinstone, Kevin and Murray, Toby and Sewell, Thomas and Kolanski, Rafal and Heiser, Gernot},
  year = {2014},
  journal = {ACM Transactions on Computer Systems (TOCS)},
  volume = {32},
  number = {1},
  pages = {1--70},
  publisher = {{ACM New York, NY, USA}},
  abstract = {We present an in-depth coverage of the comprehensive machine-checked formal verification of seL4, a general-purpose operating system microkernel. We discuss the kernel design we used to make its verification tractable. We then describe the functional correctness proof of the kernel's C implementation and we cover further steps that transform this result into a comprehensive formal verification of the kernel: a formally verified IPC fastpath, a proof that the binary code of the kernel correctly implements the C semantics, a proof of correct access-control enforcement, a proof of information-flow noninterference, a sound worst-case execution time analysis of the binary, and an automatic initialiser for user-level systems that connects kernel-level access-control enforcement with reasoning about system behaviour. We summarise these results and show how they integrate to form a coherent overall analysis, backed by machine-checked, end-to-end theorems. The seL4 microkernel is currently not just the only general-purpose operating system kernel that is fully formally verified to this degree. It is also the only example of formal proof of this scale that is kept current as the requirements, design and implementation of the system evolve over almost a decade. We report on our experience in maintaining this evolving formally verified code base.}
}

@article{kocher2020spectre,
  title = {Spectre Attacks: {{Exploiting}} Speculative Execution},
  author = {Kocher, Paul and Horn, Jann and Fogh, Anders and Genkin, Daniel and Gruss, Daniel and Haas, Werner and Hamburg, Mike and Lipp, Moritz and Mangard, Stefan and Prescher, Thomas and others},
  year = {2020},
  journal = {Communications of the ACM},
  volume = {63},
  number = {7},
  pages = {93--101},
  publisher = {{ACM New York, NY, USA}},
  abstract = {Modern processors use branch prediction and speculative execution to maximize performance. For example, if the destination of a branch depends on a memory value that is in the process of being read, CPUs will try to guess the destination and attempt to execute ahead. When the memory value finally arrives, the CPU either discards or commits the speculative computation. Speculative logic is unfaithful in how it executes, can access the victim's memory and registers, and can perform operations with measurable side effects. Spectre attacks involve inducing a victim to speculatively perform operations that would not occur during correct program execution and which leak the victim's confidential information via a side channel to the adversary. This paper describes practical attacks that combine methodology from side-channel attacks, fault attacks, and return-oriented programming that can read arbitrary memory from the victim's process. More broadly, the paper shows that speculative execution implementations violate the security assumptions underpinning numerous software security mechanisms, such as operating system process separation, containerization, just-in-time (JIT) compilation, and countermeasures to cache timing and side-channel attacks. These attacks represent a serious threat to actual systems because vulnerable speculative execution capabilities are found in microprocessors from Intel, AMD, and ARM that are used in billions of devices. Although makeshift processor-specific countermeasures are possible in some cases, sound solutions will require fixes to processor designs as well as updates to instruction set architectures (ISAs) to give hardware architects and software developers a common understanding as to what computation state CPU implementations are (and are not) permitted to leak.}
}

@article{kornacker1997concurrency,
  title = {Concurrency and Recovery in Generalized Search Trees},
  author = {Kornacker, Marcel and Mohan, C and Hellerstein, Joseph M},
  year = {1997},
  journal = {ACM SIGMOD Record},
  volume = {26},
  number = {2},
  pages = {62--72},
  publisher = {{ACM New York, NY, USA}},
  abstract = {This paper presents general algorithms for concurrency control in tree-based access methods as well as a recovery protocol and a mechanism for ensuring repeatable read. The algorithms are developed in the context of the Generalized Search Tree (GiST) data structure, an index structure supporting an extensible set of queries and data types. Although developed in a GiST context, the algorithms are generally applicable to many tree-based access methods. The concurrency control protocol is based on an extension of the link technique originally developed for B-trees, and completely avoids holding node locks during I/Os. Repeatable read isolation is achieved with a novel combination of predicate locks and two-phase locking of data records. To our knowledge, this is the first time that isolation issues have been addressed outside the context of B-trees. A discussion of the fundamental structural differences between B-trees and more general tree structures like GiSTs explains why the algorithms developed here deviate from their B-tree counterparts. An implementation of GiSTs emulating B-trees in DB2/Common Server is underway.}
}

@inproceedings{kraska2018case,
  title = {The Case for Learned Index Structures},
  booktitle = {Proceedings of the 2018 International Conference on Management of Data},
  author = {Kraska, Tim and Beutel, Alex and Chi, Ed H and Dean, Jeffrey and Polyzotis, Neoklis},
  year = {2018},
  pages = {489--504},
  abstract = {Indexes are models: a {\textbackslash}btree-Index can be seen as a model to map a key to the position of a record within a sorted array, a Hash-Index as a model to map a key to a position of a record within an unsorted array, and a BitMap-Index as a model to indicate if a data record exists or not. In this exploratory research paper, we start from this premise and posit that all existing index structures can be replaced with other types of models, including deep-learning models, which we term {\textbackslash}em learned indexes. We theoretically analyze under which conditions learned indexes outperform traditional index structures and describe the main challenges in designing learned index structures. Our initial results show that our learned indexes can have significant advantages over traditional indexes. More importantly, we believe that the idea of replacing core components of a data management system through learned models has far reaching implications for future systems designs and that this work provides just a glimpse of what might be possible.}
}

@article{krishnan2016activeclean,
  title = {Activeclean: {{Interactive}} Data Cleaning for Statistical Modeling},
  author = {Krishnan, Sanjay and Wang, Jiannan and Wu, Eugene and Franklin, Michael J and Goldberg, Ken},
  year = {2016},
  journal = {Proceedings of the VLDB Endowment},
  volume = {9},
  number = {12},
  pages = {948--959},
  publisher = {{VLDB Endowment}},
  abstract = {Analysts often clean dirty data iteratively--cleaning some data, executing the analysis, and then cleaning more data based on the results. We explore the iterative cleaning process in the context of statistical model training, which is an increasingly popular form of data analytics. We propose ActiveClean, which allows for progressive and iterative cleaning in statistical modeling problems while preserving convergence guarantees. ActiveClean supports an important class of models called convex loss models (e.g., linear regression and SVMs), and prioritizes cleaning those records likely to affect the results. We evaluate ActiveClean on five real-world datasets UCI Adult, UCI EEG, MNIST, IMDB, and Dollars For Docs with both real and synthetic errors. The results show that our proposed optimizations can improve model accuracy by up-to 2.5x for the same amount of data cleaned. Furthermore for a fixed cleaning budget and on all real dirty datasets, ActiveClean returns more accurate models than uniform sampling and Active Learning.}
}

@inproceedings{kristo2020case,
  title = {The Case for a Learned Sorting Algorithm},
  booktitle = {Proceedings of the 2020 {{ACM SIGMOD}} International Conference on Management of Data},
  author = {Kristo, Ani and Vaidya, Kapil and {\c C}etintemel, Ugur and Misra, Sanchit and Kraska, Tim},
  year = {2020},
  pages = {1001--1016},
  abstract = {Sorting is one of the most fundamental algorithms in Computer Science and a common operation in databases not just for sorting query results but also as part of joins (i.e., sort-merge-join) or indexing. In this work, we introduce a new type of distribution sort that leverages a learned model of the empirical CDF of the data. Our algorithm uses a model to efficiently get an approximation of the scaled empirical CDF for each record key and map it to the corresponding position in the output array. We then apply a deterministic sorting algorithm that works well on nearly-sorted arrays (e.g., Insertion Sort) to establish a totally sorted order. We compared this algorithm against common sorting approaches and measured its performance for up to 1 billion normally-distributed double-precision keys. The results show that our approach yields an average 3.38x performance improvement over C++ STL sort, which is an optimized Quicksort hybrid, 1.49x improvement over sequential Radix Sort, and 5.54x improvement over a C++ implementation of Timsort, which is the default sorting function for Java and Python.}
}

@article{kubiatowicz2000oceanstore,
  title = {Oceanstore: {{An}} Architecture for Global-Scale Persistent Storage},
  author = {Kubiatowicz, John and Bindel, David and Chen, Yan and Czerwinski, Steven and Eaton, Patrick and Geels, Dennis and Gummadi, Ramakrishna and Rhea, Sean and Weatherspoon, Hakim and Weimer, Westley and others},
  year = {2000},
  journal = {ACM SIGOPS Operating Systems Review},
  volume = {34},
  number = {5},
  pages = {190--201},
  publisher = {{ACM New York, NY, USA}},
  abstract = {OceanStore is a utility infrastructure designed to span the globe and provide continuous access to persistent information. Since this infrastructure is comprised of untrusted servers, data is protected through redundancy and cryptographic techniques. To improve performance, data is allowed to be cached anywhere, anytime. Additionally, monitoring of usage patterns allows adaptation to regional outages and denial of service attacks; monitoring also enhances performance through pro-active movement of data. A prototype implementation is currently under development.}
}

@article{kubiatowicz2018secure,
  title = {Secure Fog Robotics Using the Global Data Plane},
  author = {Kubiatowicz, John and Lutz, Ken and Goldberg, Ken and Joseph, Anthony and Gonzalaz, Joseph},
  year = {2018},
  journal = {NSF Proposal}
}

@inproceedings{kung1979optimality,
  title = {An Optimality Theory of Concurrency Control for Databases},
  booktitle = {Proceedings of the 1979 {{ACM SIGMOD}} International Conference on {{Management}} of Data},
  author = {Kung, Hsing-Tsung and Papadimitriou, Christos H},
  year = {1979},
  pages = {116--126}
}

@article{kuo1996model,
  title = {Model and Verification of a Data Manager Based on {{ARIES}}},
  author = {Kuo, Dean},
  year = {1996},
  journal = {ACM Transactions on Database Systems (TODS)},
  volume = {21},
  number = {4},
  pages = {427--479},
  publisher = {{ACM New York, NY, USA}},
  abstract = {In this article, we model and verify a data manager whose algorithm is based on ARIES. The work uses the I/O automata method as the formal model and the definition of correctness is defined on the interface between the scheduler and the data manager.}
}

@article{laddad2022keep,
  title = {Keep {{CALM}} and {{CRDT}} On},
  author = {Laddad, Shadaj and Power, Conor and Milano, Mae and Cheung, Alvin and Crooks, Natacha and Hellerstein, Joseph M},
  year = {2022},
  journal = {arXiv preprint arXiv:2210.12605},
  eprint = {2210.12605},
  abstract = {Despite decades of research and practical experience, developers have few tools for programming reliable distributed applications without resorting to expensive coordination techniques. Conflict-free replicated datatypes (CRDTs) are a promising line of work that enable coordination-free replication and offer certain eventual consistency guarantees in a relatively simple object-oriented API. Yet CRDT guarantees extend only to data updates; observations of CRDT state are unconstrained and unsafe. We propose an agenda that embraces the simplicity of CRDTs, but provides richer, more uniform guarantees. We extend CRDTs with a query model that reasons about which queries are safe without coordination by applying monotonicity results from the CALM Theorem, and lay out a larger agenda for developing CRDT data stores that let developers safely and efficiently interact with replicated application state.},
  archiveprefix = {arxiv}
}

@inproceedings{lagar2009snowflock,
  title = {Snowflock: Rapid Virtual Machine Cloning for Cloud Computing},
  booktitle = {Proceedings of the 4th {{ACM European}} Conference on {{Computer}} Systems},
  author = {{Lagar-Cavilla}, Horacio Andr{\'e}s and Whitney, Joseph Andrew and Scannell, Adin Matthew and Patchin, Philip and Rumble, Stephen M and De Lara, Eyal and Brudno, Michael and Satyanarayanan, Mahadev},
  year = {2009},
  pages = {1--12},
  abstract = {Virtual Machine (VM) fork is a new cloud computing abstraction that instantaneously clones a VM into multiple replicas running on different hosts. All replicas share the same initial state, matching the intuitive semantics of stateful worker creation. VM fork thus enables the straightforward creation and efficient deployment of many tasks demanding swift instantiation of stateful workers in a cloud environment, e.g. excess load handling, opportunistic job placement, or parallel computing. Lack of instantaneous stateful cloning forces users of cloud computing into ad hoc practices to manage application state and cycle provisioning. We present SnowFlock, our implementation of the VM fork abstraction. To evaluate SnowFlock, we focus on the demanding scenario of services requiring on-the-fly creation of hundreds of parallel workers in order to solve computationally-intensive queries in seconds. These services are prominent in fields such as bioinformatics, finance, and rendering. SnowFlock provides sub-second VM cloning, scales to hundreds of workers, consumes few cloud I/O resources, and has negligible runtime overhead.}
}

@article{lamb2012vertica,
  title = {The Vertica Analytic Database: {{C-store}} 7 Years Later},
  author = {Lamb, Andrew and Fuller, Matt and Varadarajan, Ramakrishna and Tran, Nga and Vandier, Ben and Doshi, Lyric and Bear, Chuck},
  year = {2012},
  journal = {arXiv preprint arXiv:1208.4173},
  eprint = {1208.4173},
  abstract = {This paper describes the system architecture of the Vertica Analytic Database (Vertica), a commercialization of the design of the C-Store research prototype. Vertica demonstrates a modern commercial RDBMS system that presents a classical relational interface while at the same time achieving the high performance expected from modern "web scale" analytic systems by making appropriate architectural choices. Vertica is also an instructive lesson in how academic systems research can be directly commercialized into a successful product.},
  archiveprefix = {arxiv}
}

@incollection{lamport1982byzantine,
  title = {The Byzantine Generals Problem},
  booktitle = {Concurrency: The Works of Leslie Lamport},
  author = {Lamport, Leslie and Shostak, Robert and Pease, Marshall},
  year = {1982},
  pages = {203--226}
}

@incollection{lamport1998part,
  title = {The Part-Time Parliament},
  booktitle = {Concurrency: The Works of Leslie Lamport},
  author = {Lamport, Leslie},
  year = {1998},
  pages = {277--317}
}

@article{lamport2001paxos,
  title = {Paxos Made Simple},
  author = {Lamport, Leslie},
  year = {2001},
  journal = {ACM SIGACT News (Distributed Computing Column) 32, 4 (Whole Number 121, December 2001)},
  pages = {51--58},
  abstract = {At the PODC 2001 conference, I got tired of everyone saying how difficult it was to understand the Paxos algorithm, published in [122]. Although people got so hung up in the pseudo-Greek names that they found the paper hard to understand, the algorithm itself is very simple. So, I cornered a couple of people at the conference and explained the algorithm to them orally, with no paper. When I got home, I wrote down the explanation as a short note, which I later revised based on comments from Fred Schneider and Butler Lampson. The current version is 13 pages long, and contains no formula more complicated than n1 {$>$} n2.}
}

@incollection{lamport2019time,
  title = {Time, Clocks, and the Ordering of Events in a Distributed System},
  booktitle = {Concurrency: The Works of Leslie Lamport},
  author = {Lamport, Leslie},
  year = {2019},
  pages = {179--196}
}

@article{larson2011high,
  title = {High-Performance Concurrency Control Mechanisms for Main-Memory Databases},
  author = {Larson, Per-{\AA}ke and Blanas, Spyros and Diaconu, Cristian and Freedman, Craig and Patel, Jignesh M and Zwilling, Mike},
  year = {2011},
  journal = {arXiv preprint arXiv:1201.0228},
  eprint = {1201.0228},
  abstract = {A database system optimized for in-memory storage can support much higher transaction rates than current systems. However, standard concurrency control methods used today do not scale to the high transaction rates achievable by such systems. In this paper we introduce two efficient concurrency control methods specifically designed for main-memory databases. Both use multiversioning to isolate read-only transactions from updates but differ in how atomicity is ensured: one is optimistic and one is pessimistic. To avoid expensive context switching, transactions never block during normal processing but they may have to wait before commit to ensure correct serialization ordering. We also implemented a main-memory optimized version of single-version locking. Experimental results show that while single-version locking works well when transactions are short and contention is low performance degrades under more demanding conditions. The multiversion schemes have higher overhead but are much less sensitive to hotspots and the presence of long-running transactions.},
  archiveprefix = {arxiv}
}

@inproceedings{lee2015f2fs,
  title = {\{\vphantom\}{{F2FS}}\vphantom\{\}: {{A}} New File System for Flash Storage},
  booktitle = {13th {{USENIX}} Conference on File and Storage Technologies ({{FAST}} 15)},
  author = {Lee, Changman and Sim, Dongho and Hwang, Jooyoung and Cho, Sangyeun},
  year = {2015},
  pages = {273--286},
  abstract = {F2FS is a Linux file system designed to perform well on modern flash storage devices. The file system builds on append-only logging and its key design decisions were made with the characteristics of flash storage in mind. This paper describes the main design ideas, data structures, algorithms and the resulting performance of F2FS. Experimental results highlight the desirable performance of F2FS; on a state-of-the-art mobile system, it outperforms EXT4 under synthetic workloads by up to 3.1  (iozone) and 2  (SQLite). It reduces elapsed time of several realistic workloads by up to 40\%. On a server system, F2FS is shown to perform better than EXT4 by up to 2.5  (SATA SSD) and 1.8  (PCIe SSD).}
}

@article{lehman1981efficient,
  title = {Efficient Locking for Concurrent Operations on {{B-trees}}},
  author = {Lehman, Philip L and Yao, S Bing},
  year = {1981},
  journal = {ACM Transactions on Database Systems (TODS)},
  volume = {6},
  number = {4},
  pages = {650--670},
  publisher = {{ACM New York, NY, USA}},
  abstract = {The B-tree and its variants have been found to be highly useful (both theoretically and in practice) for storing large amounts of information, especially on secondary storage devices. We examine the problem of overcoming the inherent difficulty of concurrent operations on such structures, using a practical storage model. A single additional ``link'' pointer in each node allows a process to easily recover from tree modifications performed by other concurrent processes. Our solution compares favorably with earlier solutions in that the locking scheme is simpler (no read-locks are used) and only a (small) constant number of nodes are locked by any update process at any given time. An informal correctness proof for our system is given.}
}

@inproceedings{leis2013adaptive,
  title = {The Adaptive Radix Tree: {{ARTful}} Indexing for Main-Memory Databases},
  booktitle = {2013 {{IEEE}} 29th International Conference on Data Engineering ({{ICDE}})},
  author = {Leis, Viktor and Kemper, Alfons and Neumann, Thomas},
  year = {2013},
  pages = {38--49},
  publisher = {{IEEE}},
  abstract = {Main memory capacities have grown up to a point where most databases fit into RAM. For main-memory database systems, index structure performance is a critical bottleneck. Traditional in-memory data structures like balanced binary search trees are not efficient on modern hardware, because they do not optimally utilize on-CPU caches. Hash tables, also often used for main-memory indexes, are fast but only support point queries. To overcome these shortcomings, we present ART, an adaptive radix tree (trie) for efficient indexing in main memory. Its lookup performance surpasses highly tuned, read-only search trees, while supporting very efficient insertions and deletions as well. At the same time, ART is very space efficient and solves the problem of excessive worst-case space consumption, which plagues most radix trees, by adaptively choosing compact and efficient data structures for internal nodes. Even though ART's performance is comparable to hash tables, it maintains the data in sorted order, which enables additional operations like range scan and prefix lookup.}
}

@inproceedings{leis2014morsel,
  title = {Morsel-Driven Parallelism: A {{NUMA-aware}} Query Evaluation Framework for the Many-Core Age},
  booktitle = {Proceedings of the 2014 {{ACM SIGMOD}} International Conference on {{Management}} of Data},
  author = {Leis, Viktor and Boncz, Peter and Kemper, Alfons and Neumann, Thomas},
  year = {2014},
  pages = {743--754},
  abstract = {With modern computer architecture evolving, two problems conspire against the state-of-the-art approaches in parallel query execution: (i) to take advantage of many-cores, all query work must be distributed evenly among (soon) hundreds of threads in order to achieve good speedup, yet (ii) dividing the work evenly is difficult even with accurate data statistics due to the complexity of modern out-of-order cores. As a result, the existing approaches for plan-driven parallelism run into load balancing and context-switching bottlenecks, and therefore no longer scale. A third problem faced by many-core architectures is the decentralization of memory controllers, which leads to Non-Uniform Memory Access (NUMA). In response, we present the morsel-driven query execution framework, where scheduling becomes a fine-grained run-time task that is NUMA-aware. Morsel-driven query processing takes small fragments of input data (morsels) and schedules these to worker threads that run entire operator pipelines until the next pipeline breaker. The degree of parallelism is not baked into the plan but can elastically change during query execution, so the dispatcher can react to execution speed of different morsels but also adjust resources dynamically in response to newly arriving queries in the workload. Further, the dispatcher is aware of data locality of the NUMA-local morsels and operator state, such that the great majority of executions takes place on NUMA-local memory. Our evaluation on the TPC-H and SSB benchmarks shows extremely high absolute performance and an average speedup of over 30 with 32 cores.}
}

@article{leis2015good,
  title = {How Good Are Query Optimizers, Really?},
  author = {Leis, Viktor and Gubichev, Andrey and Mirchev, Atanas and Boncz, Peter and Kemper, Alfons and Neumann, Thomas},
  year = {2015},
  journal = {Proceedings of the VLDB Endowment},
  volume = {9},
  number = {3},
  pages = {204--215},
  publisher = {{VLDB Endowment}},
  abstract = {Finding a good join order is crucial for query performance. In this paper, we introduce the Join Order Benchmark (JOB) and experimentally revisit the main components in the classic query optimizer architecture using a complex, real-world data set and realistic multi-join queries. We investigate the quality of industrial-strength cardinality estimators and find that all estimators routinely produce large errors. We further show that while estimates are essential for finding a good join order, query performance is unsatisfactory if the query engine relies too heavily on these estimates. Using another set of experiments that measure the impact of the cost model, we find that it has much less influence on query performance than the cardinality estimates. Finally, we investigate plan enumeration techniques comparing exhaustive dynamic programming with heuristic algorithms and find that exhaustive enumeration improves performance despite the sub-optimal cardinality estimates.}
}

@inproceedings{leis2018leanstore,
  title = {{{LeanStore}}: {{In-memory}} Data Management beyond Main Memory},
  booktitle = {2018 {{IEEE}} 34th International Conference on Data Engineering ({{ICDE}})},
  author = {Leis, Viktor and Haubenschild, Michael and Kemper, Alfons and Neumann, Thomas},
  year = {2018},
  pages = {185--196},
  publisher = {{IEEE}},
  abstract = {Disk-based database systems use buffer managers in order to transparently manage data sets larger than main memory. This traditional approach is effective at minimizing the number of I/O operations, but is also the major source of overhead in comparison with in-memory systems. To avoid this overhead, in-memory database systems therefore abandon buffer management altogether, which makes handling data sets larger than main memory very difficult. In this work, we revisit this fundamental dichotomy and design a novel storage manager that is optimized for modern hardware. Our evaluation, which is based on TPC-C and micro benchmarks, shows that our approach has little overhead in comparison with a pure in-memory system when all data resides in main memory. At the same time, like a traditional buffer manager, it is fully transparent and can manage very large data sets effectively. Furthermore, due to low-overhead synchronization, our implementation is also highly scalable on multi-core CPUs.}
}

@article{letia2009crdts,
  title = {{{CRDTs}}: {{Consistency}} without Concurrency Control},
  author = {Letia, Mihai and Pregui{\c c}a, Nuno and Shapiro, Marc},
  year = {2009},
  journal = {arXiv preprint arXiv:0907.0929},
  eprint = {0907.0929},
  abstract = {A CRDT is a data type whose operations commute when they are concurrent. Replicas of a CRDT eventually converge without any complex concurrency control. As an existence proof, we exhibit a non-trivial CRDT: a shared edit buffer called Treedoc. We outline the design, implementation and performance of Treedoc. We discuss how the CRDT concept can be generalised, and its limitations.},
  archiveprefix = {arxiv}
}

@article{levien1967computer,
  title = {A Computer System for Inference Execution and Data Retrieval},
  author = {Levien, Roger Eli and Maron, {\relax ME}},
  year = {1967},
  journal = {Communications of the ACM},
  volume = {10},
  number = {11},
  pages = {715--721},
  publisher = {{ACM New York, NY, USA}},
  file = {/Users/jayithac/Library/CloudStorage/GoogleDrive-jayithareddyp@gmail.com/My Drive/Papers/levien1967computer.pdf}
}

@inproceedings{li2016wander,
  title = {Wander Join: {{Online}} Aggregation via Random Walks},
  booktitle = {Proceedings of the 2016 International Conference on Management of Data},
  author = {Li, Feifei and Wu, Bin and Yi, Ke and Zhao, Zhuoyue},
  year = {2016},
  pages = {615--629},
  abstract = {Joins are expensive, and online aggregation over joins was proposed to mitigate the cost, which offers users a nice and flexible tradeoff between query efficiency and accuracy in a continuous, online fashion. However, the state-of-the-art approach, in both internal and external memory, is based on ripple join, which is still very expensive and even needs unrealistic assumptions (e.g., tuples in a table are stored in random order). This paper proposes a new approach, the wander join algorithm, to the online aggregation problem by performing random walks over the underlying join graph. We also design an optimizer that chooses the optimal plan for conducting the random walks without having to collect any statistics a priori. Compared with ripple join, wander join is particularly efficient for equality joins involving multiple tables, but also supports {\texttheta}-joins. Selection predicates and group-by clauses can be handled as well. Extensive experiments using the TPC-H benchmark have demonstrated the superior performance of wander join over ripple join. In particular, we have integrated and tested wander join in the latest version of PostgreSQL, demonstrating its practicality in a full-fledged database system.}
}

@article{liedtke1996toward,
  title = {Toward Real Microkernels},
  author = {Liedtke, Jochen},
  year = {1996},
  journal = {Communications of the ACM},
  volume = {39},
  number = {9},
  pages = {70--77},
  publisher = {{ACM New York, NY, USA}}
}

@article{lipp2020meltdown,
  title = {Meltdown: {{Reading}} Kernel Memory from User Space},
  author = {Lipp, Moritz and Schwarz, Michael and Gruss, Daniel and Prescher, Thomas and Haas, Werner and Horn, Jann and Mangard, Stefan and Kocher, Paul and Genkin, Daniel and Yarom, Yuval and others},
  year = {2020},
  journal = {Communications of the ACM},
  volume = {63},
  number = {6},
  pages = {46--56},
  publisher = {{ACM New York, NY, USA}},
  abstract = {Lessons learned from Meltdown's exploitation of the weaknesses in today's processors.}
}

@misc{LittleThingsComparing2021,
  title = {The {{Little Things}}: {{Comparing Floating Point Numbers}}},
  shorttitle = {The {{Little Things}}},
  year = {2021},
  month = sep,
  journal = {The Coding Nest},
  urldate = {2023-11-08},
  abstract = {There is a lot of confusion about floating-point numbers and a lot of bad advice going around. IEEE-754 floating-point numbers are a complex beast, and comparing them is not always easy, but in this post, we will take a look at different approaches and their tradeoffs.},
  howpublished = {https://codingnest.com/the-little-things-comparing-floating-point-numbers/},
  langid = {english}
}

@misc{LocalitySensitiveHashing,
  title = {Locality {{Sensitive Hashing}} ({{LSH}}): {{The Illustrated Guide}} | {{Pinecone}}},
  shorttitle = {Locality {{Sensitive Hashing}} ({{LSH}})},
  urldate = {2023-11-06},
  howpublished = {https://www.pinecone.io/learn/series/faiss/locality-sensitive-hashing/},
  langid = {english}
}

@article{lohman1991extensions,
  title = {Extensions to {{Starburst}}: {{Objects}}, Types, Functions, and Rules},
  author = {Lohman, Guy M and Lindsay, Bruce and Pirahesh, Hamid and Schiefer, K Bernhard},
  year = {1991},
  journal = {Communications of the ACM},
  volume = {34},
  number = {10},
  pages = {94--109},
  publisher = {{ACM New York, NY, USA}}
}

@inproceedings{mackert1986r,
  title = {R* Optimizer Validation and Performance Evaluation for Local Queries},
  booktitle = {Proceedings of the 1986 {{ACM SIGMOD}} International Conference on {{Management}} of Data},
  author = {Mackert, Lothar F and Lohman, Guy M},
  year = {1986},
  pages = {84--95},
  abstract = {Few database query optimizer models have been validated against actual performance. This paper presents the methodology and results of a thorough validation of the optimizer and evaluation of the performance of the experimental distributed relational database management system R*, which inherited and extended to a distributed environment the optimization algorithms of System R. Optimizer estimated costs and actual R* resources consumed were written to database tables using new SQL commands, permitting automated control from SQL application programs of test data collection and reduction. A number of tests were run over a wide variety of dynamically-created test databases, SQL queries, and system parameters. The results for single-table access, sorting, and local 2-table joins are reported here. The tests confirmed the accuracy of the majority of the I/O cost model, the significant contribution of CPU cost to total cost, and the need to model CPU cost in more detail than was done in System R. The R* optimizer now retains cost components separately and estimates the number of CPU instructions, including those for applying different kinds of predicates. The sensitivity of I/O cost to buffer space motivated the development of more detailed models of buffer utilization unclustered index scans and nested-loop joins often benefit from pages remaining in the buffers, whereas concurrent scans of the data pages and the index pages for multiple tables during joins compete for buffer share. Without an index on the join column of the inner table, the optimizer correctly avoids the nested-loop join, confirming the need for merge-scan joins. When the join column of the inner is indexed, the optimizer overestimates the cost of the nested-loop join, whose actual performance is very sensitive to three parameters that are extremely difficult to estimate (1) the join (result) cardinality, (2) the outer table's cardinality, and (3) the number of buffer pages available to store the inner table. Suggestions are given for improved database statistics, prefetch and page replacement strategies for the buffer manager, and the use of temporary indexes and Bloom filters (hashed semijoins) to reduce access of unneeded data.}
}

@inproceedings{mahdavi2019raha,
  title = {Raha: {{A}} Configuration-Free Error Detection System},
  booktitle = {Proceedings of the 2019 International Conference on Management of Data},
  author = {Mahdavi, Mohammad and Abedjan, Ziawasch and Castro Fernandez, Raul and Madden, Samuel and Ouzzani, Mourad and Stonebraker, Michael and Tang, Nan},
  year = {2019},
  pages = {865--882},
  abstract = {Detecting erroneous values is a key step in data cleaning. Error detection algorithms usually require a user to provide input configurations in the form of rules or statistical parameters. However, providing a complete, yet correct, set of configurations for each new dataset is not trivial, as the user has to know about both the dataset and the error detection algorithms upfront. In this paper, we present Raha, a new configuration-free error detection system. By generating a limited number of configurations for error detection algorithms that cover various types of data errors, we can generate an expressive feature vector for each tuple value. Leveraging these feature vectors, we propose a novel sampling and classification scheme that effectively chooses the most representative values for training. Furthermore, our system can exploit historical data to filter out irrelevant error detection algorithms and configurations. In our experiments, Raha outperforms the state-of-the-art error detection techniques with no more than 20 labeled tuples on each dataset.}
}

@article{marcus2019neo,
  title = {Neo: {{A}} Learned Query Optimizer},
  author = {Marcus, Ryan and Negi, Parimarjan and Mao, Hongzi and Zhang, Chi and Alizadeh, Mohammad and Kraska, Tim and Papaemmanouil, Olga and Tatbul, Nesime},
  year = {2019},
  journal = {arXiv preprint arXiv:1904.03711},
  eprint = {1904.03711},
  abstract = {Query optimization is one of the most challenging problems in database systems. Despite the progress made over the past decades, query optimizers remain extremely complex components that require a great deal of hand-tuning for specific workloads and datasets. Motivated by this shortcoming and inspired by recent advances in applying machine learning to data management challenges, we introduce Neo (Neural Optimizer), a novel learning-based query optimizer that relies on deep neural networks to generate query executions plans. Neo bootstraps its query optimization model from existing optimizers and continues to learn from incoming queries, building upon its successes and learning from its failures. Furthermore, Neo naturally adapts to underlying data patterns and is robust to estimation errors. Experimental results demonstrate that Neo, even when bootstrapped from a simple optimizer like PostgreSQL, can learn a model that offers similar performance to state-of-the-art commercial optimizers, and in some cases even surpass them.},
  archiveprefix = {arxiv}
}

@article{marcus2020benchmarking,
  title = {Benchmarking Learned Indexes},
  author = {Marcus, Ryan and Kipf, Andreas and {van Renen}, Alexander and Stoian, Mihail and Misra, Sanchit and Kemper, Alfons and Neumann, Thomas and Kraska, Tim},
  year = {2020},
  journal = {arXiv preprint arXiv:2006.12804},
  eprint = {2006.12804},
  abstract = {Recent advancements in learned index structures propose replacing existing index structures, like B-Trees, with approximate learned models. In this work, we present a unified benchmark that compares well-tuned implementations of three learned index structures against several state-of-the-art "traditional" baselines. Using four real-world datasets, we demonstrate that learned index structures can indeed outperform non-learned indexes in read-only in-memory workloads over a dense array. We also investigate the impact of caching, pipelining, dataset size, and key size. We study the performance profile of learned index structures, and build an explanation for why learned models achieve such good performance. Finally, we investigate other important properties of learned index structures, such as their performance in multi-threaded systems and their build times.},
  archiveprefix = {arxiv}
}

@inproceedings{marcus2021bao,
  title = {Bao: {{Making}} Learned Query Optimization Practical},
  booktitle = {Proceedings of the 2021 International Conference on Management of Data},
  author = {Marcus, Ryan and Negi, Parimarjan and Mao, Hongzi and Tatbul, Nesime and Alizadeh, Mohammad and Kraska, Tim},
  year = {2021},
  pages = {1275--1288},
  abstract = {Recent efforts applying machine learning techniques to query optimization have shown few practical gains due to substantive training overhead, inability to adapt to changes, and poor tail performance. Motivated by these difficulties, we introduce Bao (the {\textbackslash}underlineBa ndit {\textbackslash}underlineo ptimizer). Bao takes advantage of the wisdom built into existing query optimizers by providing per-query optimization hints. Bao combines modern tree convolutional neural networks with Thompson sampling, a well-studied reinforcement learning algorithm. As a result, Bao automatically learns from its mistakes and adapts to changes in query workloads, data, and schema. Experimentally, we demonstrate that Bao can quickly learn strategies that improve end-to-end query execution performance, including tail latency, for several workloads containing long-running queries. In cloud environments, we show that Bao can offer both reduced costs and better performance compared with a commercial system.}
}

@article{mckusick1984fast,
  title = {A Fast File System for {{UNIX}}},
  author = {McKusick, Marshall K and Joy, William N and Leffler, Samuel J and Fabry, Robert S},
  year = {1984},
  journal = {ACM Transactions on Computer Systems (TOCS)},
  volume = {2},
  number = {3},
  pages = {181--197},
  publisher = {{ACM New York, NY, USA}}
}

@article{mohan1986transaction,
  title = {Transaction Management in the {{R}}* Distributed Database Management System},
  author = {Mohan, C},
  year = {1986},
  journal = {ACM Transactions on Database Systems (TODS)},
  volume = {11},
  number = {4},
  pages = {378--396},
  publisher = {{ACM New York, NY, USA}},
  abstract = {This paper deals with the transaction management aspects of the R* distributed database system. It concentrates primarily on the description of the R* commit protocols, Presumed Abort (PA) and Presumed Commit (PC). PA and PC are extensions of the well-known, two-phase (2P) commit protocol. PA is optimized for read-only transactions and a class of multisite update transactions, and PC is optimized for other classes of multisite update transactions. The optimizations result in reduced intersite message traffic and log writes, and, consequently, a better response time. The paper also discusses R*'s approach toward distributed deadlock detection and resolution.}
}

@article{mohan1992aries,
  title = {{{ARIES}}: {{A}} Transaction Recovery Method Supporting Fine-Granularity Locking and Partial Rollbacks Using Write-Ahead Logging},
  author = {Mohan, Chandrasekaran and Haderle, Don and Lindsay, Bruce and Pirahesh, Hamid and Schwarz, Peter},
  year = {1992},
  journal = {ACM Transactions on Database Systems (TODS)},
  volume = {17},
  number = {1},
  pages = {94--162},
  publisher = {{ACM New York, NY, USA}},
  abstract = {DB2TM, IMS, and TandemTM systems. ARIES is applicable not only to database management systems but also to persistent object-oriented languages, recoverable file systems and transaction-based operating systems. ARIES has been implemented, to varying degrees, in IBM's OS/2TM Extended Edition Database Manager, DB2, Workstation Data Save Facility/VM, Starburst and QuickSilver, and in the University of Wisconsin's EXODUS and Gamma database machine.}
}

@inproceedings{mohan1999repeating,
  title = {Repeating History beyond {{ARIES}}},
  booktitle = {{{VLDB}}},
  author = {Mohan, C},
  year = {1999},
  volume = {99},
  pages = {7--10}
}

@article{mor2016toward,
  title = {Toward a Global Data Infrastructure},
  author = {Mor, Nitesh and Zhang, Ben and Kolb, John and Chan, Douglas S and Goyal, Nikhil and Sun, Nicholas and Lutz, Ken and Allman, Eric and Wawrzynek, John and Lee, Edward A and others},
  year = {2016},
  journal = {IEEE Internet Computing},
  volume = {20},
  number = {3},
  pages = {54--62},
  publisher = {{IEEE}},
  abstract = {The Internet of Things (IoT) represents a new class of applications that can benefit from cloud infrastructure. However, directly connecting smart devices to the cloud has multiple disadvantages and is unlikely to keep up with the growing speed of the IoT or the diverse needs of IoT applications. Here, the authors argue that fundamental IoT properties prevent the current approach from scaling. What's missing is a well-architected system extending cloud functionality and providing seamless interplay among heterogeneous components closer to the edge in the IoT space. Raising the level of abstraction to a data-centric design -- focused around the distribution, preservation, and protection of information -- better matches the IoT. To address such problems with the cloud-centric architecture, the authors present their early work on a distributed platform, the Global Data Plane.}
}

@inproceedings{mor2019global,
  title = {Global Data Plane: {{A}} Federated Vision for Secure Data in Edge Computing},
  booktitle = {2019 {{IEEE}} 39th International Conference on Distributed Computing Systems ({{ICDCS}})},
  author = {Mor, Nitesh and Pratt, Richard and Allman, Eric and Lutz, Kenneth and Kubiatowicz, John},
  year = {2019},
  pages = {1652--1663},
  publisher = {{IEEE}},
  abstract = {We propose a federated edge-computing architecture for management of data. Our vision is to enable a service provider model for "data-services", where a user can enter into economic agreements with an infrastructure maintainer to provide storage and communication of data, without necessarily trusting the infrastructure provider. Toward this vision, we present cryptographically hardened cohesive collections of data items called DataCapsules, and an overview of the underlying federated architecture, called Global Data Plane.}
}

@inproceedings{mozafari2017approximate,
  title = {Approximate Query Engines: {{Commercial}} Challenges and Research Opportunities},
  booktitle = {Proceedings of the 2017 {{ACM}} International Conference on Management of Data},
  author = {Mozafari, Barzan},
  year = {2017},
  pages = {521--524},
  abstract = {Recent years have witnessed a surge of interest in Approximate Query Processing (AQP) solutions, both in academia and the commercial world. In addition to well-known open problems in this area, there are many new research challenges that have surfaced as a result of the first interaction of AQP technology with commercial and real-world customers. We categorize these into deployment, planning, and interface challenges. At the same time, AQP settings introduce many interesting opportunities that would not be possible in a database with precise answers. These opportunities create hopes for overcoming some of the major limitations of traditional database systems. For example, we discuss how a database can reuse its past work in a generic way, and become smarter as it answers new queries. Our goal in this talk is to suggest some of the exciting research directions in this field that are worth pursuing.}
}

@inproceedings{mudgal2018deep,
  title = {Deep Learning for Entity Matching: {{A}} Design Space Exploration},
  booktitle = {Proceedings of the 2018 International Conference on Management of Data},
  author = {Mudgal, Sidharth and Li, Han and Rekatsinas, Theodoros and Doan, AnHai and Park, Youngchoon and Krishnan, Ganesh and Deep, Rohit and Arcaute, Esteban and Raghavendra, Vijay},
  year = {2018},
  pages = {19--34},
  abstract = {Entity matching (EM) finds data instances that refer to the same real-world entity. In this paper we examine applying deep learning (DL) to EM, to understand DL's benefits and limitations. We review many DL solutions that have been developed for related matching tasks in text processing (e.g., entity linking, textual entailment, etc.). We categorize these solutions and define a space of DL solutions for EM, as embodied by four solutions with varying representational power: SIF, RNN, Attention, and Hybrid. Next, we investigate the types of EM problems for which DL can be helpful. We consider three such problem types, which match structured data instances, textual instances, and dirty instances, respectively. We empirically compare the above four DL solutions with Magellan, a state-of-the-art learning-based EM solution. The results show that DL does not outperform current solutions on structured EM, but it can significantly outperform them on textual and dirty EM. For practitioners, this suggests that they should seriously consider using DL for textual and dirty EM problems. Finally, we analyze DL's performance and discuss future research directions.}
}

@inproceedings{nathan2020learning,
  title = {Learning Multi-Dimensional Indexes},
  booktitle = {Proceedings of the 2020 {{ACM SIGMOD}} International Conference on Management of Data},
  author = {Nathan, Vikram and Ding, Jialin and Alizadeh, Mohammad and Kraska, Tim},
  year = {2020},
  pages = {985--1000},
  abstract = {Scanning and filtering over multi-dimensional tables are key operations in modern analytical database engines. To optimize the performance of these operations, databases often create clustered indexes over a single dimension or multi-dimensional indexes such as R-Trees, or use complex sort orders (e.g., Z-ordering). However, these schemes are often hard to tune and their performance is inconsistent across different datasets and queries. In this paper, we introduce Flood, a multi-dimensional in-memory read-optimized index that automatically adapts itself to a particular dataset and workload by jointly optimizing the index structure and data storage layout. Flood achieves up to three orders of magnitude faster performance for range scans with predicates than state-of-the-art multi-dimensional indexes or sort orders on real-world datasets and workloads. Our work serves as a building block towards an end-to-end learned database system.}
}

@article{neumann2011efficiently,
  title = {Efficiently Compiling Efficient Query Plans for Modern Hardware},
  author = {Neumann, Thomas},
  year = {2011},
  month = jun,
  journal = {Proceedings of the VLDB Endowment},
  volume = {4},
  number = {9},
  pages = {539--550},
  issn = {2150-8097},
  doi = {10.14778/2002938.2002940},
  urldate = {2023-07-04},
  abstract = {As main memory grows, query performance is more and more determined by the raw CPU costs of query processing itself. The classical iterator style query processing technique is very simple and flexible, but shows poor performance on modern CPUs due to lack of locality and frequent instruction mispredictions. Several techniques like batch oriented processing or vectorized tuple processing have been proposed in the past to improve this situation, but even these techniques are frequently out-performed by hand-written execution plans.},
  langid = {english},
  file = {/Users/jayithac/Zotero/storage/MHVG4ASR/Neumann - 2011 - Efficiently compiling efficient query plans for mo.pdf}
}

@inproceedings{neumann2015fast,
  title = {Fast Serializable Multi-Version Concurrency Control for Main-Memory Database Systems},
  booktitle = {Proceedings of the 2015 {{ACM SIGMOD}} International Conference on Management of Data},
  author = {Neumann, Thomas and M{\"u}hlbauer, Tobias and Kemper, Alfons},
  year = {2015},
  pages = {677--689},
  abstract = {Multi-Version Concurrency Control (MVCC) is a widely employed concurrency control mechanism, as it allows for execution modes where readers never block writers. However, most systems implement only snapshot isolation (SI) instead of full serializability. Adding serializability guarantees to existing SI implementations tends to be prohibitively expensive. We present a novel MVCC implementation for main-memory database systems that has very little overhead compared to serial execution with single-version concurrency control, even when maintaining serializability guarantees. Updating data in-place and storing versions as before-image deltas in undo buffers not only allows us to retain the high scan performance of single-version systems but also forms the basis of our cheap and fine-grained serializability validation mechanism. The novel idea is based on an adaptation of precision locking and verifies that the (extensional) writes of recently committed transactions do not intersect with the (intensional) read predicate space of a committing transaction. We experimentally show that our MVCC model allows very fast processing of transactions with point accesses as well as read-heavy transactions and that there is little need to prefer SI over full serializability any longer.}
}

@article{o1996log,
  title = {The Log-Structured Merge-Tree ({{LSM-tree}})},
  author = {O'Neil, Patrick and Cheng, Edward and Gawlick, Dieter and O'Neil, Elizabeth},
  year = {1996},
  journal = {Acta Informatica},
  volume = {33},
  pages = {351--385},
  publisher = {{Springer}},
  abstract = {High-performance transaction system applications typically insert rows in a History table to provide an activity trace; at the same time the transaction system generates log records for purposes of system recovery. Both types of generated information can benefit from efficient indexing. An example in a well-known setting is the TPC-A benchmark application, modified to support efficient queries on the history for account activity for specific accounts. This requires an index by account-id on the fast-growing History table. Unfortunately, standard disk-based index structures such as the B-tree will effectively double the I/O cost of the transaction to maintain an index such as this in real time, increasing the total system cost up to fifty percent. Clearly a method for maintaining a real-time index at low cost is desirable. The log-structured mergetree (LSM-tree) is a disk-based data structure designed to provide low-cost indexing for a file experiencing a high rate of record inserts (and deletes) over an extended period. The LSM-tree uses an algorithm that defers and batches index changes, cascading the changes from a memory-based component through one or more disk components in an efficient manner reminiscent of merge sort. During this process all index values are continuously accessible to retrievals (aside from very short locking periods), either through the memory component or one of the disk components. The algorithm has greatly reduced disk arm movements compared to a traditional access methods such as B-trees, and will improve cost-performance in domains where disk arm costs for inserts with traditional access methods overwhelm storage media costs. The LSM-tree approach also generalizes to operations other than insert and delete. However, indexed finds requiring immediate response will lose I/O efficiency in some cases, so the LSM-tree is most useful in applications where index inserts are more common than finds that retrieve the entries. This seems to be a common property for history tables and log files, for example. The conclusions of Sect. 6 compare the hybrid use of memory and disk components in the LSM-tree access method with the commonly understood advantage of the hybrid method to buffer disk pages in memory.},
  file = {/Users/jayithac/Library/CloudStorage/GoogleDrive-jayithareddyp@gmail.com/My Drive/Papers/o1996log.pdf}
}

@inproceedings{o1997improved,
  title = {Improved Query Performance with Variant Indexes},
  booktitle = {Proceedings of the 1997 {{ACM SIGMOD}} International Conference on {{Management}} of Data},
  author = {O'Neil, Patrick and Quass, Dallan},
  year = {1997},
  pages = {38--49},
  abstract = {The read-mostly environment of data warehousing makes it possible to use more complex indexes to speed up queries than in situations where concurrent updates are present. The current paper presents a short review of current indexing technology, including row-set representation by Bitmaps, and then introduces two approaches we call Bit-Sliced indexing and Projection indexing. A Projection index materializes all values of a column in RID order, and a Bit-Sliced index essentially takes an orthogonal bit-by-bit view of the same data. While some of these concepts started with the MODEL 204 product, and both Bit-Sliced and Projection indexing are now fully realized in Sybase IQ, this is the first rigorous examination of such indexing capabilities in the literature. We compare algorithms that become feasible with these variant index types against algorithms using more conventional indexes. The analysis demonstrates important performance advantages for variant indexes in some types of SQL aggregation, predicate evaluation, and grouping. The paper concludes by introducing a new method whereby multi-dimensional group-by queries, reminiscent of OLAP/Datacube queries but with more flexibility, can be very efficiently performed.}
}

@misc{ObsidianMarpElevating2023,
  title = {Obsidian + {{Marp}}: {{Elevating Note Presentations}} (without {{Power Point}})},
  shorttitle = {Obsidian + {{Marp}}},
  year = {2023},
  month = nov,
  journal = {Samuele Cozzi},
  urldate = {2023-11-29},
  abstract = {In the ever-evolving landscape of note-taking and presentation tools, enthusiasts are always on the lookout for innovative solutions that seamlessly integrate into their workflow. Enter Obsidian and Marp - an extraordinary power duo that transforms the way we present and share our notes. Obsidian: Your Personal Knowledge Hub in Markdown Obsidian doesn't need a presentation. It is a powerful knowledge management and note-taking application that emphasizes the interconnectedness of ideas. It uses a markdown-based approach to capture and organize your thoughts, creating a network of knowledge that grows with your intellectual journey.},
  chapter = {posts},
  howpublished = {https://samuele-cozzi-io.github.io/website/posts/2023/marp-obsidian/},
  langid = {english},
  file = {/Users/jayithac/Zotero/storage/GJYE4LZT/marp-obsidian.html}
}

@inproceedings{oki1988viewstamped,
  title = {Viewstamped Replication: {{A}} New Primary Copy Method to Support Highly-Available Distributed Systems},
  booktitle = {Proceedings of the Seventh Annual {{ACM Symposium}} on {{Principles}} of Distributed Computing},
  author = {Oki, Brian M and Liskov, Barbara H},
  year = {1988},
  pages = {8--17}
}

@inproceedings{ongaro2014search,
  title = {In Search of an Understandable Consensus Algorithm},
  booktitle = {2014 {{USENIX}} Annual Technical Conference ({{USENIX ATC}} 14)},
  author = {Ongaro, Diego and Ousterhout, John},
  year = {2014},
  pages = {305--319},
  abstract = {Raft is a consensus algorithm for managing a replicated log. It produces a result equivalent to (multi-)Paxos, and it is as efficient as Paxos, but its structure is different from Paxos; this makes Raft more understandable than Paxos and also provides a better foundation for building practical systems. In order to enhance understandability, Raft separates the key elements of consensus, such as leader election, log replication, and safety, and it enforces a stronger degree of coherency to reduce the number of states that must be considered. Results from a user study demonstrate that Raft is easier for students to learn than Paxos. Raft also includes a new mechanism for changing the cluster membership, which uses overlapping majorities to guarantee safety.}
}

@inproceedings{ooi2000indexing,
  title = {Indexing the Edges{\textemdash}a Simple and yet Efficient Approach to High-Dimensional Indexing},
  booktitle = {Proceedings of the Nineteenth {{ACM SIGMOD-SIGACT-SIGART}} Symposium on {{Principles}} of Database Systems},
  author = {Ooi, Beng Chin and Tan, Kian-Lee and Yu, Cui and Bressan, Stephane},
  year = {2000},
  pages = {166--174},
  abstract = {In this paper, we propose a new tunable index scheme, called iMinMax({$O$}), that maps points in high dimensional spaces to single dimension values determined by their maximum or minimum values among all dimensions. By varying the tuning ``knob'' {$O$}, we can obtain different family of iMinMax structures that are optimized for different distributions of data sets. For a d-dimensional space, a range query need to be transformed into d subqueries. However, some of these subqueries can be pruned away without evaluation, further enhancing the efficiency of the scheme. Experimental results show that iMinMax({$O$}) can outperform the more complex Pyramid technique by a wide margin.},
  file = {/Users/jayithac/Library/CloudStorage/GoogleDrive-jayithareddyp@gmail.com/My Drive/Papers/ooi2000indexing.pdf}
}

@article{pan2009lithe,
  title = {Lithe: {{Enabling}} Efficient Composition of Parallel Libraries},
  author = {Pan, Heidi and Hindman, Benjamin and Asanovic, Krste},
  year = {2009},
  journal = {Proc. of HotPar},
  volume = {9}
}

@inproceedings{pan2010composing,
  title = {Composing Parallel Software Efficiently with Lithe},
  booktitle = {Proceedings of the 31st {{ACM SIGPLAN}} Conference on Programming Language Design and Implementation},
  author = {Pan, Heidi and Hindman, Benjamin and Asanovi{\'c}, Krste},
  year = {2010},
  pages = {376--387},
  abstract = {Applications composed of multiple parallel libraries perform poorly when those libraries interfere with one another by obliviously using the same physical cores, leading to destructive resource oversubscription. This paper presents the design and implementation of Lithe, a low-level substrate that provides the basic primitives and a standard interface for composing parallel codes efficiently. Lithe can be inserted underneath the runtimes of legacy parallel libraries to provide bolt-on composability without needing to change existing application code. Lithe can also serve as the foundation for building new parallel abstractions and libraries that automatically interoperate with one another. In this paper, we show versions of Threading Building Blocks (TBB) and OpenMP perform competitively with their original implementations when ported to Lithe. Furthermore, for two applications composed of multiple parallel libraries, we show that leveraging our substrate outperforms their original, even expertly tuned, implementations.}
}

@inproceedings{papadias2003optimal,
  title = {An Optimal and Progressive Algorithm for Skyline Queries},
  booktitle = {Proceedings of the 2003 {{ACM SIGMOD}} International Conference on {{Management}} of Data},
  author = {Papadias, Dimitris and Tao, Yufei and Fu, Greg and Seeger, Bernhard},
  year = {2003},
  pages = {467--478},
  abstract = {The skyline of a set of d-dimensional points contains the points that are not dominated by any other point on all dimensions. Skyline computation has recently received considerable attention in the database community, especially for progressive (or online) algorithms that can quickly return the first skyline points without having to read the entire data file. Currently, the most efficient algorithm is NN ({$<$}u{$>$}n{$<$}/u{$>$}earest {$<$}u{$>$}n{$<$}/u{$>$}eighbors), which applies the divide -and-conquer framework on datasets indexed by R-trees. Although NN has some desirable features (such as high speed for returning the initial skyline points, applicability to arbitrary data distributions and dimensions), it also presents several inherent disadvantages (need for duplicate elimination if d{$>$}2, multiple accesses of the same node, large space overhead). In this paper we develop BBS ({$<$}u{$>$}b{$<$}/u{$>$}ranch-and-{$<$}u{$>$}b{$<$}/u{$>$}ound {$<$}u{$>$}s{$<$}/u{$>$}kyline), a progressive algorithm also based on nearest neighbor search, which is IO optimal, i.e., it performs a single access only to those R-tree nodes that may contain skyline points. Furthermore, it does not retrieve duplicates and its space overhead is significantly smaller than that of NN. Finally, BBS is simple to implement and can be efficiently applied to a variety of alternative skyline queries. An analytical and experimental comparison shows that BBS outperforms NN (usually by orders of magnitude) under all problem instances.},
  file = {/Users/jayithac/Library/CloudStorage/GoogleDrive-jayithareddyp@gmail.com/My Drive/Papers/papadias2003optimal.pdf}
}

@article{park2008reconfigurable,
  title = {A Reconfigurable {{FTL}} (Flash Translation Layer) Architecture for {{NAND}} Flash-Based Applications},
  author = {Park, Chanik and Cheon, Wonmoon and Kang, Jeonguk and Roh, Kangho and Cho, Wonhee and Kim, Jin-Soo},
  year = {2008},
  journal = {ACM Transactions on Embedded Computing Systems (TECS)},
  volume = {7},
  number = {4},
  pages = {1--23},
  publisher = {{ACM New York, NY, USA}},
  abstract = {In this article, a novel FTL (flash translation layer) architecture is proposed for NAND flash-based applications such as MP3 players, DSCs (digital still cameras) and SSDs (solid-state drives). Although the basic function of an FTL is to translate a logical sector address to a physical sector address in flash memory, efficient algorithms of an FTL have a significant impact on performance as well as the lifetime. After the dominant parameters that affect the performance and endurance are categorized, the design space of the FTL architecture is explored based on a diverse workload analysis. With the proposed FTL architectural framework, it is possible to decide which configuration of FTL mapping parameters yields the best performance, depending on the differing characteristics of various NAND flash-based applications.}
}

@inproceedings{park2018verdictdb,
  title = {Verdictdb: {{Universalizing}} Approximate Query Processing},
  booktitle = {Proceedings of the 2018 International Conference on Management of Data},
  author = {Park, Yongjoo and Mozafari, Barzan and Sorenson, Joseph and Wang, Junhao},
  year = {2018},
  pages = {1461--1476},
  abstract = {Despite 25 years of research in academia, approximate query processing (AQP) has had little industrial adoption. One of the major causes of this slow adoption is the reluctance of traditional vendors to make radical changes to their legacy codebases, and the preoccupation of newer vendors (e.g., SQL-on-Hadoop products) with implementing standard features. Additionally, the few AQP engines that are available are each tied to a specific platform and require users to completely abandon their existing databases---an unrealistic expectation given the infancy of the AQP technology. Therefore, we argue that a universal solution is needed: a database-agnostic approximation engine that will widen the reach of this emerging technology across various platforms. Our proposal, called VerdictDB, uses a middleware architecture that requires no changes to the backend database, and thus, can work with all off-the-shelf engines. Operating at the driver-level, VerdictDB intercepts analytical queries issued to the database and rewrites them into another query that, if executed by any standard relational engine, will yield sufficient information for computing an approximate answer. VerdictDB uses the returned result set to compute an approximate answer and error estimates, which are then passed on to the user or application. However, lack of access to the query execution layer introduces significant challenges in terms of generality, correctness, and efficiency. This paper shows how VerdictDB overcomes these challenges and delivers up to 171{\texttimes} speedup (18.45{\texttimes} on average) for a variety of existing engines, such as Impala, Spark SQL, and Amazon Redshift, while incurring less than 2.6\% relative error. VerdictDB is open-sourced under Apache License.}
}

@inproceedings{patterson1988case,
  title = {A Case for Redundant Arrays of Inexpensive Disks ({{RAID}})},
  booktitle = {Proceedings of the 1988 {{ACM SIGMOD}} International Conference on {{Management}} of Data},
  author = {Patterson, David A and Gibson, Garth and Katz, Randy H},
  year = {1988},
  pages = {109--116},
  abstract = {Increasing performance of CPUs and memories will be squandered if not matched by a similar performance increase in I/O. While the capacity of Single Large Expensive Disks (SLED) has grown rapidly, the performance improvement of SLED has been modest. Redundant Arrays of Inexpensive Disks (RAID), based on the magnetic disk technology developed for personal computers, offers an attractive alternative to SLED, promising improvements of an order of magnitude in performance, reliability, power consumption, and scalability. This paper introduces five levels of RAIDs, giving their relative cost/performance, and compares RAID to an IBM 3380 and a Fujitsu Super Eagle.}
}

@inproceedings{pavlo2009comparison,
  title = {A Comparison of Approaches to Large-Scale Data Analysis},
  booktitle = {Proceedings of the 2009 {{ACM SIGMOD}} International Conference on Management of Data},
  author = {Pavlo, Andrew and Paulson, Erik and Rasin, Alexander and Abadi, Daniel J and DeWitt, David J and Madden, Samuel and Stonebraker, Michael},
  year = {2009},
  pages = {165--178},
  abstract = {There is currently considerable enthusiasm around the MapReduce (MR) paradigm for large-scale data analysis [17]. Although the basic control flow of this framework has existed in parallel SQL database management systems (DBMS) for over 20 years, some have called MR a dramatically new computing model [8, 17]. In this paper, we describe and compare both paradigms. Furthermore, we evaluate both kinds of systems in terms of performance and development complexity. To this end, we define a benchmark consisting of a collection of tasks that we have run on an open source version of MR as well as on two parallel DBMSs. For each task, we measure each system's performance for various degrees of parallelism on a cluster of 100 nodes. Our results reveal some interesting trade-offs. Although the process to load data into and tune the execution of parallel DBMSs took much longer than the MR system, the observed performance of these DBMSs was strikingly better. We speculate about the causes of the dramatic performance difference and consider implementation concepts that future systems should take from both kinds of architectures.}
}

@inproceedings{pavlo2017self,
  title = {Self-Driving Database Management Systems.},
  booktitle = {{{CIDR}}},
  author = {Pavlo, Andrew and Angulo, Gustavo and Arulraj, Joy and Lin, Haibin and Lin, Jiexi and Ma, Lin and Menon, Prashanth and Mowry, Todd C and Perron, Matthew and Quah, Ian and others},
  year = {2017},
  volume = {4},
  pages = {1}
}

@book{petrov2019database,
  title = {Database {{Internals}}: {{A}} Deep Dive into How Distributed Data Systems Work},
  author = {Petrov, Alex},
  year = {2019},
  publisher = {{O'Reilly Media}}
}

@inproceedings{pillai2013extending,
  title = {Extending High-Dimensional Indexing Techniques Pyramid and Iminmax ({{{\texttheta}}}): {{Lessons}} Learned},
  booktitle = {Big Data: 29th British National Conference on Databases, {{BNCOD}} 2013, Oxford, {{UK}}, July 8-10, 2013. {{Proceedings}} 29},
  author = {Pillai, Karthik Ganesan and Sturlaugson, Liessman and Banda, Juan M and Angryk, Rafal A},
  year = {2013},
  pages = {253--267},
  publisher = {{Springer}},
  abstract = {Pyramid Technique and iMinMax({\texttheta}) are two popular high-dimensional indexing approaches that map points in a high-dimensional space to a single-dimensional index. In this work, we perform the first independent experimental evaluation of Pyramid Technique and iMinMax({\texttheta}), and discuss in detail promising extensions for testing k-Nearest Neighbor (kNN) and range queries. For datasets with skewed distributions, the parameters of these algorithms must be tuned to maintain balanced partitions. We show that, by using the medians of the distribution we can optimize these parameters. For the Pyramid Technique, different approximate median methods on data space partitioning are experimentally compared using kNN queries. For the iMinMax({\texttheta}), the default parameter setting and parameters tuned using the distribution median are experimentally compared using range queries. Also, as proposed in the iMinMax({\texttheta}) paper, we investigated the benefit of maintaining a parameter to account for the skewness of each dimension separately instead of a single parameter over all the dimensions.}
}

@inproceedings{prabhakaran2005analysis,
  title = {Analysis and Evolution of Journaling File Systems.},
  booktitle = {{{USENIX}} Annual Technical Conference, General Track},
  author = {Prabhakaran, Vijayan and {Arpaci-Dusseau}, Andrea C and {Arpaci-Dusseau}, Remzi H},
  year = {2005},
  volume = {194},
  pages = {196--215},
  abstract = {We develop and apply two new methods for analyzing file system behavior and evaluating file system changes. First, semantic block-level analysis (SBA) combines knowledge of on-disk data structures with a trace of disk traffic to infer file system behavior; in contrast to standard benchmarking approaches, SBA enables users to understand why the file system behaves as it does. Second, semantic trace playback (STP) enables traces of disk traffic to be easily modified to represent changes in the file system implementation; in contrast to directly modifying the file system, STP enables users to rapidly gauge the benefits of new policies. We use SBA to analyze Linux ext3, ReiserFS, JFS, and Windows NTFS; in the process, we uncover many strengths and weaknesses of these journaling file systems. We also apply STP to evaluate several modifications to ext3, demonstrating the benefits of various optimizations without incurring the costs of a real implementation.}
}

@article{qiao2019hyper,
  title = {Hyper Dimension Shuffle: {{Efficient}} Data Repartition at Petabyte Scale in Scope},
  author = {Qiao, Shi and Nicoara, Adrian and Sun, Jin and Friedman, Marc and Patel, Hiren and Ekanayake, Jaliya},
  year = {2019},
  journal = {Proceedings of the VLDB Endowment},
  volume = {12},
  number = {10},
  pages = {1113--1125},
  publisher = {{VLDB Endowment}},
  abstract = {In distributed query processing, data shuffle is one of the most costly operations. We examined scaling limitations to data shuffle that current systems and the research literature do not solve. As the number of input and output partitions increases, na{\"i}ve shuffling will result in high fan-out and fan-in. There are practical limits to fan-out, as a consequence of limits on memory buffers, network ports and I/O handles. There are practical limits to fan-in because it multiplies the communication errors due to faults in commodity clusters impeding progress. Existing solutions that limit fan-out and fan-in do so at the cost of scaling quadratically in the number of nodes in the data flow graph. This dominates the costs of shuffling large datasets. We propose a novel algorithm called Hyper Dimension Shuffle that we have introduced in production in SCOPE, Microsoft's internal big data analytics system. Hyper Dimension Shuffle is inspired by the divide and conquer concept, and utilizes a recursive partitioner with intermediate aggregations. It yields quasilinear complexity of the shuffling graph with tight guarantees on fan-out and fan-in. We demonstrate how it avoids the shuffling graph blow-up of previous algorithms to shuffle at petabyte-scale efficiently on both synthetic benchmarks and real applications.}
}

@article{rahman2017ve,
  title = {I've Seen" Enough" Incrementally Improving Visualizations to Support Rapid Decision Making},
  author = {Rahman, Sajjadur and Aliakbarpour, Maryam and Kong, Ha Kyung and Blais, Eric and Karahalios, Karrie and Parameswaran, Aditya and Rubinfield, Ronitt},
  year = {2017},
  journal = {Proceedings of the VLDB Endowment},
  volume = {10},
  number = {11},
  pages = {1262--1273},
  publisher = {{VLDB Endowment}},
  abstract = {Data visualization is an effective mechanism for identifying trends, insights, and anomalies in data. On large datasets, however, generating visualizations can take a long time, delaying the extraction of insights, hampering decision making, and reducing exploration time. One solution is to use online sampling-based schemes to generate visualizations faster while improving the displayed estimates incrementally, eventually converging to the exact visualization computed on the entire data. However, the intermediate visualizations are approximate, and often fluctuate drastically, leading to potentially incorrect decisions. We propose sampling-based incremental visualization algorithms that reveal the "salient" features of the visualization quickly---with a 46{\texttimes} speedup relative to baselines---while minimizing error, thus enabling rapid and error-free decision making. We demonstrate that these algorithms are optimal in terms of sample complexity, in that given the level of interactivity, they generate approximations that take as few samples as possible. We have developed the algorithms in the context of an incremental visualization tool, titled IncVisage, for trendline and heatmap visualizations. We evaluate the usability of IncVisage via user studies and demonstrate that users are able to make effective decisions with incrementally improving visualizations, especially compared to vanilla online-sampling based schemes.}
}

@inproceedings{raman2002partial,
  title = {Partial Results for Online Query Processing},
  booktitle = {Proceedings of the 2002 {{ACM SIGMOD}} International Conference on {{Management}} of Data},
  author = {Raman, Vijayshankar and Hellerstein, Joseph M},
  year = {2002},
  pages = {275--286},
  abstract = {Traditional query processors generate full, accurate query results, either in batch or in pipelined fashion. We argue that this strict model is too rigid for exploratory queries over diverse and distributed data sources, such as sources on the Internet. Instead, we propose a looser model of querying in which a user submits a broad initial query outline, and the system continually generates partial result tuples that may contain values for only some of the output fields. The user can watch these partial results accumulate at the user interface, and accordingly refine the query by specifying their interest in different kinds of partial results.After describing our querying model and user interface, we present a query processing architecture for this model which is implemented in the Telegraph dataflow system. Our architecture is designed to generate partial results quickly, and to adapt query execution to changing user interests. The crux of this architecture is a dataflow operator that supports two kinds of reorderings: reordering of intermediate tuples within a dataflow, and reordering of query plan operators through which tuples flow. We study reordering policies that optimize for the quality of partial results delivered over time, and experimentally demonstrate the benefits of our architecture in this context.}
}

@inproceedings{rao2000making,
  title = {Making {{B}}+-Trees Cache Conscious in Main Memory},
  booktitle = {Proceedings of the 2000 {{ACM SIGMOD}} International Conference on {{Management}} of Data},
  author = {Rao, Jun and Ross, Kenneth A},
  year = {2000},
  pages = {475--486},
  abstract = {Previous research has shown that cache behavior is important for main memory index structures. Cache conscious index structures such as Cache Sensitive Search Trees (CSS-Trees) perform lookups much faster than binary search and T-Trees. However, CSS-Trees are designed for decision support workloads with relatively static data. Although B+-Trees are more cache conscious than binary search and T-Trees, their utilization of a cache line is low since half of the space is used to store child pointers. Nevertheless, for applications that require incremental updates, traditional B+-Trees perform well. Our goal is to make B+-Trees as cache conscious as CSS-Trees without increasing their update cost too much. We propose a new indexing technique called ``Cache Sensitive B+-Trees'' (CSB+-Trees). It is a variant of B+-Trees that stores all the child nodes of any given node contiguously, and keeps only the address of the first child in each node. The rest of the children can be found by adding an offset to that address. Since only one child pointer is stored explicitly, the utilization of a cache line is high. CSB+-Trees support incremental updates in a way similar to B+-Trees. We also introduce two variants of CSB+-Trees. Segmented CSB+-Trees divide the child nodes into segments. Nodes within the same segment are stored contiguously and only pointers to the beginning of each segment are stored explicitly in each node. Segmented CSB+-Trees can reduce the copying cost when there is a split since only one segment needs to be moved. Full CSB+-Trees preallocate space for the full node group and thus reduce the split cost. Our performance studies show that CSB+-Trees are useful for a wide range of applications.}
}

@article{rashd1991microkernel,
  title = {Microkernel Operating System Architecture and Mach},
  author = {RAsHD, RICHARD F and DRAvEs, R{\i}cARD P and DEAN, RANDALL W},
  year = {1991},
  journal = {Journal of Information Processing},
  volume = {14},
  number = {4}
}

@article{rekatsinas2017holoclean,
  title = {Holoclean: {{Holistic}} Data Repairs with Probabilistic Inference},
  author = {Rekatsinas, Theodoros and Chu, Xu and Ilyas, Ihab F and R{\'e}, Christopher},
  year = {2017},
  journal = {arXiv preprint arXiv:1702.00820},
  eprint = {1702.00820},
  abstract = {We introduce HoloClean, a framework for holistic data repairing driven by probabilistic inference. HoloClean unifies existing qualitative data repairing approaches, which rely on integrity constraints or external data sources, with quantitative data repairing methods, which leverage statistical properties of the input data. Given an inconsistent dataset as input, HoloClean automatically generates a probabilistic program that performs data repairing. Inspired by recent theoretical advances in probabilistic inference, we introduce a series of optimizations which ensure that inference over HoloClean's probabilistic model scales to instances with millions of tuples. We show that HoloClean scales to instances with millions of tuples and find data repairs with an average precision of {\textasciitilde}90\% and an average recall of above {\textasciitilde}76\% across a diverse array of datasets exhibiting different types of errors. This yields an average F1 improvement of more than 2x against state-of-the-art methods.},
  archiveprefix = {arxiv}
}

@misc{RelativePerformanceRust,
  title = {The Relative Performance of {{C}} and {{Rust}} {\textendash} {{The Observation Deck}}},
  urldate = {2023-11-27},
  howpublished = {https://bcantrill.dtrace.org/2018/09/28/the-relative-performance-of-c-and-rust/},
  file = {/Users/jayithac/Zotero/storage/GK2LPT4K/the-relative-performance-of-c-and-rust.html}
}

@inproceedings{revilak2011precisely,
  title = {Precisely Serializable Snapshot Isolation ({{PSSI}})},
  booktitle = {2011 {{IEEE}} 27th International Conference on Data Engineering},
  author = {Revilak, Stephen and O'Neil, Patrick and O'Neil, Elizabeth},
  year = {2011},
  pages = {482--493},
  publisher = {{IEEE}},
  abstract = {Many popular database management systems provide snapshot isolation (SI) for concurrency control, either in addition to or in place of full serializability based on locking. Snapshot isolation was introduced in 1995, with noted anomalies that can lead to serializability violations. Full serializability was provided in 2008 and improved in 2009 by aborting transactions in dangerous structures, which had been shown in 2005 to be precursors to potential SI anomalies. This approach resulted in a runtime environment guaranteeing a serializable form of snapshot isolation (which we call SSI or ESSI) for arbitrary applications. But transactions in a dangerous structure frequently do not cause true anomalies so, as the authors point out, their method is conservative: it can cause unnecessary aborts. In the current paper, we demonstrate our PSSI algorithm to detect cycles in a snapshot isolation dependency graph and abort transactions to break the cycle. This algorithm provides a much more precise criterion to perform aborts. We have implemented our algorithm in an open source production database system (MySQL/InnoDB), and our performance study shows that PSSI throughput improves on ESSI, with significantly fewer aborts.}
}

@inproceedings{rhea2003pond,
  title = {Pond: {{The}} \{\vphantom\}{{OceanStore}}\vphantom\{\} Prototype},
  booktitle = {{{2Nd USENIX}} Conference on File and Storage Technologies ({{FAST}} 03)},
  author = {Rhea, Sean and Eaton, Patrick and Geels, Dennis and Weatherspoon, Hakim and Zhao, Ben and Kubiatowicz, John},
  year = {2003},
  abstract = {OceanStore is an Internet-scale, persistent data store designed for incremental scalability, secure sharing, and long-term durability. Pond is the OceanStore prototype; it contains many of the features of a complete system including location-independent routing, Byzantine update commitment, push-based update of cached copies through an overlay multicast network, and continuous archiving to erasure-coded form. In the wide area, Pond outperforms NFS by up to a factor of 4.6 on read-intensive phases of the Andrew benchmark, but underperforms NFS by as much as a factor of 7.3 on write-intensive phases. Microbenchmarks show that write performance is limited by the speed of erasure coding and threshold signature generation, two important areas of future research. Further microbenchmarks show that Pond manages replica consistency in a bandwidth-efficient manner and quantify the latency cost imposed by this bandwidth savings.}
}

@inproceedings{rhea2004handling,
  title = {Handling Churn in a {{DHT}}},
  booktitle = {Proceedings of the {{USENIX}} Annual Technical Conference},
  author = {Rhea, Sean and Geels, Dennis and Roscoe, Timothy and Kubiatowicz, John and others},
  year = {2004},
  volume = {6},
  pages = {127--140},
  publisher = {{Boston, MA, USA}}
}

@article{ritchie1978unix,
  title = {The {{UNIX}} Time-Sharing System},
  author = {Ritchie, Dennis M and Thompson, Ken},
  year = {1978},
  journal = {Bell System Technical Journal},
  volume = {57},
  number = {6},
  pages = {1905--1929},
  publisher = {{Wiley Online Library}},
  abstract = {unix* is a general-purpose, multi-user, interactive operating system for the larger Digital Equipment Corporation pdp-11 and the Interdata 8/32 computers. It offers a number of features seldom found even in larger operating systems, including (i) A hierarchical file system incorporating demountable volumes, (ii) Compatible file, device, and inter-process I/O, (iii) The ability to initiate asynchronous processes, (iv) System command language selectable on a per-user basis, (v) Over 100 subsystems including a dozen languages, (vi) High degree of portability. This paper discusses the nature and implementation of the file system and of the user command interface.}
}

@misc{roscoe2007writing,
  title = {Writing Reviews for Systems Conferences},
  author = {Roscoe, Timothy},
  year = {2007},
  file = {/Users/jayithac/Library/CloudStorage/GoogleDrive-jayithareddyp@gmail.com/My Drive/Papers/roscoe2007writing.pdf}
}

@article{rosenblum1992design,
  title = {The Design and Implementation of a Log-Structured File System},
  author = {Rosenblum, Mendel and Ousterhout, John K},
  year = {1992},
  journal = {ACM Transactions on Computer Systems (TOCS)},
  volume = {10},
  number = {1},
  pages = {26--52},
  publisher = {{ACM New York, NY, USA}},
  abstract = {This paper presents a new technique for disk storage management called a log-structured file system. A log-structured file system writes all modifications to disk sequentially in a log-like structure, thereby speeding up both file writing and crash recovery. The log is the only structure on disk; it contains indexing information so that files can be read back from the log efficiently. In order to maintain large free areas on disk for fast writing, we divide the log intosegmentsand use a segment cleaner to compress the live information from heavily fragmented segments. We present a series of simulations that demonstrate the efficiency of a simple cleaning policy based on cost and benefit. We have implemented a prototype log-structured file system called Sprite LFS; it outperforms current Unix file systems by an order of magnitude for small-file writes while matching or exceeding Unix performance for reads and large writes. Even when the overhead for cleaning is included, Sprite LFS can use 70\% of the disk bandwidth for writing, whereas Unix file systems typically can use only 5{\textendash}10\%.}
}

@inproceedings{rowstron2001pastry,
  title = {Pastry: {{Scalable}}, Decentralized Object Location, and Routing for Large-Scale Peer-to-Peer Systems},
  booktitle = {Middleware 2001: {{IFIP}}/{{ACM}} International Conference on Distributed Systems Platforms Heidelberg, Germany, November 12{\textendash}16, 2001 Proceedings 2},
  author = {Rowstron, Antony and Druschel, Peter},
  year = {2001},
  pages = {329--350},
  publisher = {{Springer}},
  abstract = {This paper presents the design and evaluation of Pastry, a scalable, distributed object location and routing substrate for wide-area peer-to-peer applications. Pastry performs application-level routing and object location in a potentially very large overlay network of nodes connected via the Internet. It can be used to support a variety of peer-to-peer applications, including global data storage, data sharing, group communication and naming. Each node in the Pastry network has a unique identifier (nodeId). When presented with a message and a key, a Pastry node efficiently routes the message to the node with a nodeId that is numerically closest to the key, among all currently live Pastry nodes. Each Pastry node keeps track of its immediate neighbors in the nodeId space, and notifies applications of new node arrivals, node failures and recoveries. Pastry takes into account network locality; it seeks to minimize the distance messages travel, according to a to scalar proximity metric like the number of IP routing hops Pastry is completely decentralized, scalable, and self-organizing; it automatically adapts to the arrival, departure and failure of nodes. Experimental results obtained with a prototype implementation on an emulated network of up to 100,000 nodes confirm Pastry's scalability and efficiency, its ability to self-organize and adapt to node failures, and its good network locality properties}
}

@article{rowstron2001storage,
  title = {Storage Management and Caching in {{PAST}}},
  author = {Rowstron, Antony and Druschel, Peter},
  year = {2001},
  journal = {A Large-scale, Persistent Peer-to-peer Storage Utility},
  pages = {188--201}
}

@misc{RustCollectionsCase,
  title = {Rust {{Collections Case Study}}: {{BTreeMap}}},
  urldate = {2023-11-27},
  howpublished = {https://cglab.ca/{\textasciitilde}abeinges/blah/rust-btree-case/},
  file = {/Users/jayithac/Zotero/storage/3RG6QI6Y/rust-btree-case.html}
}

@article{sabek2022can,
  title = {Can Learned Models Replace Hash Functions?},
  author = {Sabek, Ibrahim and Vaidya, Kapil and Horn, Dominik and Kipf, Andreas and Mitzenmacher, Michael and Kraska, Tim},
  year = {2022},
  journal = {Proceedings of the VLDB Endowment},
  volume = {16},
  number = {3},
  pages = {532--545},
  publisher = {{VLDB Endowment}},
  abstract = {Hashing is a fundamental operation in database management, playing a key role in the implementation of numerous core database data structures and algorithms. Traditional hash functions aim to mimic a function that maps a key to a random value, which can result in collisions, where multiple keys are mapped to the same value. There are many well-known schemes like chaining, probing, and cuckoo hashing to handle collisions. In this work, we aim to study if using learned models instead of traditional hash functions can reduce collisions and whether such a reduction translates to improved performance, particularly for indexing and joins. We show that learned models reduce collisions in some cases, which depend on how the data is distributed. To evaluate the effectiveness of learned models as hash function, we test them with bucket chaining, linear probing, and cuckoo hash tables. We find that learned models can (1) yield a 1.4x lower probe latency, and (2) reduce the non-partitioned hash join runtime with 28\% over the next best baseline for certain datasets. On the other hand, if the data distribution is not suitable, we either do not see gains or see worse performance. In summary, we find that learned models can indeed outperform hash functions, but only for certain data distributions.}
}

@article{saltzer1984end,
  title = {End-to-End Arguments in System Design},
  author = {Saltzer, Jerome H and Reed, David P and Clark, David D},
  year = {1984},
  journal = {ACM Transactions on Computer Systems (TOCS)},
  volume = {2},
  number = {4},
  pages = {277--288},
  publisher = {{Acm New York, NY, USA}}
}

@phdthesis{samuel2021hydroflow,
  title = {Hydroflow: {{A}} Model and Runtime for Distributed Systems Programming},
  author = {Samuel, Mingwei and Hellerstein, Joseph M and Cheung, Alvin},
  year = {2021},
  school = {Master's thesis. EECS Department, University of California, Berkeley. http {\ldots}}
}

@article{santana2016fast,
  title = {A Fast and Slippery Slope for File Systems},
  author = {Santana, Ricardo and Rangaswami, Raju and Tarasov, Vasily and Hildebrand, Dean},
  year = {2016},
  journal = {ACM SIGOPS Operating Systems Review},
  volume = {49},
  number = {2},
  pages = {27--34},
  publisher = {{ACM New York, NY, USA}},
  abstract = {There is a vast number and variety of file systems currently available, each optimizing for an ever growing number of storage devices and workloads. Users have an unprecedented, and somewhat overwhelming, number of data management options. At the same time, the fastest storage devices are only getting faster, and it is unclear on how well the existing file systems will adapt. Using emulation techniques, we evaluate five popular Linux file systems across a range of storage device latencies typical to low-end hard drives, latest high-performance persistent memory block devices, and in between. Our findings are often surprising. Depending on the workload, we find that some file systems can clearly scale with faster storage devices much better than others. Further, as storage device latency decreases, we find unexpected performance inversions across file systems. Finally, file system scalability in the higher device latency range is not representative of scalability in the lower, submillisecond, latency range. We then focus on Nilfs2 as an especially alarming example of an unexpectedly poor scalability and present detailed instructions for identifying bottlenecks in the I/O stack.}
}

@article{satyanarayanan1994lightweight,
  title = {Lightweight Recoverable Virtual Memory},
  author = {Satyanarayanan, Mahadev and Mashburn, Henry H and Kumar, Puneet and Steere, David C and Kistler, James J},
  year = {1994},
  journal = {ACM Transactions on Computer Systems (TOCS)},
  volume = {12},
  number = {1},
  pages = {33--57},
  publisher = {{ACM New York, NY, USA}},
  abstract = {Recoverable virtual memoryrefers to regions of a virtual address space on which transactional guarantees are offered. This article describes RVM, an efficient, portable, and easily used implementation of recoverable virtual memory for Unix environments. A unique characteristic of RVM is that it allows independent control over the transactional properties of atomicity, permanence, and serializability. This leads to considerable flexibility in the use of RVM, potentially enlarging the range of applications that can benefit from transactions. It also simplifies the layering of functionality such as nesting and distribution. The article shows that RVM performs well over its intended range of usage even though it does not benefit from specialized operating system support. It also demonstrates the importance of intra- and inter-transaction optimizations.}
}

@article{satyanarayanan2009case,
  title = {The Case for Vm-Based Cloudlets in Mobile Computing},
  author = {Satyanarayanan, Mahadev and Bahl, Paramvir and Caceres, Ram{\'o}n and Davies, Nigel},
  year = {2009},
  journal = {IEEE pervasive Computing},
  volume = {8},
  number = {4},
  pages = {14--23},
  publisher = {{IEEE}},
  abstract = {Mobile computing continuously evolve through the sustained effort of many researchers. It seamlessly augments users' cognitive abilities via compute-intensive capabilities such as speech recognition, natural language processing, etc. By thus empowering mobile users, we could transform many areas of human activity. This article discusses the technical obstacles to these transformations and proposes a new architecture for overcoming them. In this architecture, a mobile user exploits virtual machine (VM) technology to rapidly instantiate customized service software on a nearby cloudlet and then uses that service over a wireless LAN; the mobile device typically functions as a thin client with respect to the service. A cloudlet is a trusted, resource-rich computer or cluster of computers that's well-connected to the Internet and available for use by nearby mobile devices. Our strategy of leveraging transiently customized proximate infrastructure as a mobile device moves with its user through the physical world is called cloudlet-based, resource-rich, mobile computing. Crisp interactive response, which is essential for seamless augmentation of human cognition, is easily achieved in this architecture because of the cloudlet's physical proximity and one-hop network latency. Using a cloudlet also simplifies the challenge of meeting the peak bandwidth demand of multiple users interactively generating and receiving media such as high-definition video and high-resolution images. Rapid customization of infrastructure for diverse applications emerges as a critical requirement, and our results from a proof-of-concept prototype suggest that VM technology can indeed help meet this requirement.}
}

@article{satyanarayanan2017emergence,
  title = {The Emergence of Edge Computing},
  author = {Satyanarayanan, Mahadev},
  year = {2017},
  journal = {Computer},
  volume = {50},
  number = {1},
  pages = {30--39},
  publisher = {{IEEE}},
  abstract = {Industry investment and research interest in edge computing, in which computing and storage nodes are placed at the Internet's edge in close proximity to mobile devices or sensors, have grown dramatically in recent years. This emerging technology promises to deliver highly responsive cloud services for mobile computing, scalability and privacy-policy enforcement for the Internet of Things, and the ability to mask transient cloud outages. The web extra at www.youtube.com/playlist?list=PLmrZVvFtthdP3fwHPy\_4d61oDvQY\_RBgS includes a five-video playlist demonstrating proof-of-concept implementations for three tasks: assembling 2D Lego models, freehand sketching, and playing Ping-Pong.}
}

@article{schuhknecht2013uncracked,
  title = {The Uncracked Pieces in Database Cracking},
  author = {Schuhknecht, Felix Martin and Jindal, Alekh and Dittrich, Jens},
  year = {2013},
  journal = {Proceedings of the VLDB Endowment},
  volume = {7},
  number = {2},
  pages = {97--108},
  publisher = {{VLDB Endowment}},
  abstract = {Database cracking has been an area of active research in recent years. The core idea of database cracking is to create indexes adaptively and incrementally as a side-product of query processing. Several works have proposed different cracking techniques for different aspects including updates, tuple-reconstruction, convergence, concurrency-control, and robustness. However, there is a lack of any comparative study of these different methods by an independent group. In this paper, we conduct an experimental study on database cracking. Our goal is to critically review several aspects, identify the potential, and propose promising directions in database cracking. With this study, we hope to expand the scope of database cracking and possibly leverage cracking in database engines other than MonetDB. We repeat several prior database cracking works including the core cracking algorithms as well as three other works on convergence (hybrid cracking), tuple-reconstruction (sideways cracking), and robustness (stochastic cracking) respectively. We evaluate these works and show possible directions to do even better. We further test cracking under a variety of experimental settings, including high selectivity queries, low selectivity queries, and multiple query access patterns. Finally, we compare cracking against different sorting algorithms as well as against different main-memory optimised indexes, including the recently proposed Adaptive Radix Tree (ART). Our results show that: (i) the previously proposed cracking algorithms are repeatable, (ii) there is still enough room to significantly improve the previously proposed cracking algorithms, (iii) cracking depends heavily on query selectivity, (iv) cracking needs to catch up with modern indexing trends, and (v) different indexing algorithms have different indexing signatures.}
}

@article{sears2009segment,
  title = {Segment-Based Recovery: Write-Ahead Logging Revisited},
  author = {Sears, Russell and Brewer, Eric},
  year = {2009},
  journal = {Proceedings of the VLDB Endowment},
  volume = {2},
  number = {1},
  pages = {490--501},
  publisher = {{VLDB Endowment}},
  abstract = {Although existing write-ahead logging algorithms scale to conventional database workloads, their communication and synchronization overheads limit their usefulness for modern applications and distributed systems. We revisit write-ahead logging with an eye toward finer-grained concurrency and an increased range of workloads, then remove two core assumptions: that pages are the unit of recovery and that times-tamps (LSNs) should be stored on each page. Recovering individual application-level objects (rather than pages) simplifies the handing of systems with object sizes that differ from the page size. We show how to remove the need for LSNs on the page, which in turn enables DMA or zero-copy I/O for large objects, increases concurrency, and reduces communication between the application, buffer manager and log manager. Our experiments show that the looser coupling significantly reduces the impact of latency among the components. This makes the approach particularly applicable to large scale distributed systems, and enables a "cross pollination" of ideas from distributed systems and transactional storage. However, these advantages come at a cost; segments are incompatible with physiological redo, preventing a number of important optimizations. We show how allocation enables (or prevents) mixing of ARIES pages (and physiological redo) with segments. We present an allocation policy that avoids undesirable interactions that complicate other combinations of ARIES and LSN-free pages, and then present a proof that both approaches and our combination are correct. Many optimizations presented here were proposed in the past. However, we believe this is the first unified approach.}
}

@inproceedings{shapiro2011conflict,
  title = {Conflict-Free Replicated Data Types},
  booktitle = {Stabilization, Safety, and Security of Distributed Systems: 13th International Symposium, {{SSS}} 2011, Grenoble, France, October 10-12, 2011. {{Proceedings}} 13},
  author = {Shapiro, Marc and Pregui{\c c}a, Nuno and Baquero, Carlos and Zawirski, Marek},
  year = {2011},
  pages = {386--400},
  publisher = {{Springer}},
  abstract = {Replicating data under Eventual Consistency (EC) allows any replica to accept updates without remote synchronisation. This ensures performance and scalability in large-scale distributed systems (e.g., clouds). However, published EC approaches are ad-hoc and error-prone. Under a formal Strong Eventual Consistency (SEC) model, we study sufficient conditions for convergence. A data type that satisfies these conditions is called a Conflict-free Replicated Data Type (CRDT). Replicas of any CRDT are guaranteed to converge in a self-stabilising manner, despite any number of failures. This paper formalises two popular approaches (state- and operation-based) and their relevant sufficient conditions. We study a number of useful CRDTs, such as sets with clean semantics, supporting both add and remove operations, and consider in depth the more complex Graph data type. CRDT types can be composed to develop large-scale distributed applications, and have interesting theoretical properties.}
}

@misc{shneorWritingStorageEngine2021,
  title = {Writing a Storage Engine in {{Rust}}: {{Writing}} a Persistent {{BTree}} ({{Part}} 1)},
  shorttitle = {Writing a Storage Engine in {{Rust}}},
  author = {Shneor, Nimrod},
  year = {2021},
  month = mar,
  journal = {Medium},
  urldate = {2023-11-07},
  abstract = {As part of a recent personal journey to better understand databases and better learn Rust, I have recently took on the project of writing{\ldots}},
  langid = {english},
  file = {/Users/jayithac/Zotero/storage/5BCEIFT4/writing-a-storage-engine-in-rust-writing-a-persistent-btree-part-1-916b6f3e2934.html;/Users/jayithac/Zotero/storage/UTW2N79I/writing-a-storage-engine-in-rust-writing-a-persistent-btree-part-1-916b6f3e2934.html}
}

@book{silberschatz2005database,
  title = {Database Systems Concepts},
  author = {Silberschatz, Abraham and Korth, Henry and Sudarshan, Shashank},
  year = {2005},
  publisher = {{McGraw-Hill, Inc.}}
}

@article{singh2016blinkfill,
  title = {Blinkfill: {{Semi-supervised}} Programming by Example for Syntactic String Transformations},
  author = {Singh, Rishabh},
  year = {2016},
  journal = {Proceedings of the VLDB Endowment},
  volume = {9},
  number = {10},
  pages = {816--827},
  publisher = {{VLDB Endowment}},
  abstract = {The recent Programming By Example (PBE) techniques such as FlashFill have shown great promise for enabling end-users to perform data transformation tasks using input-output examples. Since examples are inherently an under-specification, there are typically a large number of hypotheses conforming to the examples, and the PBE techniques suffer from scalability issues for finding the intended program amongst the large space. We present a semi-supervised learning technique to significantly reduce this ambiguity by using the logical information present in the input data to guide the synthesis algorithm. We develop a data structure InputDataGraph to succinctly represent a large set of logical patterns that are shared across the input data, and use this graph to efficiently learn substring expressions in a new PBE system BlinkFill. We evaluate BlinkFill on 207 real-world benchmarks and show that BlinkFill is significantly faster (on average 41x) and requires fewer input-output examples (1.27 vs 1.53) to learn the desired transformations in comparison to FlashFill.}
}

@article{slaney2008locality,
  title = {Locality-Sensitive Hashing for Finding Nearest Neighbors [Lecture Notes]},
  author = {Slaney, Malcolm and Casey, Michael},
  year = {2008},
  journal = {IEEE Signal processing magazine},
  volume = {25},
  number = {2},
  pages = {128--131},
  publisher = {{IEEE}},
  abstract = {This lecture note describes a technique known as locality-sensitive hashing (LSH) that allows one to quickly find similar entries in large databases. This approach belongs to a novel and interesting class of algorithms that are known as randomized algorithms. A randomized algorithm does not guarantee an exact answer but instead provides a high probability guarantee that it will return the correct answer or one close to it. By investing additional computational effort, the probability can be pushed as high as desired.},
  file = {/Users/jayithac/Library/CloudStorage/GoogleDrive-jayithareddyp@gmail.com/My Drive/Papers/slaney2008locality.pdf}
}

@article{stoica2003chord,
  title = {Chord: A Scalable Peer-to-Peer Lookup Protocol for Internet Applications},
  author = {Stoica, Ion and Morris, Robert and {Liben-Nowell}, David and Karger, David R and Kaashoek, M Frans and Dabek, Frank and Balakrishnan, Hari},
  year = {2003},
  journal = {IEEE/ACM Transactions on networking},
  volume = {11},
  number = {1},
  pages = {17--32},
  publisher = {{IEEE}},
  abstract = {A fundamental problem that confronts peer-to-peer applications is the efficient location of the node that stores a desired data item. This paper presents Chord, a distributed lookup protocol that addresses this problem. Chord provides support for just one operation: given a key, it maps the key onto a node. Data location can be easily implemented on top of Chord by associating a key with each data item, and storing the key/data pair at the node to which the key maps. Chord adapts efficiently as nodes join and leave the system, and can answer queries even if the system is continuously changing. Results from theoretical analysis and simulations show that Chord is scalable: Communication cost and the state maintained by each node scale logarithmically with the number of Chord nodes.}
}

@incollection{stonebraker1976design,
  title = {The Design and Implementation of {{INGRES}}},
  booktitle = {Making Databases Work: The Pragmatic Wisdom of Michael Stonebraker},
  author = {Stonebraker, Michael and Wong, Eugene and Kreps, Peter and Held, Gerald},
  year = {1976},
  pages = {561--605},
  file = {/Users/jayithac/Library/CloudStorage/GoogleDrive-jayithareddyp@gmail.com/My Drive/Papers/stonebraker1987design.pdf}
}

@book{stonebraker1987design,
  title = {The Design of the {{Postgres}} Storage System},
  author = {Stonebraker, Michael},
  year = {1987}
}

@article{stonebraker1991postgres,
  title = {The {{POSTGRES}} next Generation Database Management System},
  author = {Stonebraker, Michael and Kemnitz, Greg},
  year = {1991},
  journal = {Communications of the ACM},
  volume = {34},
  number = {10},
  pages = {78--92},
  publisher = {{ACM New York, NY, USA}}
}

@incollection{stonebraker2018c,
  title = {C-Store: A Column-Oriented {{DBMS}}},
  booktitle = {Making Databases Work: The Pragmatic Wisdom of Michael Stonebraker},
  author = {Stonebraker, Mike and Abadi, Daniel J and Batkin, Adam and Chen, Xuedong and Cherniack, Mitch and Ferreira, Miguel and Lau, Edmond and Lin, Amerson and Madden, Sam and O'Neil, Elizabeth and others},
  year = {2005},
  pages = {491--518},
  abstract = {This paper presents the design of a read-optimized relational DBMS that contrasts sharply with most current systems, which are write-optimized. Among the many differences in its design are: storage of data by column rather than by row, careful coding and packing of objects into storage including main memory during query processing, storing an overlapping collection of column-oriented projections, rather than the current fare of tables and indexes, a non-traditional implementation of transactions which includes high availability and snapshot isolation for read-only transactions, and the extensive use of bitmap indexes to complement B-tree structures. We present preliminary performance data on a subset of TPC-H and show that the system we are building, C-Store, is substantially faster than popular commercial products. Hence, the architecture looks very encouraging.}
}

@article{stonebraker2010mapreduce,
  title = {{{MapReduce}} and Parallel {{DBMSs}}: Friends or Foes?},
  author = {Stonebraker, Michael and Abadi, Daniel and DeWitt, David J and Madden, Sam and Paulson, Erik and Pavlo, Andrew and Rasin, Alexander},
  year = {2010},
  journal = {Communications of the ACM},
  volume = {53},
  number = {1},
  pages = {64--71},
  publisher = {{ACM New York, NY, USA}},
  abstract = {MapReduce complements DBMSs since databases are not designed for extract-transform-load tasks, a MapReduce specialty.}
}

@article{sun2019end,
  title = {An End-to-End Learning-Based Cost Estimator},
  author = {Sun, Ji and Li, Guoliang},
  year = {2019},
  journal = {VLDB},
  abstract = {Cost and cardinality estimation is vital to query optimizer, which can guide the plan selection. However traditional empirical cost and cardinality estimation techniques cannot provide high-quality estimation, because they cannot capture the correlation between multiple columns. Recently the database community shows that the learning-based cardinality estimation is better than the empirical methods. However, existing learning-based methods have several limitations. Firstly, they can only estimate the cardinality, but cannot estimate the cost. Secondly, convolutional neural network (CNN) with average pooling is hard to represent complicated structures, e.g., complex predicates, and the model is hard to be generalized. To address these challenges, we propose an effective end-to-end learning-based cost estimation framework based on a tree-structured model, which can estimate both cost and cardinality simultaneously. To the best of our knowledge, this is the first end-to-end cost estimator based on deep learning. We propose effective feature extraction and encoding techniques, which consider both queries and physical operations in feature extraction. We embed these features into our tree-structured model. We propose an effective method to encode string values, which can improve the generalization ability for predicate matching. As it is prohibitively expensive to enumerate all string values, we design a patten-based method, which selects patterns to cover string values and utilizes the patterns to embed string values. We conducted experiments on real-world datasets and experimental results showed that our method outperformed baselines.}
}

@inproceedings{sweeney1996scalability,
  title = {Scalability in the {{XFS}} File System.},
  booktitle = {{{USENIX}} Annual Technical Conference},
  author = {Sweeney, Adam and Doucette, Doug and Hu, Wei and Anderson, Curtis and Nishimoto, Mike and Peck, Geoff},
  year = {1996},
  volume = {15}
}

@inproceedings{szekeres2020meerkat,
  title = {Meerkat: {{Multicore-scalable}} Replicated Transactions Following the Zero-Coordination Principle},
  booktitle = {Proceedings of the Fifteenth European Conference on Computer Systems},
  author = {Szekeres, Adriana and Whittaker, Michael and Li, Jialin and Sharma, Naveen Kr and Krishnamurthy, Arvind and Ports, Dan RK and Zhang, Irene},
  year = {2020},
  pages = {1--14},
  abstract = {Traditionally, the high cost of network communication between servers has hidden the impact of cross-core coordination in replicated systems. However, new technologies, like kernel-bypass networking and faster network links, have exposed hidden bottlenecks in distributed systems. This paper explores how to build multicore-scalable, replicated storage systems. We introduce a new guideline for their design, called the Zero-Coordination Principle. We use this principle to design a new multicore-scalable, in-memory, replicated, key-value store, called Meerkat. Unlike existing systems, Meerkat eliminates all cross-core and cross-replica coordination, both of which pose a scalability bottleneck. Our experiments found that Meerkat is able to scale up to 80 hyper-threads and execute 8.3 million transactions per second. Meerkat represents an improvement of 12X on state-of-the-art, fault-tolerant, in-memory, transactional storage systems built using leader-based replication and a shared transaction log.}
}

@inproceedings{tan2001efficient,
  title = {Efficient Progressive Skyline Computation},
  booktitle = {{{VLDB}}},
  author = {Tan, Kian-Lee and Eng, Pin-Kwang and Ooi, Beng Chin and others},
  year = {2001},
  volume = {1},
  pages = {301--310},
  file = {/Users/jayithac/Library/CloudStorage/GoogleDrive-jayithareddyp@gmail.com/My Drive/Papers/tan2001efficient.pdf}
}

@misc{TiKVBlogBuilding,
  title = {The {{TiKV}} Blog | {{Building}} a {{Large-scale Distributed Storage System Based}} on {{Raft}}},
  urldate = {2023-11-27},
  howpublished = {https://tikv.org/blog/building-distributed-storage-system-on-raft/}
}

@inproceedings{van2018foreshadow,
  title = {Foreshadow: {{Extracting}} the Keys to the Intel \{\vphantom\}{{SGX}}\vphantom\{\} Kingdom with Transient \{\vphantom\}{{Out-of-Order}}\vphantom\{\} Execution},
  booktitle = {27th {{USENIX}} Security Symposium ({{USENIX}} Security 18)},
  author = {Van Bulck, Jo and Minkin, Marina and Weisse, Ofir and Genkin, Daniel and Kasikci, Baris and Piessens, Frank and Silberstein, Mark and Wenisch, Thomas F and Yarom, Yuval and Strackx, Raoul},
  year = {2018},
  pages = {991--1008},
  abstract = {Trusted execution environments, and particularly the Software Guard eXtensions (SGX) included in recent Intel x86 processors, gained significant traction in recent years. A long track of research papers, and increasingly also real-world industry applications, take advantage of the strong hardware-enforced confidentiality and integrity guarantees provided by Intel SGX. Ultimately, enclaved execution holds the compelling potential of securely offloading sensitive computations to untrusted remote platforms. We present Foreshadow, a practical software-only microarchitectural attack that decisively dismantles the security objectives of current SGX implementations. Crucially, unlike previous SGX attacks, we do not make any assumptions on the victim enclave's code and do not necessarily require kernel-level access. At its core, Foreshadow abuses a speculative execution bug in modern Intel processors, on top of which we develop a novel exploitation methodology to reliably leak plaintext enclave secrets from the CPU cache. We demonstrate our attacks by extracting full cryptographic keys from Intel's vetted architectural enclaves, and validate their correctness by launching rogue production enclaves and forging arbitrary local and remote attestation responses. The extracted remote attestation keys affect millions of devices.}
}

@article{veldhuizen2012leapfrog,
  title = {Leapfrog Triejoin: A Worst-Case Optimal Join Algorithm},
  author = {Veldhuizen, Todd L},
  year = {2012},
  journal = {arXiv preprint arXiv:1210.0481},
  eprint = {1210.0481},
  abstract = {Recent years have seen exciting developments in join algorithms. In 2008, Atserias, Grohe and Marx (henceforth AGM) proved a tight bound on the maximum result size of a full conjunctive query, given constraints on the input relation sizes. In 2012, Ngo, Porat, R\{{\'e}\} and Rudra (henceforth NPRR) devised a join algorithm with worst-case running time proportional to the AGM bound. Our commercial Datalog system LogicBlox employs a novel join algorithm, {\textbackslash}emph\{leapfrog triejoin\}, which compared conspicuously well to the NPRR algorithm in preliminary benchmarks. This spurred us to analyze the complexity of leapfrog triejoin. In this paper we establish that leapfrog triejoin is also worst-case optimal, up to a log factor, in the sense of NPRR. We improve on the results of NPRR by proving that leapfrog triejoin achieves worst-case optimality for finer-grained classes of database instances, such as those defined by constraints on projection cardinalities. We show that NPRR is {\textbackslash}emph\{not\} worst-case optimal for such classes, giving a counterexample where leapfrog triejoin runs in O(nlogn) time, compared to {$\Theta$}(n1.375) time for NPRR. On a practical note, leapfrog triejoin can be implemented using conventional data structures such as B-trees, and extends naturally to {$\exists$}1 queries. We believe our algorithm offers a useful addition to the existing toolbox of join algorithms, being easy to absorb, simple to implement, and having a concise optimality proof.},
  archiveprefix = {arxiv}
}

@inproceedings{verbitski2017amazon,
  title = {Amazon Aurora: {{Design}} Considerations for High Throughput Cloud-Native Relational Databases},
  booktitle = {Proceedings of the 2017 {{ACM}} International Conference on Management of Data},
  author = {Verbitski, Alexandre and Gupta, Anurag and Saha, Debanjan and Brahmadesam, Murali and Gupta, Kamal and Mittal, Raman and Krishnamurthy, Sailesh and Maurice, Sandor and Kharatishvili, Tengiz and Bao, Xiaofeng},
  year = {2017},
  pages = {1041--1052},
  abstract = {Amazon Aurora is a relational database service for OLTP workloads offered as part of Amazon Web Services (AWS). In this paper, we describe the architecture of Aurora and the design considerations leading to that architecture. We believe the central constraint in high throughput data processing has moved from compute and storage to the network. Aurora brings a novel architecture to the relational database to address this constraint, most notably by pushing redo processing to a multi-tenant scale-out storage service, purpose-built for Aurora. We describe how doing so not only reduces network traffic, but also allows for fast crash recovery, failovers to replicas without loss of data, and fault-tolerant, self-healing storage. We then describe how Aurora achieves consensus on durable state across numerous storage nodes using an efficient asynchronous scheme, avoiding expensive and chatty recovery protocols. Finally, having operated Aurora as a production service for over 18 months, we share the lessons we have learnt from our customers on what modern cloud applications expect from databases.}
}

@inproceedings{verbitski2018amazon,
  title = {Amazon Aurora: {{On}} Avoiding Distributed Consensus for i/Os, Commits, and Membership Changes},
  booktitle = {Proceedings of the 2018 International Conference on Management of Data},
  author = {Verbitski, Alexandre and Gupta, Anurag and Saha, Debanjan and Corey, James and Gupta, Kamal and Brahmadesam, Murali and Mittal, Raman and Krishnamurthy, Sailesh and Maurice, Sandor and Kharatishvilli, Tengiz and others},
  year = {2018},
  pages = {789--796},
  abstract = {Amazon Aurora is a high-throughput cloud-native relational database offered as part of Amazon Web Services (AWS). One of the more novel differences between Aurora and other relational databases is how it pushes redo processing to a multi-tenant scale-out storage service, purpose-built for Aurora. Doing so reduces networking traffic, avoids checkpoints and crash recovery, enables failovers to replicas without loss of data, and enables fault-tolerant storage that heals without database involvement. Traditional implementations that leverage distributed storage would use distributed consensus algorithms for commits, reads, replication, and membership changes and amplify cost of underlying storage. In this paper, we describe how Aurora avoids distributed consensus under most circumstances by establishing invariants and leveraging local transient state. Doing so improves performance, reduces variability, and lowers costs.}
}

@inproceedings{waldspurger1994lottery,
  title = {Lottery Scheduling: {{Flexible}} Proportional-Share Resource Management},
  booktitle = {Proceedings of the 1st {{USENIX}} Conference on Operating Systems Design and Implementation},
  author = {Waldspurger, Carl A and Weihl, William E},
  year = {1994},
  pages = {1--es}
}

@inproceedings{wang2018building,
  title = {Building a Bw-Tree Takes More than Just Buzz Words},
  booktitle = {Proceedings of the 2018 International Conference on Management of Data},
  author = {Wang, Ziqi and Pavlo, Andrew and Lim, Hyeontaek and Leis, Viktor and Zhang, Huanchen and Kaminsky, Michael and Andersen, David G},
  year = {2018},
  pages = {473--488},
  abstract = {In 2013, Microsoft Research proposed the Bw-Tree (humorously termed the "Buzz Word Tree''), a lock-free index that provides high throughput for transactional database workloads in SQL Server's Hekaton engine. The Buzz Word Tree avoids locks by appending delta record to tree nodes and using an indirection layer that allows it to atomically update physical pointers using compare-and-swap (CaS). Correctly implementing this techniques requires careful attention to detail. Unfortunately, the Bw-Tree papers from Microsoft are missing important details and the source code has not been released. This paper has two contributions: First, it is the missing guide for how to build a lock-free Bw-Tree. We clarify missing points in Microsoft's original design documents and then present techniques to improve the index's performance. Although our focus here is on the Bw-Tree, many of our methods apply more broadly to designing and implementing future lock-free in-memory data structures. Our experimental evaluation shows that our optimized variant achieves 1.1--2.5{\texttimes} better performance than the original Microsoft proposal for highly concurrent workloads. Second, our evaluation shows that despite our improvements, the Bw-Tree still does not perform as well as other concurrent data structures that use locks.}
}

@inproceedings{wang2022conjunctive,
  title = {Conjunctive Queries with Comparisons},
  booktitle = {Proceedings of the 2022 International Conference on Management of Data},
  author = {Wang, Qichen and Yi, Ke},
  year = {2022},
  series = {{{SIGMOD}} '22},
  pages = {108--121},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3514221.3517830},
  abstract = {Conjunctive queries with predicates in the form of comparisons that span multiple relations have regained interest recently, due to their relevance in OLAP queries, spatiotemporal databases, and machine learning over relational data. The standard technique, predicate pushdown, has limited efficacy on such comparisons. A technique by Willard can be used to process short comparisons that are adjacent in the join tree in time linear in the input size plus output size. In this paper, we describe a new algorithm for evaluating conjunctive queries with both short and long comparisons, and identify an acyclic condition under which linear time can be achieved. We have also implemented the new algorithm on top of Spark, and our experimental results demonstrate order-of-magnitude speedups over SparkSQL on a variety of graph pattern and analytical queries.},
  isbn = {978-1-4503-9249-5},
  keywords = {acyclic joins,conjunctive query,inequality joins}
}

@article{weiss2010logoot,
  title = {Logoot-Undo: {{Distributed}} Collaborative Editing System on P2p Networks},
  author = {Weiss, Stephane and Urso, Pascal and Molli, Pascal},
  year = {2010},
  journal = {IEEE transactions on parallel and distributed systems},
  volume = {21},
  number = {8},
  pages = {1162--1174},
  publisher = {{IEEE}},
  abstract = {Peer-to-peer systems provide scalable content distribution for cheap and resist to censorship attempts. However, P2P networks mainly distribute immutable content and provide poor support for highly dynamic content such as produced by collaborative systems. A new class of algorithms called CRDT (Commutative Replicated Data Type), which ensures consistency of highly dynamic content on P2P networks, is emerging. However, if existing CRDT algorithms support the "edit anywhere, anytime'' feature, they do not support the "undo anywhere, anytime'' feature. In this paper, we present the Logoot-Undo CRDT algorithm, which integrates the "undo anywhere, anytime'' feature. We compare the performance of the proposed algorithm with related algorithms and measure the impact of the undo feature on the global performance of the algorithm. We prove that the cost of the undo feature remains low on a corpus of data extracted from Wikipedia.}
}

@article{welsh2001seda,
  title = {{{SEDA}}: {{An}} Architecture for Well-Conditioned, Scalable Internet Services},
  author = {Welsh, Matt and Culler, David and Brewer, Eric},
  year = {2001},
  journal = {ACM SIGOPS operating systems review},
  volume = {35},
  number = {5},
  pages = {230--243},
  publisher = {{ACM New York, NY, USA}},
  abstract = {We propose a new design for highly concurrent Internet services, which we call the staged event-driven architecture (SEDA). SEDA is intended to support massive concurrency demands and simplify the construction of well-conditioned services. In SEDA, applications consist of a network of event-driven stages connected by explicit queues. This architecture allows services to be well-conditioned to load, preventing resources from being overcommitted when demand exceeds service capacity. SEDA makes use of a set of dynamic resource controllers to keep stages within their operating regime despite large fluctuations in load. We describe several control mechanisms for automatic tuning and load conditioning, including thread pool sizing, event batching, and adaptive load shedding. We present the SEDA design and an implementation of an Internet services platform based on this architecture. We evaluate the use of SEDA through two applications: a high-performance HTTP server and a packet router for the Gnutella peer-to-peer file sharing network. These results show that SEDA applications exhibit higher performance than traditional service designs, and are robust to huge variations in load.}
}

@article{wilkes1996hp,
  title = {The {{HP AutoRAID}} Hierarchical Storage System},
  author = {Wilkes, John and Golding, Richard and Staelin, Carl and Sullivan, Tim},
  year = {1996},
  journal = {ACM Transactions on Computer Systems (TOCS)},
  volume = {14},
  number = {1},
  pages = {108--136},
  publisher = {{ACM New York, NY, USA}},
  abstract = {Configuring redundant disk arrays is a black art. To configure an array properly, a system administrator must understand the details of both the array and the workload it will support. Incorrect understanding of either, or changes in the workload over time, can lead to poor performance. We present a solution to this problem: a two-level storage hierarchy implemented inside a single disk-array controller. In the upper level of this hierarchy, two copies of active data are stored to provide full redundancy and excellent performance. In the lower level, RAID 5 parity protection is used to provide excellent storage cost for inactive data, at somewhat lower performance. The technology we describe in this article, know as HP AutoRAID, automatically and transparently manages migration of data blocks between these two levels as access patterns change. The result is a fully redundant storage system that is extremely easy to use, is suitable for a wide variety of workloads, is largely insensitive to dynamic workload changes, and performs much better than disk arrays with comparable numbers of spindles and much larger amounts of front-end RAM cache. Because the implementation of the HP AutoRAID technology is almost entirely in software, the additional hardware cost for these benefits is very small. We describe the HP AutoRAID technology in detail, provide performance data for an embodiment of it in a storage array, and summarize the results of simulation studies used to choose algorithms implemented in the array.}
}

@inproceedings{wolbach2008transient,
  title = {Transient Customization of Mobile Computing Infrastructure},
  booktitle = {Proceedings of the First Workshop on Virtualization in Mobile Computing},
  author = {Wolbach, Adam and Harkes, Jan and Chellappa, Srinivas and Satyanarayanan, Mahadev},
  year = {2008},
  pages = {37--41},
  abstract = {Kimberley is a system that simplifies transient use of fixed hardware infrastructure by a mobile device. It uses virtual machine (VM) technology to resolve the tension between standardizing infrastructure for ease of deployment and maintenance, and customizing that infrastructure to meet the specific needs of a user. Kimberley decomposes the state of a customized VM into a widely-available base VM and a much smaller private VM overlay. The base is downloaded by the infrastructure in advance. Only the small overlay needs to be delivered from the mobile device, or under its control from a public web site. This strategy keeps startup delay low. It may also conserve energy on the mobile device by reducing the volume of wireless transmission. We have built a prototype of Kimberley, and our experiments confirm the feasibility of this approach.}
}

@article{wu2018towards,
  title = {Towards a Learning Optimizer for Shared Clouds},
  author = {Wu, Chenggang and Jindal, Alekh and Amizadeh, Saeed and Patel, Hiren and Le, Wangchao and Qiao, Shi and Rao, Sriram},
  year = {2018},
  journal = {Proceedings of the VLDB Endowment},
  volume = {12},
  number = {3},
  pages = {210--222},
  publisher = {{VLDB Endowment}},
  abstract = {Query optimizers are notorious for inaccurate cost estimates, leading to poor performance. The root of the problem lies in inaccurate cardinality estimates, i.e., the size of intermediate (and final) results in a query plan. These estimates also determine the resources consumed in modern shared cloud infrastructures. In this paper, we present CARDLEARNER, a machine learning based approach to learn cardinality models from previous job executions and use them to predict the cardinalities in future jobs. The key intuition in our approach is that shared cloud workloads are often recurring and overlapping in nature, and so we could learn cardinality models for overlapping subgraph templates. We discuss various learning approaches and show how learning a large number of smaller models results in high accuracy and explainability. We further present an exploration technique to avoid learning bias by considering alternate join orders and learning cardinality models over them. We describe the feedback loop to apply the learned models back to future job executions. Finally, we show a detailed evaluation of our models (up to 5 orders of magnitude less error), query plans (60\% applicability), performance (up to 100\% faster, 3x fewer resources), and exploration (optimal in few 10s of executions).}
}

@article{wu2019anna,
  title = {Anna: {{A}} Kvs for Any Scale},
  author = {Wu, Chenggang and Faleiro, Jose M and Lin, Yihan and Hellerstein, Joseph M},
  year = {2019},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {33},
  number = {2},
  pages = {344--358},
  publisher = {{IEEE}},
  abstract = {Modern cloud providers offer dense hardware with multiple cores and large memories, hosted in global platforms. This raises the challenge of implementing high-performance software systems that can effectively scale from a single core to multicore to the globe. Conventional wisdom says that software designed for one scale point needs to be rewritten when scaling up by 10 - 100{\texttimes} [1]. In contrast, we explore how a system can be architected to scale across many orders of magnitude by design. We explore this challenge in the context of a new key-value store system called Anna: a partitioned, multi-mastered system that achieves high performance and elasticity via wait-free execution and coordination-free consistency. Our design rests on a simple architecture of coordination-free actors that perform state update via merge of lattice-based composite data structures. We demonstrate that a wide variety of consistency models can be elegantly implemented in this architecture with unprecedented consistency, smooth fine-grained elasticity, and performance that far exceeds the state of the art.}
}

@article{yang2019deep,
  title = {Deep Unsupervised Cardinality Estimation},
  author = {Yang, Zongheng and Liang, Eric and Kamsetty, Amog and Wu, Chenggang and Duan, Yan and Chen, Xi and Abbeel, Pieter and Hellerstein, Joseph M and Krishnan, Sanjay and Stoica, Ion},
  year = {2019},
  journal = {arXiv preprint arXiv:1905.04278},
  eprint = {1905.04278},
  abstract = {Cardinality estimation has long been grounded in statistical tools for density estimation. To capture the rich multivariate distributions of relational tables, we propose the use of a new type of high-capacity statistical model: deep autoregressive models. However, direct application of these models leads to a limited estimator that is prohibitively expensive to evaluate for range or wildcard predicates. To produce a truly usable estimator, we develop a Monte Carlo integration scheme on top of autoregressive models that can efficiently handle range queries with dozens of dimensions or more. Like classical synopses, our estimator summarizes the data without supervision. Unlike previous solutions, we approximate the joint data distribution without any independence assumptions. Evaluated on real-world datasets and compared against real systems and dominant families of techniques, our estimator achieves single-digit multiplicative error at tail, an up to 90[Math Processing Error] accuracy improvement over the second best method, and is space- and runtime-efficient.},
  archiveprefix = {arxiv}
}

@inproceedings{yang2022balsa,
  title = {Balsa: {{Learning}} a Query Optimizer without Expert Demonstrations},
  booktitle = {Proceedings of the 2022 International Conference on Management of Data},
  author = {Yang, Zongheng and Chiang, Wei-Lin and Luan, Sifei and Mittal, Gautam and Luo, Michael and Stoica, Ion},
  year = {2022},
  pages = {931--944},
  abstract = {Query optimizers are a performance-critical component in every database system. Due to their complexity, optimizers take experts months to write and years to refine. In this work, we demonstrate for the first time that learning to optimize queries without learning from an expert optimizer is both possible and efficient. We present Balsa, a query optimizer built by deep reinforcement learning. Balsa first learns basic knowledge from a simple, environment-agnostic simulator, followed by safe learning in real execution. On the Join Order Benchmark, Balsa matches the performance of two expert query optimizers, both open-source and commercial, with two hours of learning, and outperforms them by up to 2.8{\texttimes} in workload runtime after a few more hours. Balsa thus opens the possibility of automatically learning to optimize in future compute environments where expert-designed optimizers do not exist.}
}

@inproceedings{zaharia2010spark,
  title = {Spark: {{Cluster}} Computing with Working Sets},
  booktitle = {2nd {{USENIX}} Workshop on Hot Topics in Cloud Computing ({{HotCloud}} 10)},
  author = {Zaharia, Matei and Chowdhury, Mosharaf and Franklin, Michael J and Shenker, Scott and Stoica, Ion},
  year = {2010}
}

@inproceedings{zaharia2012resilient,
  title = {Resilient Distributed Datasets: {{A}} \{fault-Tolerant\} Abstraction for \{in-Memory\} Cluster Computing},
  booktitle = {9th {{USENIX}} Symposium on Networked Systems Design and Implementation ({{NSDI}} 12)},
  author = {Zaharia, Matei and Chowdhury, Mosharaf and Das, Tathagata and Dave, Ankur and Ma, Justin and McCauly, Murphy and Franklin, Michael J and Shenker, Scott and Stoica, Ion},
  year = {2012},
  pages = {15--28}
}

@article{zeldovich2011making,
  title = {Making Information Flow Explicit in {{HiStar}}},
  author = {Zeldovich, Nickolai and {Boyd-Wickizer}, Silas and Kohler, Eddie and Mazieres, David},
  year = {2011},
  journal = {Communications of the ACM},
  volume = {54},
  number = {11},
  pages = {93--101},
  publisher = {{ACM New York, NY, USA}},
  abstract = {HiStar is a new operating system designed to minimize the amount of code that must be trusted. HiStar provides strict information flow control, which allows users to specify precise data security policies without unduly limiting the structure of applications. HiStar's security features make it possible to implement a Unix-like environment with acceptable performance almost entirely in an untrusted user-level library. The system has no notion of superuser and no fully trusted code other than the kernel. HiStar's features permit several novel applications, including privacy-preserving, untrusted virus scanners and a dynamic Web server with only a few thousand lines of trusted code.}
}

@inproceedings{zhang2015cloud,
  title = {The Cloud Is Not Enough: {{Saving}} \{\vphantom\}{{IoT}}\vphantom\{\} from the Cloud},
  booktitle = {7th {{USENIX}} Workshop on Hot Topics in Cloud Computing ({{HotCloud}} 15)},
  author = {Zhang, Ben and Mor, Nitesh and Kolb, John and Chan, Douglas S and Lutz, Ken and Allman, Eric and Wawrzynek, John and Lee, Edward and Kubiatowicz, John},
  year = {2015},
  abstract = {The Internet of Things (IoT) represents a new class of applications that can benefit from cloud infrastructure. However, the current approach of directly connecting smart devices to the cloud has a number of disadvantages and is unlikely to keep up with either the growing speed of the IoT or the diverse needs of IoT applications. In this paper we explore these disadvantages and argue that fundamental properties of the IoT prevent the current approach from scaling. What is missing is a wellarchitected system that extends the functionality of the cloud and provides seamless interplay among the heterogeneous components in the IoT space. We argue that raising the level of abstraction to a data-centric design{\textemdash}focused around the distribution, preservation and protection of information{\textemdash}provides a much better match to the IoT.We present early work on such a distributed platform, called the Global Data Plane (GDP), and discuss how it addresses the problems with the cloud-centric architecture.}
}

@article{zhao2001tapestry,
  title = {Tapestry: {{An}} Infrastructure for Fault-Tolerant Wide-Area Location and Routing},
  author = {Zhao, Ben Yanbin and Kubiatowicz, John and Joseph, Anthony D and others},
  year = {2001},
  publisher = {{Computer Science Division, University of California Berkeley}}
}

@article{zhao2004tapestry,
  title = {Tapestry: {{A}} Resilient Global-Scale Overlay for Service Deployment},
  author = {Zhao, Ben Y and Huang, Ling and Stribling, Jeremy and Rhea, Sean C and Joseph, Anthony D and Kubiatowicz, John D},
  year = {2004},
  journal = {IEEE Journal on selected areas in communications},
  volume = {22},
  number = {1},
  pages = {41--53},
  publisher = {{IEEE}},
  abstract = {We present Tapestry, a peer-to-peer overlay routing infrastructure offering efficient, scalable, location-independent routing of messages directly to nearby copies of an object or service using only localized resources. Tapestry supports a generic decentralized object location and routing applications programming interface using a self-repairing, soft-state-based routing layer. The paper presents the Tapestry architecture, algorithms, and implementation. It explores the behavior of a Tapestry deployment on PlanetLab, a global testbed of approximately 100 machines. Experimental results show that Tapestry exhibits stable behavior and performance as an overlay, despite the instability of the underlying network layers. Several widely distributed applications have been implemented on Tapestry, illustrating its utility as a deployment infrastructure.}
}
